#+TITLE:         Assignment 6. The Analytics Edge
#+AUTHOR:        Sergio-Feliciano Mendoza-Barrera
#+DRAWERS:       sfmb
#+EMAIL:         smendoza.barrera@gmail.com
#+DATE:          19/07/2015
#+DESCRIPTION:   Unit 6 homework of the TAE.
#+KEYWORDS:      R, data science, emacs, ESS, org-mode, TAE, clustering, k-means
#+LANGUAGE:      en
#+OPTIONS:       H:10 num:t toc:nil \n:nil @:t ::t |:t ^:{} -:t f:t *:t <:t d:HIDDEN
#+OPTIONS:       TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+OPTIONS:       LaTeX:dvipng
#+INFOJS_OPT:    view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:
#+LINK_HOME:
#+XSLT:
#+STYLE: <link rel="stylesheet" type="text/css" href="dft.css"/>

#+LaTeX_CLASS: IEEEtran
#+LATEX_CLASS_OPTIONS: [letterpaper, 9pt, onecolumn, twoside, technote, final]
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage{makeidx}

#+LATEX_HEADER: \usepackage[lining,tabular]{fbb} % so math uses tabular lining figures
#+LATEX_HEADER: \usepackage[scaled=.95,type1]{cabin} % sans serif in style of Gill Sans
#+LATEX_HEADER: \usepackage[varqu,varl]{zi4}% inconsolata typewriter
#+LATEX_HEADER: \usepackage[T1]{fontenc} % LY1 also works
#+LATEX_HEADER: \usepackage[libertine,bigdelims]{newtxmath}
#+LATEX_HEADER: \usepackage[cal=boondoxo,bb=boondox,frak=boondox]{mathalfa}
#+LATEX_HEADER: \useosf % change normal text to use proportional oldstyle figures

#+LATEX_HEADER: \markboth{Unit 6 homework. July 2015.}%
#+LATEX_HEADER: {Sergio-Feliciano Mendoza-Barrera}

#+LATEX_HEADER: \newcommand{\degC}{$^\circ$C{}}

#+STYLE: <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script>

#+ATTR_HTML: width="500px"

# -*- mode: org; -*-
#+OPTIONS:   toc:2

#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>

#+HTML_HEAD: <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
#+HTML_HEAD: <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>

#+BEGIN_ABSTRACT
The homework of the unit 6 of the Analytics Edge.
#+END_ABSTRACT

* Document clustering with daily KOS

Document clustering, or text clustering, is a very popular application
of clustering algorithms. A web search engine, like Google, often
returns thousands of results for a simple query. For example, if you
type the search term "jaguar" into Google, around 200 million results
are returned. This makes it very difficult to browse or find relevant
information, especially if the search term has multiple meanings. If
we search for "jaguar", we might be looking for information about the
animal, the car, or the Jacksonville Jaguars football team.

Clustering methods can be used to automatically group search results
into categories, making it easier to find relavent results. This
method is used in the search engines PolyMeta and Helioid, as well as
on FirstGov.gov, the official Web portal for the U.S. government. The
two most common algorithms used for document clustering are
Hierarchical and k-means.

In this problem, we'll be clustering articles published on [[https://www.dailykos.com][Daily Kos]],
an American political blog that publishes news and opinion articles
written from a progressive point of view. Daily Kos was founded by
Markos Moulitsas in 2002, and as of September 2014, the site had an
average weekday traffic of hundreds of thousands of visits.

The file [[https://courses.edx.org/asset-v1:MITx%2B15.071x_2a%2B2T2015%2Btype@asset%2Bblock/dailykos.csv][dailykos.csv]] contains data on 3,430 news articles or blogs
that have been posted on Daily Kos. These articles were posted in
2004, leading up to the United States Presidential Election. The
leading candidates were incumbent President George W. Bush
(republican) and John Kerry (democratic). Foreign policy was a
dominant topic of the election, specifically, the 2003 invasion of
Iraq.

Each of the variables in the dataset is a word that has appeared in at
least 50 different articles (1,545 words in total). The set of  words
has been trimmed according to some of the techniques covered in the
previous week on text analytics (punctuation has been removed, and
stop words have been removed). For each document, the variable values
are the number of times that word appeared in the document.

** Problem 1.1 - Hierarchical Clustering (1 point possible)

Let's start by building a hierarchical clustering model. First, read
the data set into R.

*** Download the data sets

In this part we can download the data

#+BEGIN_SRC R :session :results output :exports all
  library(parallel)

  if(!file.exists("../data")) {
          dir.create("../data")
  }

  fileUrl <- "https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/dailykos.csv"
  fileName <- "dailykos.csv"
  dataPath <- "../data"

  filePath <- paste(dataPath, fileName, sep = "/")
  if(!file.exists(filePath)) {
          download.file(fileUrl, destfile = filePath, method = "curl")
  }
  list.files("../data")
#+END_SRC

#+RESULTS:
#+begin_example
 [1] "AnonymityPoll.csv"       "baseball.csv"
 [3] "BoeingStock.csv"         "boston.csv"
 [5] "ClaimsData.csv"          "ClaimsData.csv.zip"
 [7] "climate_change.csv"      "clinical_trial.csv"
 [9] "ClusterMeans.ods"        "CocaColaStock.csv"
[11] "CountryCodes.csv"        "CPSData.csv"
[13] "dailykos.csv"            "emails.csv"
[15] "energy_bids.csv"         "flower.csv"
[17] "FluTest.csv"             "FluTrain.csv"
[19] "framingham.csv"          "gerber.csv"
[21] "GEStock.csv"             "healthy.csv"
[23] "IBMStock.csv"            "loans_imputed.csv"
[25] "loans.csv"               "MetroAreaCodes.csv"
[27] "movieLens.txt"           "mvtWeek1.csv"
[29] "NBA_test.csv"            "NBA_train.csv"
[31] "parole.csv"              "pisa2009test.csv"
[33] "pisa2009train.csv"       "PollingData_Imputed.csv"
[35] "PollingData.csv"         "ProcterGambleStock.csv"
[37] "quality.csv"             "README.md"
[39] "songs.csv"               "stevens.csv"
[41] "stopwords.txt"           "tumor.csv"
[43] "tweets.csv"              "USDA.csv"
[45] "WHO_Europe.csv"          "WHO.csv"
[47] "wiki.csv"                "wine_test.csv"
[49] "wine.csv"
#+end_example

*** Load the data set

#+BEGIN_SRC R :session :results output :exports all
  writeLines("    Loading data set into their data frame...")
  dailykos <- read.table("../data/dailykos.csv", sep = ",", header = TRUE)
#+END_SRC

#+RESULTS:
:     Loading data set into their data frame...

Then, compute the distances (using method="euclidean"), and use hclust
to build the model (using method="ward.D"). You should cluster on all
of the variables.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Compute distances...")
  distances <- dist(dailykos, method = "euclidean")

  writeLines("\n :: Hierarchical clustering...")
  clusterDailykos <- hclust(distances, method = "ward.D")
#+end_src

Running the dist function will probably take you a while. Why? Select
all that apply.

*** Answer

- [ ] We have a lot of observations, so it takes a long time to
  compute the distance between each pair of observations.

- [ ] We have a lot of variables, so the distance computation is long.

- [ ] Our variables have a wide range of values, so the distances are
  more complicated.

- [ ] The euclidean distance is known to take a long time to compute,
  regardless of the size of the data.
