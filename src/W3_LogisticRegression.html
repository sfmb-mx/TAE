<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Unit 3 - Logistic Regression</title>
<!-- 2015-07-30 Thu 07:25 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="Sergio-Feliciano Mendoza-Barrera" />
<meta  name="description" content="R introduction, remembering the syntax and some useful examples"
 />
<meta  name="keywords" content="R, data science, emacs, ESS, org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Unit 3 - Logistic Regression</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. Modeling the Expert: An Introduction to Logistic Regression</a>
<ul>
<li><a href="#sec-1-1">1.1. Logistic Regression</a></li>
<li><a href="#sec-1-2">1.2. Logistic Regression in R</a></li>
<li><a href="#sec-1-3">1.3. Video 1: Replicating Expert Assessment</a></li>
<li><a href="#sec-1-4">1.4. Video 2: Building the Dataset</a></li>
<li><a href="#sec-1-5">1.5. Quick Question (2 points possible)</a></li>
<li><a href="#sec-1-6">1.6. Video 3: Logistic Regression</a></li>
<li><a href="#sec-1-7">1.7. Understanding the Logistic Regression Function</a></li>
<li><a href="#sec-1-8">1.8. Quick Question (3 points possible)</a></li>
<li><a href="#sec-1-9">1.9. Video 4: Logistic Regression in R</a></li>
<li><a href="#sec-1-10">1.10. Quick Question (1 point possible)</a></li>
<li><a href="#sec-1-11">1.11. Quick Question (1 point possible)</a></li>
<li><a href="#sec-1-12">1.12. Video 5: Thresholding</a></li>
<li><a href="#sec-1-13">1.13. The confusion matrix or classification matrix</a></li>
<li><a href="#sec-1-14">1.14. Confusion matrices questions</a></li>
<li><a href="#sec-1-15">1.15. Video 6: ROC Curves</a></li>
<li><a href="#sec-1-16">1.16. Quick Question (2 points possible)</a></li>
<li><a href="#sec-1-17">1.17. Video 7: Interpreting the Model</a></li>
<li><a href="#sec-1-18">1.18. Quick Question (1 point possible)</a></li>
<li><a href="#sec-1-19">1.19. Video 8: The Analytics Edge</a></li>
</ul>
</li>
<li><a href="#sec-2">2. The Framingham Heart Study: Evaluating Risk Factors to Save Lives</a>
<ul>
<li><a href="#sec-2-1">2.1. Video 1: The Framingham Heart Study</a></li>
<li><a href="#sec-2-2">2.2. Quick Question (1 point possible)</a></li>
<li><a href="#sec-2-3">2.3. Video 2: Risk Factors</a></li>
<li><a href="#sec-2-4">2.4. Quick Question (2 points possible)</a></li>
<li><a href="#sec-2-5">2.5. Video 3: - A Logistic Regression Model</a></li>
<li><a href="#sec-2-6">2.6. Quick Question (2 points possible)</a></li>
<li><a href="#sec-2-7">2.7. Video 4: Validating the Model</a></li>
<li><a href="#sec-2-8">2.8. Quick Question (1 point possible)</a></li>
<li><a href="#sec-2-9">2.9. Video 5: Interventions</a></li>
<li><a href="#sec-2-10">2.10. Quick Question (1 point possible)</a></li>
<li><a href="#sec-2-11">2.11. Video 6: Overall Impact</a></li>
</ul>
</li>
<li><a href="#sec-3">3. Election Forecasting: Predicting the Winner Before any Votes are Cast (Recitation)</a>
<ul>
<li><a href="#sec-3-1">3.1. Video 1: Election Prediction</a></li>
<li><a href="#sec-3-2">3.2. Video 2: Dealing with Missing Data</a></li>
<li><a href="#sec-3-3">3.3. Video 3: A Sophisticated Baseline Method</a></li>
<li><a href="#sec-3-4">3.4. Video 4: Logistic Regression Models</a></li>
<li><a href="#sec-3-5">3.5. Video 5: Test Set Predictions</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="abstract">
<p>
Logistic Regression topics. For the course "MITx: 15.071x The Analytics Edge".
</p>

</div>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> Modeling the Expert: An Introduction to Logistic Regression</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-sec-1-1" class="outline-3">
<h3 id="sec-1-1"><span class="section-number-3">1.1</span> Logistic Regression</h3>
<div class="outline-text-3" id="text-1-1">
<p>
<b>The Method</b>
</p>

<p>
Logistic regression extends the idea of linear regression to cases
where the dependent variable, \(y\), only has two possible outcomes,
called classes. Examples of dependent variables that could be used
with logistic regression are predicting whether a new business will
succeed or fail, predicting the approval or disapproval of a loan, and
predicting whether a stock will increase or decrease in value. These
are all called <b>classification problems</b>, since the goal is to figure
out which class each observation belongs to.
</p>

<p>
Similar to linear regression, logistic regression uses a set of
independent variables to make predictions, but instead of predicting a
continuous value for the dependent variable, it instead predicts the
probability of each of the possible outcomes, or classes.
</p>

<p>
Logistic regression consists of two steps. The first step is to
compute the probability that an observation belongs to class 1, using
the <b>Logistic Response Function</b>:
</p>

<p>
$$
P(y = 1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1x_1 + \beta_2x_2 + \ldots + \beta_kx_k)}}
$$
</p>

<p>
The coefficients, or \(\beta\) values, are selected to maximize the likelihood
of predicting a high probability for observations actually belonging
to class 1, and predicting a low probability for observations actually
belonging to class 0.
</p>

<p>
In the second step of logistic regression, a threshold value is used
to classify each observation into one of the classes. A common choice
is \(0.5\), meaning that if \(P(y = 1) \geq 0.5\), the observation is
classified into class 1, and if \(P(y = 1) < 0.5\), the observation is
classified into class 0. Simply stated, each observation is classified
into the class with the highest probability.
</p>

<p>
However, other threshold values can be chosen, and in some cases are
more appropriate. The threshold value that should be selected often
depends on error preferences. When the probabilities are converted
into class predictions, two types of errors can be made: false
positives, and false negatives. A <b>false positive error</b> is made when
the model predicts class 1, but the observation actually belongs to
class 0. A <b>false negative error</b> is made when the model predicts class
0, but the observation actually belongs to class 1. If a higher
threshold value is selected, more false negative errors will be
made. If a lower threshold value is selected, more false positive
errors will be made.
</p>

<p>
One application where decision-makers often have an error preference
is in disease prediction. Suppose you built a model to predict whether
or not someone will develop heart disease in the next 10 years (like
the model we saw in the Framingham Heart Study lecture). We will
consider class 1 to be the outcome in which the person does develop
heart disease, and class 0 the outcome in which the person does not
develop heart disease. If you pick a high threshold, you will tend to
make more false negative errors, which means that you predicted that
the person would not develop heart disease, but they actually did. If
you pick a lower threshold, you will tend to make more false positive
errors, which means that you predicted they would develop heart
disease, but they actually did not. In this case, a false positive
error is often preferred. Unnecessary resources might be spent
treating a patient who did not need to worry, but you did not let as
many patients go untreated (which is what a false negative error
does).
</p>

<p>
Now, let's consider spam filters. Almost every email provider has a
built in spam filter that tries to detect whether or not an email
message is spam. Let's classify spam messages as class 1 and non-spam
messages as class 0. Then if we build a logistic regression model to
predict spam, we will probably want to select a high threshold. Why?
In this case, a false positive error means that we predicted a message
was spam, and sent it to the spam folder, when it actually was not
spam. We might have just sent an important email to the junk folder!
On the other hand, a false negative error means that we predicted a
message was not spam, when it actually was. This creates a slight
annoyance for the user (since they have to delete the message from the
inbox themselves) but at least an important message was not missed.
</p>

<p>
This error trade-off can be formalized with a <a href="https://courses.edx.org/wiki/15.071x_2/logistic-regression/confusion-matrix">Confusion Matrix</a> or a
<a href="https://courses.edx.org/wiki/15.071x_2/logistic-regression/roc-curve">Receiver Operator Characteristic Curve (ROC curve)</a>. A confusion matrix
compares predicted classes with actual classes for a particular
threshold value, while an ROC curve plots the false positive rate
versus the true positive rate for all possible threshold values. The
ROC curve motivates an important metric for classification problems:
the AUC, or Area Under the Curve. The AUC of a model gives the area
under the ROC curve, and is a number between 0 and 1. The higher the
AUC, the more area under the ROC curve, and the better the model. The
AUC of a model can be interpreted as the model's ability to
distinguish between the two different classes. If the model were
handed two random observations from the dataset, one belonging to one
class and one belonging to the other class, the AUC gives the
proportion of the time when the observation from class 1 has a higher
predicted probability of being in class 1. If you were to just guess
which observation was which, this would be an AUC of 0.5. So a model
with an AUC greater than 0.5 is doing something smarter than just
guessing, but we want the AUC of a model to be as close to 1 as
possible.
</p>
</div>
</div>

<div id="outline-container-sec-1-2" class="outline-3">
<h3 id="sec-1-2"><span class="section-number-3">1.2</span> Logistic Regression in R</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Suppose the training data for your model is in a data frame called
"TrainingData", consisting of your dependent variable "DependentVar",
and your two independent variables "IndependentVar1" and
"IndependentVar2". (If you just have one dataset, you can <a href="https://courses.edx.org/wiki/15.071x_2/logistic-regression/randomly-splitting-data">randomly
split</a> your data frame into a training set and testing set with the
sample.split function.) Then you can build a logistic regression model
with the following command:
</p>

<p>
<code>LogModel = glm(DependentVar ~ IndependentVar1 + IndependentVar2,
data=TrainingData, family=binomial)</code>
</p>

<p>
You can see the coefficients and other information about the model
with the summary function:
</p>

<p>
<code>summary(LogModel)</code>
</p>

<p>
You can then create a vector of predictions for the training set and
generate different confusion matrices with the predict() and table()
functions:
</p>

<p>
<code>TrainPredictions = predict(LogModel, type="response")</code>
<code>table(TrainingData$DependentVar, TrainPredictions &gt;= 0.5)</code>
<code>table(TrainingData$DependentVar, TrainPredictions &gt;= 0.3)</code>
</p>

<p>
You can generate an ROC curve with the following commands (you first
need to install and load the "ROCR" package):
</p>

<p>
<code>ROC.Pred = prediction(TrainPredictions, TrainingData$DependentVar)</code>
<code>ROC.Perf = performance(ROC.Pred, "tpr", "fpr")</code>
<code>plot(ROC.Perf)</code>
</p>

<p>
To add threshold labels and colors, replace the plot command with the following:
</p>

<p>
<code>plot(ROC.Perf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1),
text.adj=c(-0.2,1.7))</code>
</p>

<p>
The AUC of the model can be computed with the following command:
</p>

<p>
<code>as.numeric(performance(ROC.Pred, "auc")@y.values)</code>
</p>

<p>
To make predictions on a test set called "TestData", you can use the
predict() function:
</p>

<p>
<code>TestPredictions = predict(LogModel, newdata=TestData,
type="response")</code>
</p>

<p>
You can then create confusion matrices, an ROC curve, and compute the
AUC just like we did for the training set on the test set.
</p>
</div>
</div>

<div id="outline-container-sec-1-3" class="outline-3">
<h3 id="sec-1-3"><span class="section-number-3">1.3</span> Video 1: Replicating Expert Assessment</h3>
<div class="outline-text-3" id="text-1-3">
<p>
We'll examine how analytics can model an expert, in this case a
physician, in the context of assessing the quality of healthcare
patients receive, and introduce a technique called logistic regression
to achieve this objective.
</p>


<div class="figure">
<p><img src="../graphs/AskTheExperts.png" alt="AskTheExperts.png" />
</p>
</div>

<p>
The large scale problem:
</p>


<div class="figure">
<p><img src="../graphs/ExpertsAreHuman.png" alt="ExpertsAreHuman.png" />
</p>
</div>

<p>
Clearly, physicians cannot assess quality for millions of patients,
and D2Hawkeye had, indeed, millions of patients who receive claims
data on a monthly basis that the quality of them needs to be assessed.
</p>

<p>
So the key question is as follows. Can we develop analytics tools that
replicate expert assessment on a large scale?
</p>

<p>
The goal is to learn from expert human judgment by developing a model,
interpret the results of the model, and further adjust the model to
improve predictability. The objective is to make predictions and
evaluations on a large scale basis, to be able to process millions of
assessing the health care quality for millions of people.
</p>
</div>
</div>

<div id="outline-container-sec-1-4" class="outline-3">
<h3 id="sec-1-4"><span class="section-number-3">1.4</span> Video 2: Building the Dataset</h3>
<div class="outline-text-3" id="text-1-4">
<p>
So let us explain what claims data is. So medical claims are generated
when a patient visits a doctor. Medical claims include diagnosis code,
procedures codes, as well as costs.
</p>

<p>
Pharmacy claims involve drugs, the quantity of these drugs, the
prescribing doctor, as well as the medication costs. Claims data are
electronically available, they are standardized, they use
well-established codes.
</p>

<p>
<b>However, since humans generate them, they are not 100% accurate</b>.
</p>


<div class="figure">
<p><img src="../graphs/ClaimsData.png" alt="ClaimsData.png" />
</p>
</div>

<p>
And often, under-reporting is common in the sense that it's a tedious
job to record these claims, and as a result, often people under-report
them. Also, claims for hospital visits can be vague.
</p>

<p>
In creating a data set, our objective was to assess quality, health
care quality.
</p>


<div class="figure">
<p><img src="../graphs/CreatingTheDataSet01.png" alt="CreatingTheDataSet01.png" />
</p>
</div>

<p>
So we used a large health insurance claims database, and we randomly
selected 131 diabetes patients. The ages ranged between 35 to 55 and
the costs were in the neighborhood of $10,000 to $20,000.
</p>

<p>
The period in which these claims were recorded were September 1, 2003
to August 31, 2005.
</p>


<div class="figure">
<p><img src="../graphs/CreatingTheDataSet02.png" alt="CreatingTheDataSet02.png" />
</p>
</div>

<p>
An expert physician reviewed the claims and wrote descriptive notes,
like "ongoing use of narcotics"; "only on Avandia, not a good first
choice drug"; "had regular visits, mammogram, and immunizations"; "was
given home testing supplies".
</p>

<p>
After this review, this expert physician rated the quality of care on
a two-point scale, poor or good. Examples included, I'd say care was
poor. Poorly treated diabetes. Not an eye exam, but overall I'd say
high quality.
</p>


<div class="figure">
<p><img src="../graphs/CreatingTheDataSet03.png" alt="CreatingTheDataSet03.png" />
</p>
</div>

<p>
So based on these comments, we extracted variables. The dependent
variable was the <b>quality of care</b>. The independent variables involve
the <b>ongoing use of narcotics</b>; only on Avandia, not a good first choice
drug; had <b>regular visits</b>, <b>mammogram</b>, and <b>immunizations</b>; was given home
testing supplies.
</p>


<div class="figure">
<p><img src="../graphs/CreatingTheDataSet04.png" alt="CreatingTheDataSet04.png" />
</p>
</div>

<p>
Overall, the independent variables involved diabetes treatment
variables, patient demographics, health care utilization, providers,
claims, and prescriptions. The dependent variable was modeled as a
binary variable &#x2013; 1 for low-quality care and 0 for high-quality
care.
</p>


<div class="figure">
<p><img src="../graphs/CreatingTheDataSet05.png" alt="CreatingTheDataSet05.png" />
</p>
</div>

<p>
This is by its nature a categorical variable. It only takes two
possible values. We have seen linear regression as a way of predicting
continuous outcomes.
</p>


<div class="figure">
<p><img src="../graphs/PredictingQualityOfCare.png" alt="PredictingQualityOfCare.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-1-5" class="outline-3">
<h3 id="sec-1-5"><span class="section-number-3">1.5</span> Quick Question (2 points possible)</h3>
<div class="outline-text-3" id="text-1-5">
</div><div id="outline-container-sec-1-5-1" class="outline-4">
<h4 id="sec-1-5-1"><span class="section-number-4">1.5.1</span> Question a</h4>
<div class="outline-text-4" id="text-1-5-1">
<p>
Which of the following dependent variables are categorical? (Select
all that apply.)
</p>

<ul class="org-ul">
<li><code>[X]</code> Deciding whether to buy, sell, or hold a stock
</li>
<li><code>[&#xa0;]</code> The weekly revenue of a company
</li>
<li><code>[X]</code> The winner of an election with two candidates
</li>
<li><code>[X]</code> The day of the week with the highest revenue
</li>
<li><code>[&#xa0;]</code> The number of daily car thefts in New York City
</li>
<li><code>[X]</code> Whether or not revenue will exceed $50,000
</li>
</ul>
</div>

<div id="outline-container-sec-1-5-1-1" class="outline-5">
<h5 id="sec-1-5-1-1"><span class="section-number-5">1.5.1.1</span> Answer</h5>
<div class="outline-text-5" id="text-1-5-1-1">
<p>
<b>Explanation</b>
</p>

<p>
The weekly revenue of a company is not categorical, since it has a
large number of possible values, on a continuous range. The number of
daily car thefts in New York City is also not categorical because the
number of car thefts could range from 0 to hundreds.
</p>

<p>
On the other hand, the other options each have a limited number of
possible outcomes.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-1-5-2" class="outline-4">
<h4 id="sec-1-5-2"><span class="section-number-4">1.5.2</span> Question b</h4>
<div class="outline-text-4" id="text-1-5-2">
<p>
Which of the following dependent variables are binary? (Select all
that apply.)
</p>

<ul class="org-ul">
<li><code>[&#xa0;]</code> Deciding whether to buy, sell, or hold a stock
</li>
<li><code>[&#xa0;]</code> The weekly revenue of a company
</li>
<li><code>[X]</code> The winner of an election with two candidates
</li>
<li><code>[&#xa0;]</code> The day of the week with the highest revenue
</li>
<li><code>[&#xa0;]</code> The number of daily car thefts in New York City
</li>
<li><code>[X]</code> Whether or not revenue will exceed $50,000
</li>
</ul>
</div>

<div id="outline-container-sec-1-5-2-1" class="outline-5">
<h5 id="sec-1-5-2-1"><span class="section-number-5">1.5.2.1</span> Answer</h5>
<div class="outline-text-5" id="text-1-5-2-1">
<p>
<b>Explanation</b>
</p>

<p>
The only variables with two possible outcomes are the winner of an
election with two candidates, and whether or not revenue will exceed
$50,000.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-1-6" class="outline-3">
<h3 id="sec-1-6"><span class="section-number-3">1.6</span> Video 3: Logistic Regression</h3>
<div class="outline-text-3" id="text-1-6">
<p>
<b>Logistic regression</b> predicts the probability of the outcome variable
being <b>true</b>. In this example, a logistic regression model would predict
the probability that the patient is receiving <b>poor care</b>. Or if we
denote the PoorCare variable by \(y\), the probability that \(y = 1\).
</p>


<div class="figure">
<p><img src="../graphs/LogisticRegression.png" alt="LogisticRegression.png" />
</p>
</div>

<p>
So by predicting the probability that \(y = 1\), we also get the
probability that \(y = 0\). Just like in linear regression, we have a
set of independent variables, \(x_1\) through \(x_k\), where \(k\) is the
total number of independent variables we have.
</p>

<p>
Then to predict the probability that \(y = 1\), we use what's called the
<b>Logistic Response Function</b>. This seems like a complicated, nonlinear
equation, but you can see the familiar linear regression equation in
this Logistic Response Function.
</p>

<p>
The Logistic Response Function is used to produce a number between \(0\)
and \(1\).
</p>
</div>
</div>

<div id="outline-container-sec-1-7" class="outline-3">
<h3 id="sec-1-7"><span class="section-number-3">1.7</span> Understanding the Logistic Regression Function</h3>
<div class="outline-text-3" id="text-1-7">

<div class="figure">
<p><img src="../graphs/UnderstandingTheLF.png" alt="UnderstandingTheLF.png" />
</p>
</div>

<p>
This plot shows the logistic response function for different values of
the linear regression piece. The logistic response function always
takes values between \(0\) and \(1\), which makes sense, since it equals a
probability.
</p>

<p>
A positive coefficient value for a variable increases the linear
regression piece, which increases the probability that \(y = 1\), or
increases the probability of poor care. On the other hand, a negative
coefficient value for a variable decreases the linear regression
piece, which in turn decreases the probability that \(y = 1\), or
increases the probability of good care.
</p>


<div class="figure">
<p><img src="../graphs/UnderstandingTheLF02.png" alt="UnderstandingTheLF02.png" />
</p>
</div>

<p>
The coefficients, or betas, are selected to predict a high probability
for the actual poor care cases, and to predict a low probability for
the actual good care cases.
</p>

<p>
Another useful way to think about the logistic response function is in
terms of Odds, like in gambling.
</p>


<div class="figure">
<p><img src="../graphs/UnderstandingTheLF03.png" alt="UnderstandingTheLF03.png" />
</p>
</div>

<p>
If you substitute the <b>Logistic Response Function</b> for the
probabilities in the Odds equation.
</p>


<div class="figure">
<p><img src="../graphs/TheLogit.png" alt="TheLogit.png" />
</p>
</div>

<p>
This helps us understand how the coefficients, or betas, affect our
prediction of the probability. A positive \(\beta\) value increases the
<b>Logit</b>, which in turn increases the Odds of \(1\). A negative \(\beta\)
value decreases the <b>Logit</b>, which in turn, decreases the Odds of
one.
</p>
</div>
</div>

<div id="outline-container-sec-1-8" class="outline-3">
<h3 id="sec-1-8"><span class="section-number-3">1.8</span> Quick Question (3 points possible)</h3>
<div class="outline-text-3" id="text-1-8">
<p>
Suppose the coefficients of a logistic regression model with two
independent variables are as follows:
</p>

<p>
$$
\beta_0 = -1.5,~ \beta_1 = 3,~\beta_2 = -0.5
$$
</p>

<p>
And we have an observation with the following values for the
independent variables:
</p>

<p>
$$
x_1 = 1,~x_2 = 5
$$
</p>
</div>

<div id="outline-container-sec-1-8-1" class="outline-4">
<h4 id="sec-1-8-1"><span class="section-number-4">1.8.1</span> Question a</h4>
<div class="outline-text-4" id="text-1-8-1">
<p>
What is the value of the Logit for this observation? Recall that the
Logit is log(Odds).
</p>

<p>
$$
log(Odds) = \beta_0 + \beta_1 x_1 + \beta_2 x_2
$$
</p>

<div class="org-src-container">

<pre class="src src-R">beta0 <span style="color: #db7093;">&lt;-</span> -1.5; beta1 <span style="color: #db7093;">&lt;-</span> 3; beta2 <span style="color: #db7093;">&lt;-</span> -0.5;
x1 <span style="color: #db7093;">&lt;-</span> 1; x2 <span style="color: #db7093;">&lt;-</span> 5
logit <span style="color: #db7093;">&lt;-</span> beta0 + (beta1 * x1) + (beta2 * x2)
writeLines(<span style="color: #ff6a6a;">"\n :: The value of logit is:"</span>)
logit
</pre>
</div>

<pre class="example">
 :: The value of logit is:
[1] -1
</pre>
</div>

<div id="outline-container-sec-1-8-1-1" class="outline-5">
<h5 id="sec-1-8-1-1"><span class="section-number-5">1.8.1.1</span> Answer</h5>
<div class="outline-text-5" id="text-1-8-1-1">
<p>
<b>Explanation</b>
</p>

<p>
The Logit is just log(Odds), and looks like the linear regression
equation. So the Logit is -1.5 + 3*1 - 0.5*5 = -1.
</p>
</div>
</div>
</div>


<div id="outline-container-sec-1-8-2" class="outline-4">
<h4 id="sec-1-8-2"><span class="section-number-4">1.8.2</span> Question b</h4>
<div class="outline-text-4" id="text-1-8-2">
<p>
What is the value of the Odds for this observation? Note that you can
compute e^x, for some number x, in your R console by typing
exp(x). The function exp() computes the exponential of its argument.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: The value of odds is:"</span>)
exp(logit)
</pre>
</div>

<pre class="example">
 :: The value of odds is:
[1] 0.3678794
</pre>
</div>

<div id="outline-container-sec-1-8-2-1" class="outline-5">
<h5 id="sec-1-8-2-1"><span class="section-number-5">1.8.2.1</span> Answer</h5>
<div class="outline-text-5" id="text-1-8-2-1">
<p>
<b>Explanation</b>
</p>

<p>
Using the value of the Logit from the previous question, we have that
Odds = e^(-1) = 0.3678794.
</p>
</div>
</div>
</div>


<div id="outline-container-sec-1-8-3" class="outline-4">
<h4 id="sec-1-8-3"><span class="section-number-4">1.8.3</span> Question c</h4>
<div class="outline-text-4" id="text-1-8-3">
<p>
What is the value of P(y = 1) for this observation?
</p>

<div class="org-src-container">

<pre class="src src-R">P <span style="color: #db7093;">&lt;-</span> 1 / (1 + exp(-logit))
writeLines(<span style="color: #ff6a6a;">"\n :: The probability of P(y = 1) is:"</span>)
P
</pre>
</div>

<pre class="example">
 :: The probability of P(y = 1) is:
[1] 0.2689414
</pre>
</div>

<div id="outline-container-sec-1-8-3-1" class="outline-5">
<h5 id="sec-1-8-3-1"><span class="section-number-5">1.8.3.1</span> Answer</h5>
<div class="outline-text-5" id="text-1-8-3-1">
<p>
<b>Explanation</b>
</p>

<p>
Using the Logistic Response Function, we can compute that P(y = 1) =
1/(1 + e^(-Logit)) = 1/(1 + e^(1)) = 0.2689414.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-1-9" class="outline-3">
<h3 id="sec-1-9"><span class="section-number-3">1.9</span> Video 4: Logistic Regression in R</h3>
<div class="outline-text-3" id="text-1-9">

<div class="figure">
<p><img src="../graphs/HealthQualityModel.png" alt="HealthQualityModel.png" />
</p>
</div>

<p>
This plot shows two of our independent variables, the number of office
visits on the x-axis and the number of narcotics prescribed on the
y-axis. Each point is an observation or a patient in our data set. The
red points are patients who received poor care, and the green points
are patients who received good care.
</p>

<p>
It's hard to see a trend in the data by just visually inspecting
it. But it looks like maybe more office visits and more narcotics, or
data points to the right of this line, are more likely to have poor
care.
</p>

<p>
We'll be using the dataset <a href="https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/quality.csv">quality.csv</a> to build a logistic regression
model in R. Please download this file to follow along.
</p>

<p>
An R script file with all of the commands used in this lecture can be
downloaded <a href="https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/Unit3_ModelingExpert.R">here</a>.
</p>
</div>

<div id="outline-container-sec-1-9-1" class="outline-4">
<h4 id="sec-1-9-1"><span class="section-number-4">1.9.1</span> Download the data sets</h4>
<div class="outline-text-4" id="text-1-9-1">
<p>
In this part we can download the data
</p>

<div class="org-src-container">

<pre class="src src-R"><span style="color: #db7093;">library</span>(parallel)

<span style="color: #4682b4;">if</span>(!file.exists(<span style="color: #ff6a6a;">"../data"</span>)) {
        dir.create(<span style="color: #ff6a6a;">"../data"</span>)
}

fileUrl <span style="color: #db7093;">&lt;-</span> <span style="color: #ff6a6a;">"https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/quality.csv"</span>

fileName <span style="color: #db7093;">&lt;-</span> <span style="color: #ff6a6a;">"quality.csv"</span>

dataPath <span style="color: #db7093;">&lt;-</span> <span style="color: #ff6a6a;">"../data"</span>

filePath <span style="color: #db7093;">&lt;-</span> paste(dataPath, fileName, sep = <span style="color: #ff6a6a;">"/"</span>)

<span style="color: #4682b4;">if</span>(!file.exists(filePath)) {
        download.file(fileUrl, destfile = filePath, method = <span style="color: #ff6a6a;">"curl"</span>)
}

list.files(<span style="color: #ff6a6a;">"../data"</span>)
</pre>
</div>

<pre class="example">
 [1] "AirlinesCluster.csv"       "AnonymityPoll.csv"
 [3] "baseball.csv"              "BoeingStock.csv"
 [5] "boston.csv"                "ClaimsData.csv"
 [7] "ClaimsData.csv.zip"        "climate_change.csv"
 [9] "clinical_trial.csv"        "ClusterMeans.ods"
[11] "CocaColaStock.csv"         "CountryCodes.csv"
[13] "CPSData.csv"               "dailykos.csv"
[15] "eBayiPadTest.csv"          "eBayiPadTrain.csv"
[17] "edges.csv"                 "emails.csv"
[19] "energy_bids.csv"           "flower.csv"
[21] "FluTest.csv"               "FluTrain.csv"
[23] "framingham.csv"            "gerber.csv"
[25] "GEStock.csv"               "healthy.csv"
[27] "households.csv"            "IBMStock.csv"
[29] "intl.csv"                  "intlall.csv"
[31] "loans_imputed.csv"         "loans.csv"
[33] "MetroAreaCodes.csv"        "movieLens.txt"
[35] "murders.csv"               "mvt.csv"
[37] "mvtWeek1.csv"              "NBA_test.csv"
[39] "NBA_train.csv"             "parole.csv"
[41] "pisa2009test.csv"          "pisa2009train.csv"
[43] "PollingData_Imputed.csv"   "PollingData.csv"
[45] "PollingImputed.csv"        "ProcterGambleStock.csv"
[47] "quality.csv"               "README.md"
[49] "SampleSubmission.csv"      "songs.csv"
[51] "stevens.csv"               "StocksCluster.csv"
[53] "stopwords.txt"             "SubmissionLR2.csv"
[55] "SubmissionSimpleLogV1.csv" "tumor.csv"
[57] "tweets.csv"                "tweetsU7.csv"
[59] "USDA.csv"                  "users.csv"
[61] "WHO_Europe.csv"            "WHO.csv"
[63] "WHOu7.csv"                 "wiki.csv"
[65] "wine_test.csv"             "wine.csv"
</pre>
</div>
</div>

<div id="outline-container-sec-1-9-2" class="outline-4">
<h4 id="sec-1-9-2"><span class="section-number-4">1.9.2</span> Load the data set</h4>
<div class="outline-text-4" id="text-1-9-2">
<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"    Loading data into their data frames."</span>)
quality <span style="color: #db7093;">&lt;-</span> read.table(<span style="color: #ff6a6a;">"../data/quality.csv"</span>, sep = <span style="color: #ff6a6a;">","</span>, header = <span style="color: #3cb371;">TRUE</span>)
str(quality)
summary(quality)
</pre>
</div>

<pre class="example">
    Loading data into their data frames.
'data.frame':	131 obs. of  14 variables:
 $ MemberID            : int  1 2 3 4 5 6 7 8 9 10 ...
 $ InpatientDays       : int  0 1 0 0 8 2 16 2 2 4 ...
 $ ERVisits            : int  0 1 0 1 2 0 1 0 1 2 ...
 $ OfficeVisits        : int  18 6 5 19 19 9 8 8 4 0 ...
 $ Narcotics           : int  1 1 3 0 3 2 1 0 3 2 ...
 $ DaysSinceLastERVisit: num  731 411 731 158 449 ...
 $ Pain                : int  10 0 10 34 10 6 4 5 5 2 ...
 $ TotalVisits         : int  18 8 5 20 29 11 25 10 7 6 ...
 $ ProviderCount       : int  21 27 16 14 24 40 19 11 28 21 ...
 $ MedicalClaims       : int  93 19 27 59 51 53 40 28 20 17 ...
 $ ClaimLines          : int  222 115 148 242 204 156 261 87 98 66 ...
 $ StartedOnCombination: logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ AcuteDrugGapSmall   : int  0 1 5 0 0 4 0 0 0 0 ...
 $ PoorCare            : int  0 0 0 0 0 1 0 0 1 0 ...
    MemberID     InpatientDays       ERVisits       OfficeVisits
 Min.   :  1.0   Min.   : 0.000   Min.   : 0.000   Min.   : 0.00
 1st Qu.: 33.5   1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 7.00
 Median : 66.0   Median : 0.000   Median : 1.000   Median :12.00
 Mean   : 66.0   Mean   : 2.718   Mean   : 1.496   Mean   :13.23
 3rd Qu.: 98.5   3rd Qu.: 3.000   3rd Qu.: 2.000   3rd Qu.:18.50
 Max.   :131.0   Max.   :30.000   Max.   :11.000   Max.   :46.00
   Narcotics      DaysSinceLastERVisit      Pain         TotalVisits
 Min.   : 0.000   Min.   :  6.0        Min.   :  0.00   Min.   : 0.00
 1st Qu.: 0.000   1st Qu.:207.0        1st Qu.:  1.00   1st Qu.: 8.00
 Median : 1.000   Median :641.0        Median :  8.00   Median :15.00
 Mean   : 4.573   Mean   :480.6        Mean   : 15.56   Mean   :17.44
 3rd Qu.: 3.000   3rd Qu.:731.0        3rd Qu.: 23.00   3rd Qu.:22.50
 Max.   :59.000   Max.   :731.0        Max.   :104.00   Max.   :69.00
 ProviderCount   MedicalClaims      ClaimLines    StartedOnCombination
 Min.   : 5.00   Min.   : 11.00   Min.   : 20.0   Mode :logical
 1st Qu.:15.00   1st Qu.: 25.50   1st Qu.: 83.5   FALSE:125
 Median :20.00   Median : 37.00   Median :120.0   TRUE :6
 Mean   :23.98   Mean   : 43.24   Mean   :142.9   NA's :0
 3rd Qu.:30.00   3rd Qu.: 49.50   3rd Qu.:185.0
 Max.   :82.00   Max.   :194.00   Max.   :577.0
 AcuteDrugGapSmall    PoorCare
 Min.   : 0.000    Min.   :0.0000
 1st Qu.: 0.000    1st Qu.:0.0000
 Median : 1.000    Median :0.0000
 Mean   : 2.695    Mean   :0.2519
 3rd Qu.: 3.000    3rd Qu.:0.5000
 Max.   :71.000    Max.   :1.0000
</pre>

<p>
We'll be using the number of office visits and the number of
prescriptions for narcotics that the patient had.
</p>
</div>
</div>

<div id="outline-container-sec-1-9-3" class="outline-4">
<h4 id="sec-1-9-3"><span class="section-number-4">1.9.3</span> Data dictionary</h4>
<div class="outline-text-4" id="text-1-9-3">
<p>
The variables in the dataset quality.csv are as follows:
</p>

<ul class="org-ul">
<li><b>MemberID</b> numbers the patients from 1 to 131, and is just an
identifying number.
</li>

<li><b>InpatientDays</b> is the number of inpatient visits, or number of days
the person spent in the hospital.
</li>

<li><b>ERVisits</b> is the number of times the patient visited the emergency
room.
</li>

<li><b>OfficeVisits</b> is the number of times the patient visited any
doctor's office.
</li>

<li><b>Narcotics</b> is the number of prescriptions the patient had for
narcotics.
</li>

<li><b>DaysSinceLastERVisit</b> is the number of days between the patient's
last emergency room visit and the end of the study period (set to
the length of the study period if they never visited the ER).
</li>

<li><b>Pain</b> is the number of visits for which the patient complained
about pain.
</li>

<li><b>TotalVisits</b> is the total number of times the patient visited any
healthcare provider.
</li>

<li><b>ProviderCount</b> is the number of providers that served the patient.
</li>

<li><b>MedicalClaims</b> is the number of days on which the patient had a
medical claim.
</li>

<li><b>ClaimLines</b> is the total number of medical claims.
</li>

<li><b>StartedOnCombination</b> is whether or not the patient was started on
a combination of drugs to treat their diabetes (TRUE or FALSE).
</li>

<li><b>AcuteDrugGapSmall</b> is the fraction of acute drugs that were
refilled quickly after the prescription ran out.
</li>

<li><b>PoorCare</b> is the outcome or dependent variable, and is equal to 1
if the patient had poor care, and equal to 0 if the patient had good
care.
</li>
</ul>

<p>
In this part we learned how to use the <code>sample.split()</code> function from
the <b>caTools</b> package to split data for a classification problem,
balancing the positive and negative observations in the training and
testing sets.
</p>

<p>
If you wanted to instead split a data frame data, where the dependent
variable is a continuous outcome (this was the case for all the
datasets we used last week), you could instead use the <code>sample()</code>
function. Here is how to select \(70\%\) of observations for the training
set (called <b>train</b>) and \(30\%\) of observations for the testing set
(called <b>test</b>):
</p>

<p>
<code>spl = sample(1:nrow(data), size=0.7 * nrow(data))</code>
</p>

<p>
<code>train = data[spl,]</code>
</p>

<p>
<code>test = data[-spl,]</code>
</p>
</div>
</div>

<div id="outline-container-sec-1-9-4" class="outline-4">
<h4 id="sec-1-9-4"><span class="section-number-4">1.9.4</span> Logistic Regression model building</h4>
<div class="outline-text-4" id="text-1-9-4">
<p>
In a classification problem, a standard baseline method is to just
predict the most frequent outcome for all observations.
</p>

<p>
Since good care is more common than poor care, in this case, we would
predict that all patients are receiving good care. If we did this, we
would get \(98\) out of the \(131\) observations correct, or have an accuracy
of about \(75\%\).
</p>

<p>
So our baseline model has an accuracy of \(75\%\). This is what we'll
try to beat with our logistic regression model.
</p>

<p>
we only have one data set. So we want to randomly split our data set
into a training set and testing set so that we'll have a test set to
measure our out-of-sample accuracy.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Install and load caTools package (Only once)"</span>)
<span style="color: #9932cc;">## </span><span style="color: #ba55d3;">install.packages('caTools', repos='http://cran.rstudio.com/')</span>
<span style="color: #db7093;">library</span>(caTools)

writeLines(<span style="color: #ff6a6a;">"\n :: Randomly split data"</span>)
set.seed(88)
split <span style="color: #db7093;">&lt;-</span> sample.split(quality$PoorCare, SplitRatio = 0.75)
head(split)
</pre>
</div>

<pre class="example">
 :: Install and load caTools package (Only once)

 :: Randomly split data
[1]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE
</pre>

<p>
Since sample.split randomly splits your data, it could split it
differently for each of us. To make sure that we all get the same
split, we'll set our seed. This initializes the random number
generator.
</p>

<p>
<code>Sample.split</code> randomly splits the data. But it also makes sure that
the outcome variable is well-balanced in each piece. We saw earlier
that about \(75\%\) of our patients are receiving good care.
</p>

<p>
This function makes sure that in our training set, \(75\%\) of our patients
are receiving good care and in our testing set \(75\%\) of our patients are
receiving good care.
</p>
</div>
</div>

<div id="outline-container-sec-1-9-5" class="outline-4">
<h4 id="sec-1-9-5"><span class="section-number-4">1.9.5</span> Building the training and the testing sets</h4>
<div class="outline-text-4" id="text-1-9-5">
<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Create training and testing sets"</span>)
qualityTrain <span style="color: #db7093;">&lt;-</span> subset(quality, split == <span style="color: #3cb371;">TRUE</span>)
qualityTest <span style="color: #db7093;">&lt;-</span> subset(quality, split == <span style="color: #3cb371;">FALSE</span>)

writeLines(<span style="color: #ff6a6a;">"\n :: The number of observations in the training set"</span>)
nrow(qualityTrain)

writeLines(<span style="color: #ff6a6a;">"\n :: The number of observations in the testing set"</span>)
nrow(qualityTest)
</pre>
</div>

<pre class="example">
 :: Create training and testing sets

 :: The number of observations in the training set
[1] 99

 :: The number of observations in the testing set
[1] 32
</pre>

<p>
We are ready to build a logistic regression model using <b>OfficeVisits</b>
and <b>Narcotics</b> as independent variables.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: First Logistic Regression Model"</span>)
QualityLog <span style="color: #db7093;">&lt;-</span> glm(PoorCare ~ OfficeVisits + Narcotics,
                  data=qualityTrain, family = binomial)
summary(QualityLog)
</pre>
</div>

<pre class="example">
 :: First Logistic Regression Model

Call:
glm(formula = PoorCare ~ OfficeVisits + Narcotics, family = binomial,
    data = qualityTrain)

Deviance Residuals:
     Min        1Q    Median        3Q       Max
-2.06303  -0.63155  -0.50503  -0.09689   2.16686

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -2.64613    0.52357  -5.054 4.33e-07 ***
OfficeVisits  0.08212    0.03055   2.688  0.00718 **
Narcotics     0.07630    0.03205   2.381  0.01728 *
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 111.888  on 98  degrees of freedom
Residual deviance:  89.127  on 96  degrees of freedom
AIC: 95.127

Number of Fisher Scoring iterations: 4
</pre>

<p>
This gives the estimate values for the coefficients, or the betas, for
our logistic regression model. We see here that the coefficients for
<b>OfficeVisits</b> and <b>Narcotics</b> are both positive, which means that higher
values in these two variables are indicative of poor care as we
suspected from looking at the data.
</p>

<p>
We also see that both of these variables have at least one star,
meaning that they're significant in our model.
</p>

<p>
The preferred model is the one with the minimum <b>AIC</b>.
</p>
</div>
</div>

<div id="outline-container-sec-1-9-6" class="outline-4">
<h4 id="sec-1-9-6"><span class="section-number-4">1.9.6</span> Predictions in the training set</h4>
<div class="outline-text-4" id="text-1-9-6">
<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Make predictions on training set"</span>)
predictTrain <span style="color: #db7093;">&lt;-</span> predict(QualityLog, type = <span style="color: #ff6a6a;">"response"</span>)
</pre>
</div>

<pre class="example">
:: Make predictions on training set
</pre>

<p>
The second argument which is type="response". This tells the predict
function to give us probabilities. Let's take a look at the
statistical summary of our predictions.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Analyze predictions"</span>)
summary(predictTrain)
tapply(predictTrain, qualityTrain$PoorCare, mean)
</pre>
</div>

<pre class="example">
 :: Analyze predictions
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
0.06623 0.11910 0.15970 0.25250 0.26760 0.98460
        0         1
0.1894512 0.4392246
</pre>

<p>
Since we're expecting probabilities, all of the numbers should be
between zero and one. And we see that the minimum value is about
\(0.07\) and the maximum value is \(0.98\).
</p>

<p>
Let's see if we're predicting higher probabilities for the actual poor
care cases as we expect. Using the <code>tapply</code> function. So we see that
for all of the <b>true poor care</b> cases, we predict an average probability
of about \(0.44\). And all of the <b>true good care</b> cases, we predict an
average probability of about \(0.19\).
</p>

<p>
<b>So this is a good sign, because it looks like we're predicting a
 higher probability for the actual poor care cases</b>.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-1-10" class="outline-3">
<h3 id="sec-1-10"><span class="section-number-3">1.10</span> Quick Question (1 point possible)</h3>
<div class="outline-text-3" id="text-1-10">
<p>
In R, create a logistic regression model to predict "PoorCare" using
the independent variables "StartedOnCombination" and
"ProviderCount". Use the training set we created in the previous video
to build the model.
</p>

<p>
Note: If you haven't already loaded and split the data in R, please
run these commands in your R console to load and split the data
set. Remember to first navigate to the directory where you have saved
"quality.csv".
</p>

<p>
<code>quality = read.csv("quality.csv")</code>
</p>

<p>
<code>install.packages("caTools")</code>
</p>

<p>
<code>library(caTools)</code>
</p>

<p>
<code>set.seed(88)</code>
</p>

<p>
<code>split = sample.split(quality$PoorCare, SplitRatio = 0.75)</code>
</p>

<p>
<code>qualityTrain = subset(quality, split == TRUE)</code>
</p>

<p>
<code>qualityTest = subset(quality, split == FALSE)</code>
</p>

<p>
Then recall that we built a logistic regression model to predict
PoorCare using the R command:
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: create a logistic regression model to predict PoorCare using</span>
<span style="color: #ff6a6a;">the independent variables StartedOnCombination and ProviderCount:"</span>)
QualityLog2 <span style="color: #db7093;">&lt;-</span> glm(PoorCare ~ StartedOnCombination + ProviderCount,
                   data = qualityTrain, family = binomial)
summary(QualityLog2)
</pre>
</div>

<pre class="example">
 :: create a logistic regression model to predict PoorCare using
the independent variables StartedOnCombination and ProviderCount:

Call:
glm(formula = PoorCare ~ StartedOnCombination + ProviderCount,
    family = binomial, data = qualityTrain)

Deviance Residuals:
     Min        1Q    Median        3Q       Max
-1.61826  -0.72782  -0.64555  -0.08407   1.94662

Coefficients:
                         Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)              -2.00097    0.55097  -3.632 0.000282 ***
StartedOnCombinationTRUE  1.95230    1.22342   1.596 0.110541
ProviderCount             0.03366    0.01983   1.697 0.089706 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 111.89  on 98  degrees of freedom
Residual deviance: 104.37  on 96  degrees of freedom
AIC: 110.37

Number of Fisher Scoring iterations: 4
</pre>

<p>
You will need to adjust this command to answer this question, and then
look at the <code>summary(QualityLog)</code> output.
</p>

<p>
What is the coefficient for <b>StartedOnCombination</b>?
</p>
</div>

<div id="outline-container-sec-1-10-1" class="outline-4">
<h4 id="sec-1-10-1"><span class="section-number-4">1.10.1</span> Answer</h4>
<div class="outline-text-4" id="text-1-10-1">
<p>
If you look at the output of <code>summary(Model)</code>, the value of the
coefficient (Estimate) for <code>StartedOnCombination</code> is \(1.95230\).
</p>
</div>
</div>
</div>

<div id="outline-container-sec-1-11" class="outline-3">
<h3 id="sec-1-11"><span class="section-number-3">1.11</span> Quick Question (1 point possible)</h3>
<div class="outline-text-3" id="text-1-11">
<p>
<b>StartedOnCombination</b> is a binary variable, which equals \(1\) if the
patient is started on a combination of drugs to treat their diabetes,
and equals \(0\) if the patient is not started on a combination of
drugs. All else being equal, does this model imply that starting a
patient on a combination of drugs is indicative of poor care, or good
care?
</p>
</div>

<div id="outline-container-sec-1-11-1" class="outline-4">
<h4 id="sec-1-11-1"><span class="section-number-4">1.11.1</span> Answer</h4>
<div class="outline-text-4" id="text-1-11-1">
<p>
<b>Explanation</b>
</p>

<p>
The coefficient value is positive, meaning that positive values of the
variable make the outcome of \(1\) more likely. This corresponds to Poor
Care.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-1-12" class="outline-3">
<h3 id="sec-1-12"><span class="section-number-3">1.12</span> Video 5: Thresholding</h3>
<div class="outline-text-3" id="text-1-12">
<p>
Often, we want to make an actual prediction.  Should we predict \(1\) for
<b>poor care</b>, or should we predict \(0\) for <b>good care</b>? We can convert the
probabilities to predictions using what's called a threshold value, \(t\).
</p>

<p>
If the probability of <b>poor care</b> is greater than this threshold value,
\(t\), we predict <b>poor quality care</b>. But if the probability of <b>poor care</b>
is less than the threshold value, \(t\), then we predict <b>good quality
care</b>.
</p>


<div class="figure">
<p><img src="../graphs/ThresholdValue.png" alt="ThresholdValue.png" />
</p>
</div>

<p>
<b>What value should we pick for the threshold, t?</b>
</p>

<p>
The threshold value, \(t\), is often selected based on which errors are
better. You might be thinking that making no errors is better, which
is, of course, true.
</p>

<p>
But it's rare to have a model that predicts perfectly, so you're bound
to make some errors. There are two types of errors that a model can
make &#x2013;ones where you predict \(1\), or <b>poor care</b>, but the actual
outcome is \(0\), and ones where you predict \(0\), or good care, but the
actual outcome is \(1\).
</p>


<div class="figure">
<p><img src="../graphs/ThresholdValue02.png" alt="ThresholdValue02.png" />
</p>
</div>

<ul class="org-ul">
<li>The large \(t\) selection approach would detect the patients receiving
</li>
</ul>
<p>
the worst care and prioritize them for intervention.
</p>

<ul class="org-ul">
<li>The small \(t\) selection approach would detect all patients who might
</li>
</ul>
<p>
be receiving poor care.
</p>

<p>
Some decision-makers often have a preference for one type of error
over the other, which should influence the threshold value they pick.
</p>
</div>
</div>

<div id="outline-container-sec-1-13" class="outline-3">
<h3 id="sec-1-13"><span class="section-number-3">1.13</span> The confusion matrix or classification matrix</h3>
<div class="outline-text-3" id="text-1-13">

<div class="figure">
<p><img src="../graphs/ThresholdValue03.png" alt="ThresholdValue03.png" />
</p>
</div>

<p>
The rows are labeled with the actual outcome, and the columns are
labeled with the predicted outcome.
</p>

<p>
Each entry of the table gives the number of data observations that
fall into that category. So the number of <b>true negatives</b>, or <b>TN</b>, is
the number of observations that are actually good care and for which
we predict good care.
</p>

<p>
The <b>true positives</b>, or <b>TP</b>, is the number of observations that are
actually poor care and for which we predict poor care. These are the
two types that we get correct.
</p>

<p>
The <b>false positives</b>, or <b>FP</b>, are the number of data points for which we
predict poor care, but they're actually good care. And the <b>false
negatives</b>, or FN, are the number of data points for which we predict
good care, but they're actually poor care.
</p>

<ul class="org-ul">
<li>The Sensitivity is often called <b>the true positive rate</b> and
measures the percentage of actual poor care cases that we classify
correctly.
</li>

<li>The Specificity is called <b>the true negative rate</b> and measures the
percentage of actual good care cases that we classify correctly.
</li>
</ul>

<p>
A model with a <b>higher threshold</b> will have a <b>lower sensitivity</b> and a
<b>higher specificity</b>. A model with a <b>lower threshold</b> will have a higher
<b>sensitivity</b> and a lower <b>specificity</b>.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Confusion matrix for threshold of 0.5:"</span>)
table(qualityTrain$PoorCare, predictTrain &gt; 0.5)

writeLines(<span style="color: #ff6a6a;">"\n :: Sensitivity:"</span>)
10/25

writeLines(<span style="color: #ff6a6a;">"\n :: Specificity:"</span>)
70/74
</pre>
</div>

<pre class="example">
 :: Confusion matrix for threshold of 0.5:

    FALSE TRUE
  0    70    4
  1    15   10

 :: Sensitivity:
[1] 0.4

 :: Specificity:
[1] 0.9459459
</pre>

<p>
So you can see here that for \(70\) cases, we <b>predict good care</b> and they
actually <b>received good care</b>, and for \(10\) cases, we <b>predict poor care</b>,
and they actually <b>received poor care</b>.
</p>

<p>
We make \(4\) mistakes where <b>we say poor care</b> and it's actually <b>good
care</b>, and we make \(15\) mistakes where <b>we say good care</b>, but it's
<b>actually poor care</b>.
</p>

<p>
Now we can experiment with a higher threshold:
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Confusion matrix for threshold of 0.7"</span>)
table(qualityTrain$PoorCare, predictTrain &gt; 0.7)

writeLines(<span style="color: #ff6a6a;">"\n :: Sensitivity:"</span>)
8/25

writeLines(<span style="color: #ff6a6a;">"\n :: Specificity:"</span>)
73/74
</pre>
</div>

<pre class="example">
 :: Confusion matrix for threshold of 0.7

    FALSE TRUE
  0    73    1
  1    17    8

 :: Sensitivity:
[1] 0.32

 :: Specificity:
[1] 0.9864865
</pre>

<p>
So by increasing the threshold, our sensitivity went down and our
specificity went up.
</p>

<p>
If now we choose a small threshold:
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Confusion matrix for threshold of 0.2"</span>)
table(qualityTrain$PoorCare, predictTrain &gt; 0.2)

writeLines(<span style="color: #ff6a6a;">"\n :: Sensitivity:"</span>)
16/25

writeLines(<span style="color: #ff6a6a;">"\n :: Specificity:"</span>)
54/74
</pre>
</div>

<pre class="example">
 :: Confusion matrix for threshold of 0.2

    FALSE TRUE
  0    54   20
  1     9   16

 :: Sensitivity:
[1] 0.64

 :: Specificity:
[1] 0.7297297
</pre>

<p>
So with the lower threshold, our sensitivity went up, and our
specificity went down.
</p>

<p>
<b>But which threshold should we pick?</b> Maybe \(0.4\) is better, or
 \(0.6\). How do we decide?
</p>
</div>
</div>

<div id="outline-container-sec-1-14" class="outline-3">
<h3 id="sec-1-14"><span class="section-number-3">1.14</span> Confusion matrices questions</h3>
<div class="outline-text-3" id="text-1-14">
<p>
This question asks about the following two confusion matrices:
</p>
</div>

<div id="outline-container-sec-1-14-1" class="outline-4">
<h4 id="sec-1-14-1"><span class="section-number-4">1.14.1</span> Confusion Matrix #1:</h4>
<div class="outline-text-4" id="text-1-14-1">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="right" />

<col  class="right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">&#xa0;</th>
<th scope="col" class="right">Predicted = 0</th>
<th scope="col" class="right">Predicted = 1</th>
</tr>
</thead>
<tbody>
<tr>
<td class="left">Actual = 0</td>
<td class="right">15</td>
<td class="right">10</td>
</tr>

<tr>
<td class="left">Actual = 1</td>
<td class="right">5</td>
<td class="right">20</td>
</tr>
</tbody>
</table>

<div class="org-src-container">

<pre class="src src-R">TP <span style="color: #db7093;">&lt;-</span> 20; TN <span style="color: #db7093;">&lt;-</span> 15; FP <span style="color: #db7093;">&lt;-</span> 10; FN <span style="color: #db7093;">&lt;-</span> 5;
writeLines(<span style="color: #ff6a6a;">"\n :: Sensitivity:"</span>)
Sensitivity <span style="color: #db7093;">&lt;-</span> TP / (TP + FN)
Sensitivity

writeLines(<span style="color: #ff6a6a;">"\n :: Specificity:"</span>)
Specificity <span style="color: #db7093;">&lt;-</span> TN / (TN + FP)
Specificity
</pre>
</div>

<pre class="example">
 :: Sensitivity:
[1] 0.8

 :: Specificity:
[1] 0.6
</pre>
</div>

<div id="outline-container-sec-1-14-1-1" class="outline-5">
<h5 id="sec-1-14-1-1"><span class="section-number-5">1.14.1.1</span> Answer</h5>
<div class="outline-text-5" id="text-1-14-1-1">
<p>
<b>Explanation</b>
</p>

<p>
The <b>sensitivity</b> of a confusion matrix is the true positives, divided
by the true positives plus the false negatives. In this case, it is
20/(20+5) = 0.8
</p>

<p>
<b>Explanation</b>
</p>

<p>
The <b>specificity</b> of a confusion matrix is the true negatives, divided
by the true negatives plus the false positives. In this case, it is
15/(15+10) = 0.6
</p>
</div>
</div>
</div>



<div id="outline-container-sec-1-14-2" class="outline-4">
<h4 id="sec-1-14-2"><span class="section-number-4">1.14.2</span> Confusion Matrix #2:</h4>
<div class="outline-text-4" id="text-1-14-2">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="right" />

<col  class="right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">&#xa0;</th>
<th scope="col" class="right">Predicted = 0</th>
<th scope="col" class="right">Predicted = 1</th>
</tr>
</thead>
<tbody>
<tr>
<td class="left">Actual = 0</td>
<td class="right">20</td>
<td class="right">5</td>
</tr>

<tr>
<td class="left">Actual = 1</td>
<td class="right">10</td>
<td class="right">15</td>
</tr>
</tbody>
</table>

<div class="org-src-container">

<pre class="src src-R">TP <span style="color: #db7093;">&lt;-</span> 15; TN <span style="color: #db7093;">&lt;-</span> 20; FP <span style="color: #db7093;">&lt;-</span> 5; FN <span style="color: #db7093;">&lt;-</span> 10;
writeLines(<span style="color: #ff6a6a;">"\n :: Sensitivity:"</span>)
Sensitivity <span style="color: #db7093;">&lt;-</span> TP / (TP + FN)
Sensitivity

writeLines(<span style="color: #ff6a6a;">"\n :: Specificity:"</span>)
Specificity <span style="color: #db7093;">&lt;-</span> TN / (TN + FP)
Specificity
</pre>
</div>

<pre class="example">
 :: Sensitivity:
[1] 0.6

 :: Specificity:
[1] 0.8
</pre>
</div>

<div id="outline-container-sec-1-14-2-1" class="outline-5">
<h5 id="sec-1-14-2-1"><span class="section-number-5">1.14.2.1</span> Answer</h5>
<div class="outline-text-5" id="text-1-14-2-1">
<p>
The <b>Sensitivity</b> in the matrix 1 was \(0.8\) and was \(0.6\) in the
second matrix, then the threshold went up.
</p>

<p>
<b>Explanation</b>
</p>

<p>
We predict the outcome 1 less often in Confusion Matrix #2. This means
we must have increased the threshold.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-1-15" class="outline-3">
<h3 id="sec-1-15"><span class="section-number-3">1.15</span> Video 6: ROC Curves</h3>
<div class="outline-text-3" id="text-1-15">
<p>
Picking a good threshold value is often challenging. A Receiver
Operator Characteristic curve, or ROC curve, can help you decide which
value of the threshold is best.
</p>


<div class="figure">
<p><img src="../graphs/ROC.png" alt="ROC.png" />
</p>
</div>

<p>
The sensitivity or true positive rate is in the \(y\) axis and the false
positive rate, or 1 minus the specificity, is given on the x-axis.
</p>

<ul class="org-ul">
<li>The line shows how these two outcome measures vary with different
threshold values. The ROC curve always starts at the point \((0,
  0)\). This corresponds to a threshold value of \(1\). If you have a
threshold of \(1\), you will not catch any poor care cases, or have a
sensitivity of \(0\). But you will correctly label of all the good
care cases, meaning you have a false positive rate of \(0\).
</li>

<li>The ROC curve always ends at the point \((1, 1)\), which corresponds
to a threshold value of \(0\). If you have a threshold of \(0\), you'll
catch all of the poor care cases, or have a sensitivity of \(1\), but
you'll label all of the good care cases as poor care cases too,
meaning you have a false positive rate of \(1\).
</li>

<li>The threshold decreases as you move from \((0, 0)\) to \((1, 1)\). At
the point \((0, 0.4)\), or about here, you're correctly labeling about
\(40\%\) of the poor care cases with a very small false positive
rate.
</li>

<li>On the other hand, at the point \((0.6, 0.9)\), you're correctly
labeling about \(90\%\) of the poor care cases, but have a false
positive rate of \(60\%\).
</li>

<li>In the middle, around \((0.3, 0.8)\), you're correctly labeling about
80% of the poor care cases, with a \(30\%\) false positive rate.
</li>
</ul>


<div class="figure">
<p><img src="../graphs/ROC02.png" alt="ROC02.png" />
</p>
</div>

<ul class="org-ul">
<li>The <b>higher the threshold</b>, or closer to \((0, 0)\), the <b>higher</b> the
<b>specificity</b> and the <b>lower</b> the <b>sensitivity</b>. The <b>lower the
threshold</b>,   or closer to \((1,1)\), the <b>higher</b> the <b>sensitivity</b>
and lower the <b>specificity</b>.
</li>
</ul>

<p>
<b>So which threshold value should you pick?</b> You should select the best
 threshold for the trade-off you want to make.
</p>

<ul class="org-ul">
<li>If you're more concerned with having a <b>high specificity</b> or <b>low
false positive rate</b>, pick the threshold that <b>maximizes the true
positive rate</b> while <b>keeping the false positive rate really low</b>. A
threshold around \((0.1, 0.5)\) on this ROC curve looks like a good
choice in this case.
</li>

<li>On the other hand, if you're more concerned with having a <b>high
sensitivity</b> or <b>high true positive rate</b>, pick a threshold that
<b>minimizes the false positive rate but has a very high true positive
rate</b>. A threshold around \((0.3, 0.8)\) looks like a good choice in
this case.
</li>
</ul>


<div class="figure">
<p><img src="../graphs/ROC03.png" alt="ROC03.png" />
</p>
</div>

<p>
Recall that we made predictions on our training set and called them
<b>predictTrain</b>. We'll use these predictions to create our ROC
curve. First, we'll call the <b>prediction function of ROCR</b>. We'll call
the output of this function <b>ROCRpred</b>, and then use the prediction
function.
</p>

<p>
This function takes two arguments. The first is the predictions we
made with our model, which we called <b>predictTrain</b>. The second argument
is the true outcomes of our data points, which in our case, is
<b>qualityTrain$PoorCare</b>.
</p>

<p>
Now, we need to use the <b>performance function</b>. This defines what we'd
like to plot on the x and y-axes of our ROC curve. We'll call the
output of this <b>ROCRperf</b>, and use the performance function, which takes
as arguments the <b>output of the prediction function</b>, and then what we
want on the x and y-axes.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Install package only once"</span>)
<span style="color: #9932cc;">## </span><span style="color: #ba55d3;">install.packages('ROCR', repos='http://cran.rstudio.com/')</span>
<span style="color: #db7093;">library</span>(ROCR)

writeLines(<span style="color: #ff6a6a;">"\n :: Prediction function"</span>)
ROCRpred = prediction(predictTrain, qualityTrain$PoorCare)

writeLines(<span style="color: #ff6a6a;">"\n :: Performance function"</span>)
ROCRperf = performance(ROCRpred, <span style="color: #ff6a6a;">"tpr"</span>, <span style="color: #ff6a6a;">"fpr"</span>)
</pre>
</div>

<pre class="example">
 :: Install package only once
Loading required package: gplots

Attaching package: ‘gplots’

The following object is masked from ‘package:stats’:

    lowess

 :: Prediction function

 :: Performance function
</pre>


<div id="fig:ThresholdLabelsPlot" class="figure">
<p><img src="../graphs/ThresholdLabelsPlot.png" alt="ThresholdLabelsPlot.png" />
</p>
<p><span class="figure-number">Figure 22:</span> Add threshold labels to better pick up a right value of t</p>
</div>
</div>
</div>

<div id="outline-container-sec-1-16" class="outline-3">
<h3 id="sec-1-16"><span class="section-number-3">1.16</span> Quick Question (2 points possible)</h3>
<div class="outline-text-3" id="text-1-16">
</div><div id="outline-container-sec-1-16-1" class="outline-4">
<h4 id="sec-1-16-1"><span class="section-number-4">1.16.1</span> Question a</h4>
<div class="outline-text-4" id="text-1-16-1">
<p>
Given this ROC curve, which threshold would you pick if you wanted to
correctly identify a small group of patients who are receiving the
worst care with high confidence?
</p>
</div>

<div id="outline-container-sec-1-16-1-1" class="outline-5">
<h5 id="sec-1-16-1-1"><span class="section-number-5">1.16.1.1</span> Answer</h5>
<div class="outline-text-5" id="text-1-16-1-1">
<p>
<b>Explanation</b>
</p>

<p>
The threshold \(0.7\) is best to identify a small group of patients who
are receiving the worst care with high confidence, since at this
threshold we make very few false positive mistakes, and identify about
35% of the true positives.
</p>

<p>
The threshold \(t = 0.8\) is not a good choice, since it makes about the
same number of false positives, but only identifies \(10\%\) of the true
positives. The thresholds \(0.2\) and \(0.3\) both identify more of the true
positives, but they make more false positive mistakes, so our
confidence decreases.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-1-16-2" class="outline-4">
<h4 id="sec-1-16-2"><span class="section-number-4">1.16.2</span> Question b</h4>
<div class="outline-text-4" id="text-1-16-2">
<p>
Which threshold would you pick if you wanted to correctly identify
half of the patients receiving poor care, while making as few errors
as possible?
</p>
</div>

<div id="outline-container-sec-1-16-2-1" class="outline-5">
<h5 id="sec-1-16-2-1"><span class="section-number-5">1.16.2.1</span> Answer</h5>
<div class="outline-text-5" id="text-1-16-2-1">
<p>
<b>Explanation</b>
</p>

<p>
The threshold \(0.3\) is the best choice in this scenerio. The threshold
\(0.2\) also identifies over half of the patients receiving poor care,
but it makes many more false positive mistakes. The thresholds \(0.7\)
and \(0.8\) don't identify at least half of the patients receiving poor
care.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-1-17" class="outline-3">
<h3 id="sec-1-17"><span class="section-number-3">1.17</span> Video 7: Interpreting the Model</h3>
<div class="outline-text-3" id="text-1-17">
<p>
<b>Multicollinearity</b> occurs when the various independent variables are
correlated, and this might confuse the coefficients&#x2013; the betas&#x2013; in
the model. So tests to address that involve checking the correlations
of independent variables.
</p>

<p>
If they are excessively high, this would mean that there might be
multicollinearity, and you have to potentially revisit the model, as
well as whether the signs of the coefficients make sense.
</p>


<div class="figure">
<p><img src="../graphs/Multicollinearity.png" alt="Multicollinearity.png" />
</p>
</div>

<p>
Is the coefficient beta positive or negative? If it agrees with
intuition, then multicollinearity has not been a problem, but if
intuition suggests a different sign, this might be a sign of
multicollinearity.
</p>

<p>
The next important element is <b>significance</b>. So how do we interpret the
results, and how do we understand whether we have a good model or not?
For that purpose, let's take a look at what is called Area Under the
Curve, or AUC for short.
</p>


<div class="figure">
<p><img src="../graphs/ROCArea.png" alt="ROCArea.png" />
</p>
</div>

<p>
So the <b>Area Under the Curve</b> shows an absolute measure of quality of
prediction&#x2013; in this particular case, \(77.5\%\), which means that,
given that the perfect score is \(100\%\), so this is like a B, whereas,
as we'll see later, a \(50\%\) score, which is pure guessing, is a
\(50\%\) rate of success.
</p>

<p>
So the area under the curve gives an absolute measure of quality, and
it's less affected by various benchmarks. So it illustrates how
accurate the model is on a more absolute sense.
</p>
</div>

<div id="outline-container-sec-1-17-1" class="outline-4">
<h4 id="sec-1-17-1"><span class="section-number-4">1.17.1</span> So what is a good AUC?</h4>
<div class="outline-text-4" id="text-1-17-1">
<p>
The area on the right shows the maximum possible of a perfect
prediction, whereas the area on this curve now&#x2013; it is \(0.5\), and it's
pure guessing. Other outcome measures that are important for us to
discuss is the so-called confusion matrix.
</p>


<div class="figure">
<p><img src="../graphs/GoodAUC.png" alt="GoodAUC.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-1-17-2" class="outline-4">
<h4 id="sec-1-17-2"><span class="section-number-4">1.17.2</span> Other outcome measures</h4>
<div class="outline-text-4" id="text-1-17-2">
<p>
Other outcome measures that are important for us to discuss
is the so-called confusion matrix.
</p>


<div class="figure">
<p><img src="../graphs/OutcomeMeasures.png" alt="OutcomeMeasures.png" />
</p>
</div>

<p>
The <b>actual class</b> is \(0\) means, in our example, <b>good quality of care</b>,
and <b>actual</b> \(class = 1\) means <b>poor quality of care</b>, whereas the <b>predicted</b>
\(class = 0\) means that will <b>predict good quality</b>, and the <b>predicted</b>
\(class = 1\) means that we predict <b>poor quality</b>.
</p>

<p>
So if \(N\) is the number of observations, the <b>overall accuracy</b> is
basically the number of true negatives and true positives divided by
\(N\). It's basically the terms in the diagonal of this two by two matrix
divided by the total observations.
</p>

<p>
The <b>overall error</b> rate is the terms off-diagonal&#x2013; the false
positives, plus the false negatives, divided by the total number of
observations.
</p>

<p>
An important component is the so-called <b>sensitivity</b>, and sensitivity
is TP, the true positives, whenever we predict poor quality, and
indeed it is poor quality, divided by TP, these true positives, plus
FN, which is the total number of cases of poor quality.
</p>

<p>
So this is the total number of times that we predict poor quality, and
it is, indeed, poor quality, versus the total number of times the
actual quality is, in fact, poor.
</p>

<p>
And specificity is TN, true negatives, the number of times we predict
the quality is good, and, in fact, the quality is good, divided by
this number, TN, plus false positives.
</p>
</div>
</div>

<div id="outline-container-sec-1-17-3" class="outline-4">
<h4 id="sec-1-17-3"><span class="section-number-4">1.17.3</span> Making predictions</h4>
<div class="outline-text-4" id="text-1-17-3">
<p>
So in our test, we utilized 32 cases, and the R command that makes the
statements about the quality of a prediction out-of-sample is
illustrated here in the slide.
</p>

<p>
So in that way, we make predictions about probabilities, of course,
simply because logistic regression makes predictions about
probabilities, and then we transform them to a binary outcome&#x2013; the
quality is good, or the quality is poor&#x2013; using a threshold.
</p>

<p>
In this particular example, we used a threshold value of \(0.3\), and in
doing so, we obtain the following confusion matrix.
</p>


<div class="figure">
<p><img src="../graphs/Predictions.png" alt="Predictions.png" />
</p>
</div>

<div class="org-src-container">

<pre class="src src-R">predictTest <span style="color: #db7093;">&lt;-</span> predict(QualityLog, type = <span style="color: #ff6a6a;">"response"</span>, newdata = qualityTest)

writeLines(<span style="color: #ff6a6a;">"\n :: Confusion matrix for threshold of 0.3:"</span>)
table(qualityTest$PoorCare, predictTest &gt; 0.3)

TN <span style="color: #db7093;">&lt;-</span> 19; TP <span style="color: #db7093;">&lt;-</span> 6; FN <span style="color: #db7093;">&lt;-</span> 2; FP <span style="color: #db7093;">&lt;-</span> 5

writeLines(<span style="color: #ff6a6a;">"\n :: Overall accuracy:"</span>)
OverallAccuracy <span style="color: #db7093;">&lt;-</span> (TN + TP) / nrow(qualityTest)
OverallAccuracy

writeLines(<span style="color: #ff6a6a;">"\n :: Sensitivity:"</span>)
Sensitivity <span style="color: #db7093;">&lt;-</span> TP / (TP + FN)
Sensitivity

writeLines(<span style="color: #ff6a6a;">"\n :: Specificity:"</span>)
Specificity <span style="color: #db7093;">&lt;-</span> TN / (TN + FP)
Specificity

writeLines(<span style="color: #ff6a6a;">"\n :: Overall error rate:"</span>)
OverallErrorRate <span style="color: #db7093;">&lt;-</span> (FP + FN) / nrow(qualityTest)
OverallErrorRate

writeLines(<span style="color: #ff6a6a;">"\n :: False Negative Error Rate:"</span>)
FalseNegativeErrorRate <span style="color: #db7093;">&lt;-</span> FN / (TP + FN)
FalseNegativeErrorRate

writeLines(<span style="color: #ff6a6a;">"\n :: False Positive Error Rate:"</span>)
FalsePositiveErrorRate <span style="color: #db7093;">&lt;-</span> FP / ( TN + FP)
FalsePositiveErrorRate
</pre>
</div>

<pre class="example">
 :: Confusion matrix for threshold of 0.3:

    FALSE TRUE
  0    19    5
  1     2    6

 :: Overall accuracy:
[1] 0.78125

 :: Sensitivity:
[1] 0.75

 :: Specificity:
[1] 0.7916667

 :: Overall error rate:
[1] 0.21875

 :: False Negative Error Rate:
[1] 0.25

 :: False Positive Error Rate:
[1] 0.2083333
</pre>

<p>
Notice, if you compare this model with making always&#x2013; let's say one
alternative is to say we predict good care all the time. In that
situation, we will be correct 19 plus 5, 24 times, versus 25 times, in
our case. But notice that predicting always good care does not capture
the dynamics of what is happening, versus the logistic regression
model that is far more intelligent in capturing these effects.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-1-18" class="outline-3">
<h3 id="sec-1-18"><span class="section-number-3">1.18</span> Quick Question (1 point possible)</h3>
<div class="outline-text-3" id="text-1-18">
<p>
IMPORTANT NOTE: This question uses the original model with the
independent variables <b>OfficeVisits</b> and <b>Narcotics</b>. Be sure to use
this model, instead of the model you built in Quick Question 4.
</p>

<p>
Compute the test set predictions in R by running the command:
</p>

<p>
<code>predictTest = predict(QualityLog, type="response", newdata=qualityTest)</code>
</p>

<p>
You can compute the test set AUC by running the following two commands
in R:
</p>

<div class="org-src-container">

<pre class="src src-R">ROCRpredTest <span style="color: #db7093;">&lt;-</span> prediction(predictTest, qualityTest$PoorCare)
auc <span style="color: #db7093;">&lt;-</span> as.numeric(performance(ROCRpredTest, <span style="color: #ff6a6a;">"auc"</span>)@y.values)

writeLines(<span style="color: #ff6a6a;">"\n :: The AUC of the test set is:"</span>)
auc
</pre>
</div>

<pre class="example">
 :: The AUC of the test set is:
[1] 0.7994792
</pre>
</div>

<div id="outline-container-sec-1-18-1" class="outline-4">
<h4 id="sec-1-18-1"><span class="section-number-4">1.18.1</span> Question a</h4>
<div class="outline-text-4" id="text-1-18-1">
<p>
What is the AUC of this model on the test set?
</p>
</div>

<div id="outline-container-sec-1-18-1-1" class="outline-5">
<h5 id="sec-1-18-1-1"><span class="section-number-5">1.18.1.1</span> Answer</h5>
<div class="outline-text-5" id="text-1-18-1-1">
<p>
$$
AUC = 0.7994792
$$
</p>

<p>
The AUC of a model has the following nice interpretation: given a
random patient from the dataset who actually received poor care, and a
random patient from the dataset who actually received good care, the
AUC is the perecentage of time that our model will classify which is
which correctly.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-1-19" class="outline-3">
<h3 id="sec-1-19"><span class="section-number-3">1.19</span> Video 8: The Analytics Edge</h3>
<div class="outline-text-3" id="text-1-19">
<p>
<b>Conclusions</b>
</p>

<ul class="org-ul">
<li>An expert-trained model can accurately identify diabetics receiving
low-quality care
</li>

<li>Out-of-sample accuracy of \(78\%\)
</li>

<li>Identifies most patients receiving poor care
</li>

<li>In practice, the probabilities returned by the logistic regression
model can be used to prioritize patients for intervention
</li>

<li>Electronic medical records could be used in the future
</li>
</ul>

<p>
So a model like the one we built can be used to analyze literally
millions of records. Whereas a human can only accurately analyze
rather small amounts of information. So clearly such a model allows
significantly larger scalability.
</p>

<p>
Of course models do not replace expert judgement. However, models
provide a way to translate expert judgement to a reproducible,
testable prediction methodology that has significantly higher
scalability, as we discussed. And of course experts can continuously
improve and refine the model, as we have seen in this lecture.
</p>

<ul class="org-ul">
<li>While humans can accurately analyze small amounts of information,
models allow larger scalability
</li>

<li>Models do not replace expert judgment
</li>

<li>Experts can improve and refine the model
</li>

<li>Models can integrate assessments of many experts into one final
unbiased and unemotional prediction.
</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> The Framingham Heart Study: Evaluating Risk Factors to Save Lives</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> Video 1: The Framingham Heart Study</h3>
<div class="outline-text-3" id="text-2-1">
<p>
We'll describe the Framingham Heart Study, one of the most important
epidemiological studies ever conducted, and the underlying analytics
that led to our current understanding of cardiovascular disease.
</p>

<p>
To motivate how the study affected our understanding of blood pressure
today, we describe the case of Franklin Delano Roosevelt, FDR for
short, who was the President of the United States from 1933 to 1945.
</p>

<p>
He died while President on April 12, 1945. Before the presidency,
FDR's blood pressure was 140/100. Today, healthy blood pressure is
considered to be less than 120/80.
</p>

<p>
So therefore, 140/100 is today considered high blood pressure. One
year before his death, his blood pressure was 210/120. Today this is
called hypertensive crisis, and emergency care is needed.
</p>

<p>
On the other hand, FDR's personal physician said a moderate degree of
atherosclerosis although no more than normal for a man of his age. Two
months before his death, his blood pressure was 260/150, and the day
of his death was 300/190.
</p>


<div class="figure">
<p><img src="../graphs/FDR.png" alt="FDR.png" />
</p>
</div>
</div>

<div id="outline-container-sec-2-1-1" class="outline-4">
<h4 id="sec-2-1-1"><span class="section-number-4">2.1.1</span> FDR’s Blood Pressure</h4>
<div class="outline-text-4" id="text-2-1-1">
<ul class="org-ul">
<li>Before presidency, blood pressure of 140/100

<ul class="org-ul">
<li>Healthy blood pressure is less than 120/80
</li>
<li>Today, this is already considered high blood pressure
</li>
</ul>
</li>

<li>One year before death, 210/120
</li>

<li>Today, this is called Hypertensive Crisis, and emergency care is
needed
</li>

<li>FDR’s personal physician:
</li>
</ul>

<p>
"A moderate degree of arteriosclerosis, although no more than normal
for a man of his age"
</p>

<ul class="org-ul">
<li>Two months before death: 260/150 - Day of death: 300/190
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-2-1-2" class="outline-4">
<h4 id="sec-2-1-2"><span class="section-number-4">2.1.2</span> Early Misconceptions</h4>
<div class="outline-text-4" id="text-2-1-2">
<ul class="org-ul">
<li>High blood pressure dubbed essential hypertension

<ul class="org-ul">
<li>Considered important to force blood through arteries
</li>
<li>Considered harmful to lower blood pressure
</li>
</ul>
</li>

<li>Today, we know better
</li>
</ul>

<p>
"Today, presidential blood pressure numbers like FDR’s would send the
country’s leading doctors racing down hallways &#x2026; whisking the
nation’s leader into the cardiac care unit of Bethesda Naval
Hospital."
</p>

<p>
&#x2013; Daniel Levy, Framingham Heart Study Director
</p>
</div>
</div>

<div id="outline-container-sec-2-1-3" class="outline-4">
<h4 id="sec-2-1-3"><span class="section-number-4">2.1.3</span> How did we Learn?</h4>
<div class="outline-text-4" id="text-2-1-3">

<div class="figure">
<p><img src="../graphs/HowDidWeLearn.png" alt="HowDidWeLearn.png" />
</p>
</div>

<p>
So in 1948, the <b>Framingham Heart Study</b> started. The study included
5,209 patients, aged 30 to 59. Patients were given a questionnaire and
an examination every two years.
</p>

<p>
During this examination, their physical characteristics were recorded,
their behavioral characteristics, as well as test results. Exams and
questions expanded over time.
</p>


<div class="figure">
<p><img src="../graphs/FraminghamHeartStudy.png" alt="FraminghamHeartStudy.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> Quick Question (1 point possible)</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Why was the city of Framingham, Massachusetts selected for this study?
Select all that apply.
</p>

<ul class="org-ul">
<li><code>[&#xa0;]</code> It represented all types of people in the United States.
</li>
<li><code>[X]</code> It had an appropriate size.
</li>
<li><code>[X]</code> It had a stable population to observe over time.
</li>
<li><code>[&#xa0;]</code> It contained an abnormally large number of people with heart
disease.
</li>
<li><code>[X]</code> The doctors and residents in Framingham were willing to
participate.
</li>
</ul>
</div>

<div id="outline-container-sec-2-2-1" class="outline-4">
<h4 id="sec-2-2-1"><span class="section-number-4">2.2.1</span> Answer</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
<b>Explanation</b>
</p>

<p>
The reasons for Framingham being selected for this study are listed on
Slide 4 of the previous video: it had an appropriate size, it had a
stable population, and the doctors and residents in the town were
willing to participate. However, the city did not represent all types
of people in the United States (we'll see later in the lecture how to
extend the model to different populations) and there were not an
abnormally large number of people with heart disease.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3"><span class="section-number-3">2.3</span> Video 2: Risk Factors</h3>
<div class="outline-text-3" id="text-2-3">

<div class="figure">
<p><img src="../graphs/Analitics2PreventHeartDisease.png" alt="Analitics2PreventHeartDisease.png" />
</p>
</div>

<p>
We'll be using analytical models to prevent heart disease. The first
step is to identify risk factors, or the independent variables, that
we will use in our model. Then, using data, we'll create a logistic
regression model to predict heart disease.
</p>

<p>
Using more data, we'll validate our model to make sure it performs
well out of sample and on different populations than the training set
population. Lastly, we'll discuss how medical interventions can be
defined using the model.
</p>

<p>
We'll be predicting the 10-year risk of coronary heart disease or
CHD.
</p>


<div class="figure">
<p><img src="../graphs/CHD.png" alt="CHD.png" />
</p>
</div>

<p>
This is in part due to earlier detection and monitoring partly because
of the Framingham Heart Study. Before building a logistic regression
model, we need to identify the independent variables we want to use.
</p>

<p>
When predicting the risk of a disease, we want to identify what are
known as risk factors. These are the variables that increase the
chances of developing a disease. The term risk factors was actually
coined by William Kannell and Roy Dawber from the Framingham Heart
Study.
</p>


<div class="figure">
<p><img src="../graphs/RiskFactors.png" alt="RiskFactors.png" />
</p>
</div>

<p>
In this lecture, we'll focus on the risk factors that they collected
data for in the original data collection for the Framingham Heart
Study.
</p>


<div class="figure">
<p><img src="../graphs/HypothesizedCHDRiskFactors.png" alt="HypothesizedCHDRiskFactors.png" />
</p>
</div>

<p>
Variables (2)
</p>


<div class="figure">
<p><img src="../graphs/HypothesizedCHDRiskFactors02.png" alt="HypothesizedCHDRiskFactors02.png" />
</p>
</div>

<p>
Variables (3)
</p>


<div class="figure">
<p><img src="../graphs/HypothesizedCHDRiskFactors03.png" alt="HypothesizedCHDRiskFactors03.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-4" class="outline-3">
<h3 id="sec-2-4"><span class="section-number-3">2.4</span> Quick Question (2 points possible)</h3>
<div class="outline-text-3" id="text-2-4">
</div><div id="outline-container-sec-2-4-1" class="outline-4">
<h4 id="sec-2-4-1"><span class="section-number-4">2.4.1</span> Question a</h4>
<div class="outline-text-4" id="text-2-4-1">
<p>
Are <b>risk factors</b> the independent variables or the dependent
variables in our model?
</p>
</div>

<div id="outline-container-sec-2-4-1-1" class="outline-5">
<h5 id="sec-2-4-1-1"><span class="section-number-5">2.4.1.1</span> Answer</h5>
<div class="outline-text-5" id="text-2-4-1-1">
<ul class="org-ul">
<li><code>[X]</code> Independent Variables
</li>
<li><code>[&#xa0;]</code> Dependent Variables
</li>
<li><code>[&#xa0;]</code> Neither
</li>
</ul>

<p>
<b>Explanation</b>
</p>

<p>
Risk factors are the independent variables in our model, and are what
we will use to predict the dependent variable.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-4-2" class="outline-4">
<h4 id="sec-2-4-2"><span class="section-number-4">2.4.2</span> Question b</h4>
<div class="outline-text-4" id="text-2-4-2">
<p>
In many situations, a dataset is handed to you and you are tasked with
discovering which variables are important. But for the Framingham
Heart Study, the researchers had to collect data from patients. In a
situation like this one, where data needs to be collected by the
researchers, should the potential risk factors be defined before or
after the data is collected?
</p>
</div>

<div id="outline-container-sec-2-4-2-1" class="outline-5">
<h5 id="sec-2-4-2-1"><span class="section-number-5">2.4.2.1</span> Answer</h5>
<div class="outline-text-5" id="text-2-4-2-1">
<ul class="org-ul">
<li><code>[X]</code> Before
</li>
<li><code>[&#xa0;]</code> After
</li>
</ul>

<p>
<b>Explanation</b>
</p>

<p>
The researchers should first hypothesize potential risk factors, and
then collect data corresponding to those risk factors. Of course, they
could always define more risk factors later and collect more data, but
this data would take longer to collect.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-2-5" class="outline-3">
<h3 id="sec-2-5"><span class="section-number-3">2.5</span> Video 3: - A Logistic Regression Model</h3>
<div class="outline-text-3" id="text-2-5">
<p>
In this video, we'll use the dataset <a href="https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/framingham.csv">framingham.csv</a> to build a
logistic regression model. Please download this dataset to following
along. This data comes from the <a href="https://biolincc.nhlbi.nih.gov/static/studies/teaching/framdoc.pdf">BioLINCC website</a>.
</p>

<p>
An R script file with all of the commands used in this lecture can be
downloaded <a href="https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/Unit3_Framingham.R">here</a>.
</p>
</div>

<div id="outline-container-sec-2-5-1" class="outline-4">
<h4 id="sec-2-5-1"><span class="section-number-4">2.5.1</span> Download the data sets</h4>
<div class="outline-text-4" id="text-2-5-1">
<p>
In this part we can download the data
</p>

<div class="org-src-container">

<pre class="src src-R"><span style="color: #db7093;">library</span>(parallel)

<span style="color: #4682b4;">if</span>(!file.exists(<span style="color: #ff6a6a;">"../data"</span>)) {
        dir.create(<span style="color: #ff6a6a;">"../data"</span>)
}

fileUrl <span style="color: #db7093;">&lt;-</span> <span style="color: #ff6a6a;">"https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/framingham.csv"</span>

fileName <span style="color: #db7093;">&lt;-</span> <span style="color: #ff6a6a;">"framingham.csv"</span>

dataPath <span style="color: #db7093;">&lt;-</span> <span style="color: #ff6a6a;">"../data"</span>

filePath <span style="color: #db7093;">&lt;-</span> paste(dataPath, fileName, sep = <span style="color: #ff6a6a;">"/"</span>)

<span style="color: #4682b4;">if</span>(!file.exists(filePath)) {
        download.file(fileUrl, destfile = filePath, method = <span style="color: #ff6a6a;">"curl"</span>)
}

list.files(<span style="color: #ff6a6a;">"../data"</span>)
</pre>
</div>

<pre class="example">
 [1] "AirlinesCluster.csv"       "AnonymityPoll.csv"
 [3] "baseball.csv"              "BoeingStock.csv"
 [5] "boston.csv"                "ClaimsData.csv"
 [7] "ClaimsData.csv.zip"        "climate_change.csv"
 [9] "clinical_trial.csv"        "ClusterMeans.ods"
[11] "CocaColaStock.csv"         "CountryCodes.csv"
[13] "CPSData.csv"               "dailykos.csv"
[15] "eBayiPadTest.csv"          "eBayiPadTrain.csv"
[17] "edges.csv"                 "emails.csv"
[19] "energy_bids.csv"           "flower.csv"
[21] "FluTest.csv"               "FluTrain.csv"
[23] "framingham.csv"            "gerber.csv"
[25] "GEStock.csv"               "healthy.csv"
[27] "households.csv"            "IBMStock.csv"
[29] "intl.csv"                  "intlall.csv"
[31] "loans_imputed.csv"         "loans.csv"
[33] "MetroAreaCodes.csv"        "movieLens.txt"
[35] "murders.csv"               "mvt.csv"
[37] "mvtWeek1.csv"              "NBA_test.csv"
[39] "NBA_train.csv"             "parole.csv"
[41] "pisa2009test.csv"          "pisa2009train.csv"
[43] "PollingData_Imputed.csv"   "PollingData.csv"
[45] "PollingImputed.csv"        "ProcterGambleStock.csv"
[47] "quality.csv"               "README.md"
[49] "SampleSubmission.csv"      "songs.csv"
[51] "stevens.csv"               "StocksCluster.csv"
[53] "stopwords.txt"             "SubmissionLR2.csv"
[55] "SubmissionSimpleLogV1.csv" "tumor.csv"
[57] "tweets.csv"                "tweetsU7.csv"
[59] "USDA.csv"                  "users.csv"
[61] "WHO_Europe.csv"            "WHO.csv"
[63] "WHOu7.csv"                 "wiki.csv"
[65] "wine_test.csv"             "wine.csv"
</pre>
</div>
</div>

<div id="outline-container-sec-2-5-2" class="outline-4">
<h4 id="sec-2-5-2"><span class="section-number-4">2.5.2</span> Load the data set</h4>
<div class="outline-text-4" id="text-2-5-2">
<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"    Loading data into their data frames."</span>)
framingham <span style="color: #db7093;">&lt;-</span> read.table(<span style="color: #ff6a6a;">"../data/framingham.csv"</span>, sep = <span style="color: #ff6a6a;">","</span>, header = <span style="color: #3cb371;">TRUE</span>)
str(framingham)
summary(framingham)
</pre>
</div>

<pre class="example">
    Loading data into their data frames.
'data.frame':	4240 obs. of  16 variables:
 $ male           : int  1 0 1 0 0 0 0 0 1 1 ...
 $ age            : int  39 46 48 61 46 43 63 45 52 43 ...
 $ education      : int  4 2 1 3 3 2 1 2 1 1 ...
 $ currentSmoker  : int  0 0 1 1 1 0 0 1 0 1 ...
 $ cigsPerDay     : int  0 0 20 30 23 0 0 20 0 30 ...
 $ BPMeds         : int  0 0 0 0 0 0 0 0 0 0 ...
 $ prevalentStroke: int  0 0 0 0 0 0 0 0 0 0 ...
 $ prevalentHyp   : int  0 0 0 1 0 1 0 0 1 1 ...
 $ diabetes       : int  0 0 0 0 0 0 0 0 0 0 ...
 $ totChol        : int  195 250 245 225 285 228 205 313 260 225 ...
 $ sysBP          : num  106 121 128 150 130 ...
 $ diaBP          : num  70 81 80 95 84 110 71 71 89 107 ...
 $ BMI            : num  27 28.7 25.3 28.6 23.1 ...
 $ heartRate      : int  80 95 75 65 85 77 60 79 76 93 ...
 $ glucose        : int  77 76 70 103 85 99 85 78 79 88 ...
 $ TenYearCHD     : int  0 0 0 1 0 0 1 0 0 0 ...
      male             age          education     currentSmoker
 Min.   :0.0000   Min.   :32.00   Min.   :1.000   Min.   :0.0000
 1st Qu.:0.0000   1st Qu.:42.00   1st Qu.:1.000   1st Qu.:0.0000
 Median :0.0000   Median :49.00   Median :2.000   Median :0.0000
 Mean   :0.4292   Mean   :49.58   Mean   :1.979   Mean   :0.4941
 3rd Qu.:1.0000   3rd Qu.:56.00   3rd Qu.:3.000   3rd Qu.:1.0000
 Max.   :1.0000   Max.   :70.00   Max.   :4.000   Max.   :1.0000
                                  NA's   :105
   cigsPerDay         BPMeds        prevalentStroke     prevalentHyp
 Min.   : 0.000   Min.   :0.00000   Min.   :0.000000   Min.   :0.0000
 1st Qu.: 0.000   1st Qu.:0.00000   1st Qu.:0.000000   1st Qu.:0.0000
 Median : 0.000   Median :0.00000   Median :0.000000   Median :0.0000
 Mean   : 9.006   Mean   :0.02962   Mean   :0.005896   Mean   :0.3106
 3rd Qu.:20.000   3rd Qu.:0.00000   3rd Qu.:0.000000   3rd Qu.:1.0000
 Max.   :70.000   Max.   :1.00000   Max.   :1.000000   Max.   :1.0000
 NA's   :29       NA's   :53
    diabetes          totChol          sysBP           diaBP
 Min.   :0.00000   Min.   :107.0   Min.   : 83.5   Min.   : 48.0
 1st Qu.:0.00000   1st Qu.:206.0   1st Qu.:117.0   1st Qu.: 75.0
 Median :0.00000   Median :234.0   Median :128.0   Median : 82.0
 Mean   :0.02571   Mean   :236.7   Mean   :132.4   Mean   : 82.9
 3rd Qu.:0.00000   3rd Qu.:263.0   3rd Qu.:144.0   3rd Qu.: 90.0
 Max.   :1.00000   Max.   :696.0   Max.   :295.0   Max.   :142.5
                   NA's   :50
      BMI          heartRate         glucose         TenYearCHD
 Min.   :15.54   Min.   : 44.00   Min.   : 40.00   Min.   :0.0000
 1st Qu.:23.07   1st Qu.: 68.00   1st Qu.: 71.00   1st Qu.:0.0000
 Median :25.40   Median : 75.00   Median : 78.00   Median :0.0000
 Mean   :25.80   Mean   : 75.88   Mean   : 81.96   Mean   :0.1519
 3rd Qu.:28.04   3rd Qu.: 83.00   3rd Qu.: 87.00   3rd Qu.:0.0000
 Max.   :56.80   Max.   :143.00   Max.   :394.00   Max.   :1.0000
 NA's   :19      NA's   :1        NA's   :388
</pre>

<p>
Now that we have identified a set of risk factors, let's use this data
to predict the 10 year risk of CHD. First, we'll randomly split our
patients into a training set and a testing set.
</p>

<p>
Then, we'll use logistic regression to predict whether or not a
patient experienced CHD within 10 years of the first examination. Keep
in mind that all of the risk factors were collected at the first
examination of the patients.
</p>

<p>
After building our model, we'll evaluate the predictive power of the
model on the test set.
</p>


<div class="figure">
<p><img src="../graphs/AnalyticalApproach.png" alt="AnalyticalApproach.png" />
</p>
</div>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Load the library caTools"</span>)
<span style="color: #db7093;">library</span>(caTools)

writeLines(<span style="color: #ff6a6a;">"\n :: Randomly split the data into training and testing sets"</span>)
set.seed(1000)
split <span style="color: #db7093;">&lt;-</span> sample.split(framingham$TenYearCHD, SplitRatio = 0.65)

writeLines(<span style="color: #ff6a6a;">"\n :: Split up the data using subset"</span>)
train <span style="color: #db7093;">&lt;-</span> subset(framingham, split==<span style="color: #3cb371;">TRUE</span>)
test <span style="color: #db7093;">&lt;-</span> subset(framingham, split==<span style="color: #3cb371;">FALSE</span>)
</pre>
</div>

<pre class="example">
:: Load the library caTools

:: Randomly split the data into training and testing sets

:: Split up the data using subset
</pre>

<p>
Here, we'll put \(65\%\) of the data in the training set. When you have
more data like we do here, you can afford to put less data in the
training set and more in the testing set.
</p>

<p>
This will increase our confidence in the ability of the model to
extend to new data since we have a larger test set, and still give us
enough data in the training set to create our model. You typically
want to put somewhere between \(50\%\) and \(80\%\) of the data in the
training set.
</p>

<p>
Now we're ready to build our logistic regression model using the
training set.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Logistic Regression Model"</span>)
framinghamLog <span style="color: #db7093;">&lt;-</span> glm(TenYearCHD ~ ., data = train, family=binomial)
summary(framinghamLog)
</pre>
</div>

<pre class="example">
 :: Logistic Regression Model

Call:
glm(formula = TenYearCHD ~ ., family = binomial, data = train)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-1.8487  -0.6007  -0.4257  -0.2842   2.8369

Coefficients:
                 Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     -7.886574   0.890729  -8.854  &lt; 2e-16 ***
male             0.528457   0.135443   3.902 9.55e-05 ***
age              0.062055   0.008343   7.438 1.02e-13 ***
education       -0.058923   0.062430  -0.944  0.34525
currentSmoker    0.093240   0.194008   0.481  0.63080
cigsPerDay       0.015008   0.007826   1.918  0.05514 .
BPMeds           0.311221   0.287408   1.083  0.27887
prevalentStroke  1.165794   0.571215   2.041  0.04126 *
prevalentHyp     0.315818   0.171765   1.839  0.06596 .
diabetes        -0.421494   0.407990  -1.033  0.30156
totChol          0.003835   0.001377   2.786  0.00533 **
sysBP            0.011344   0.004566   2.485  0.01297 *
diaBP           -0.004740   0.008001  -0.592  0.55353
BMI              0.010723   0.016157   0.664  0.50689
heartRate       -0.008099   0.005313  -1.524  0.12739
glucose          0.008935   0.002836   3.150  0.00163 **
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2020.7  on 2384  degrees of freedom
Residual deviance: 1792.3  on 2369  degrees of freedom
  (371 observations deleted due to missingness)
AIC: 1824.3

Number of Fisher Scoring iterations: 5
</pre>

<p>
In the <code>glm()</code> logistic regression model generation you must be
careful doing this with data sets  that have identifying variables
like a patient ID or name since you wouldn't want to use these as
independent variables.
</p>

<p>
It looks like <b>male</b>, <b>age</b>, <b>prevalent stroke</b>, <b>total cholesterol</b>,
<b>systolic blood pressure</b>, and <b>glucose</b> are all <i>significant</i> in our
model. <b>Cigarettes per day</b> and <b>prevalent hypertension</b> are almost
significant. All of the significant variables have positive
coefficients, meaning that higher values in these variables contribute
to a higher probability of <b>10-year coronary heart disease</b>.
</p>

<p>
Now, let's use this model to make predictions on our test set.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Predictions on the test set"</span>)
predictTest <span style="color: #db7093;">&lt;-</span> predict(framinghamLog, type = <span style="color: #ff6a6a;">"response"</span>, newdata = test)

writeLines(<span style="color: #ff6a6a;">"\n :: Confusion matrix with threshold of 0.5"</span>)
table(test$TenYearCHD, predictTest &gt; 0.5)

writeLines(<span style="color: #ff6a6a;">"\n :: Accuracy"</span>)
(1069 + 11) / (1069 + 6 + 187 + 11)

writeLines(<span style="color: #ff6a6a;">"\n :: Baseline accuracy"</span>)
(1069 + 6) / (1069 + 6 + 187 + 11)
</pre>
</div>

<pre class="example">
 :: Predictions on the test set

 :: Confusion matrix with threshold of 0.5

    FALSE TRUE
  0  1069    6
  1   187   11

 :: Accuracy
[1] 0.8483896

 :: Baseline accuracy
[1] 0.8444619
</pre>

<ul class="org-ul">
<li>With a threshold of \(0.5\), we predict an outcome of \(1\), the true
column, very rarely. This means that our model rarely predicts a
10-year CHD risk above \(50\%\).
</li>

<li>So the accuracy of our model is about \(84.8\%\).
</li>

<li>We need to compare this to the accuracy of a simple baseline
method. The more frequent outcome in this case is \(0\), so the baseline
method would always predict \(0\) or no <b>CHD</b>.
</li>
</ul>

<p>
So the baseline model would get an accuracy of about \(84.4\%\). So our
model barely beats the baseline in terms of accuracy. But do we still
have a valuable model by varying the threshold? Let's compute the
<b>out-of-sample AUC</b>.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Test set AUC "</span>)
<span style="color: #db7093;">library</span>(ROCR)
ROCRpred <span style="color: #db7093;">&lt;-</span> prediction(predictTest, test$TenYearCHD)
as.numeric(performance(ROCRpred, <span style="color: #ff6a6a;">"auc"</span>)@y.values)
</pre>
</div>

<pre class="example">
 :: Test set AUC
[1] 0.7421095
</pre>

<p>
This will give us the <b>AUC</b> value on our testing set. So we have an
<b>AUC</b> of about 74% on our test set, which means that the model can
differentiate between low risk patients and high risk patients pretty
well.
</p>
</div>
</div>

<div id="outline-container-sec-2-5-3" class="outline-4">
<h4 id="sec-2-5-3"><span class="section-number-4">2.5.3</span> Conclusions</h4>
<div class="outline-text-4" id="text-2-5-3">
<p>
We were able to build a logistic regression model with a few
interesting properties.
</p>


<div class="figure">
<p><img src="../graphs/ModelStrength.png" alt="ModelStrength.png" />
</p>
</div>

<p>
We saw that more <b>cigarettes per day</b>, <b>higher cholesterol</b>, <b>higher
systolic blood pressure</b>, and <b>higher glucose levels</b> all increased
risk.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-6" class="outline-3">
<h3 id="sec-2-6"><span class="section-number-3">2.6</span> Quick Question (2 points possible)</h3>
<div class="outline-text-3" id="text-2-6">
<p>
In the previous video, we computed the following confusion matrix for
        our logistic regression model on our test set with a threshold
        of 0.5:
</p>

<p>
        FALSE         TRUE
0         1069         6
1         187         11
</p>

<p>
Using this confusion matrix, answer the following questions.
</p>

<div class="org-src-container">

<pre class="src src-R">TN <span style="color: #db7093;">&lt;-</span> 1069; FP <span style="color: #db7093;">&lt;-</span> 6
FN <span style="color: #db7093;">&lt;-</span> 187; TP <span style="color: #db7093;">&lt;-</span> 11

writeLines(<span style="color: #ff6a6a;">"\n :: The Sensitivity is:"</span>)
Sensitivity <span style="color: #db7093;">&lt;-</span> TP / (TP + FN)
Sensitivity

writeLines(<span style="color: #ff6a6a;">"\n :: The Specificity is:"</span>)
Specificity <span style="color: #db7093;">&lt;-</span> TN / (TN + FP)
Specificity
</pre>
</div>

<pre class="example">
 :: The Sensitivity is:
[1] 0.05555556

 :: The Specificity is:
[1] 0.9944186
</pre>
</div>

<div id="outline-container-sec-2-6-1" class="outline-4">
<h4 id="sec-2-6-1"><span class="section-number-4">2.6.1</span> Question a</h4>
<div class="outline-text-4" id="text-2-6-1">
<p>
What is the sensitivity of our logistic regression model on the test
set, using a threshold of 0.5?
</p>
</div>

<div id="outline-container-sec-2-6-1-1" class="outline-5">
<h5 id="sec-2-6-1-1"><span class="section-number-5">2.6.1.1</span> Answer</h5>
<div class="outline-text-5" id="text-2-6-1-1">
<p>
0.05555556
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-6-2" class="outline-4">
<h4 id="sec-2-6-2"><span class="section-number-4">2.6.2</span> Question b</h4>
<div class="outline-text-4" id="text-2-6-2">
<p>
What is the specificity of our logistic regression model on the test
set, using a threshold of 0.5?
</p>
</div>

<div id="outline-container-sec-2-6-2-1" class="outline-5">
<h5 id="sec-2-6-2-1"><span class="section-number-5">2.6.2.1</span> Answer</h5>
<div class="outline-text-5" id="text-2-6-2-1">
<p>
0.9944186
</p>

<p>
<b>Explanation</b>
</p>

<p>
Using this confusion matrix, we can compute that the sensitivity is
11/(11+187) and the specificity is 1069/(1069+6).
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-2-7" class="outline-3">
<h3 id="sec-2-7"><span class="section-number-3">2.7</span> Video 4: Validating the Model</h3>
<div class="outline-text-3" id="text-2-7">
<p>
We mention that the Framingham Risk Model was tested on diverse
cohorts. The original Framingham Risk Model was actually computed by a
different sort of regression, called a Cox Proportional Hazards
Model. This method is different but related to logistic regression,
and it will return a similar estimate of 10-year CHD risk.
</p>

<p>
So far, we have used what is known as internal validation to test our
model. This means that we took the data from one set of patients and
split them into a training set and a testing set. While this confirms
that our model is good at making predictions for patients in the
Framingham Heart Study population, it's unclear if the model
generalizes to other populations.
</p>


<div class="figure">
<p><img src="../graphs/ModelValidation.png" alt="ModelValidation.png" />
</p>
</div>
</div>

<div id="outline-container-sec-2-7-1" class="outline-4">
<h4 id="sec-2-7-1"><span class="section-number-4">2.7.1</span> External validation</h4>
<div class="outline-text-4" id="text-2-7-1">
<p>
There have been many studies to test the Framingham model from the
influential 1998 paper on diverse cohorts. This table shows a sample
of studies that tested the model on populations with different races.
</p>

<p>
The researchers for each study collected the same risk factors used in
the original study, predicted CHD using the Framingham Heart Study
model, and then analyzed how accurate the model was for that
population.
</p>

<p>
For some populations, the Framingham model was accurate.
</p>



<div class="figure">
<p><img src="../graphs/ExternalValidation.png" alt="ExternalValidation.png" />
</p>
</div>

<p>
For the <b>ARIC</b> study that tested the model with black men, this figure
shows a bar graph of how the Framingham predictions compare with the
actual results.
</p>


<div class="figure">
<p><img src="../graphs/ARIC.png" alt="ARIC.png" />
</p>
</div>

<p>
The gray bars are the predictions. And the black bars are the actual
outcomes. The patients are sorted on the x-axis by predicted risk and
on the y-axis by the percentage of patients in each group who actually
developed CHD.
</p>

<p>
For the most part, the predictions are accurate. There's one group for
which the model under-predicted the risk and one group for which the
model over-predicted the risk.
</p>

<p>
For the HHS study with Japanese-American men, the Framingham model
systematically over-predicts a risk of CHD.
</p>


<div class="figure">
<p><img src="../graphs/HHS.png" alt="HHS.png" />
</p>
</div>

<p>
The model can be recalibrated for this population by scaling down the
predictions. This changes the predicted risk but not the order of the
predictions.
</p>

<p>
This changes the predicted risk but not the order of the
predictions. The high risk patients still have higher predictions than
the lower risk patients. This allows the model to have more accurate
risk estimates for populations not included in the original group of
patients. For models that will be used on different populations than
the one used to create the model, external validation is critical.
</p>


<div class="figure">
<p><img src="../graphs/Recalibration.png" alt="Recalibration.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-2-8" class="outline-3">
<h3 id="sec-2-8"><span class="section-number-3">2.8</span> Quick Question (1 point possible)</h3>
<div class="outline-text-3" id="text-2-8">
<p>
For which of the following models should external validation be used?
</p>

<p>
Consider both the population used to train the model, and the
population that the model will be used on. (Select all that apply.)
</p>

<ul class="org-ul">
<li><code>[X]</code> A model to predict obesity risk. Data from a random sample of
California residents was used to build the model, and we want to use
the model to predict the obesity risk of all United States
residents.
</li>

<li><code>[&#xa0;]</code> A model to predict the stress of MIT students. Data from a
random sample of MIT students was used to build the model, and we
want to use the model to predict the stress level of all MIT
students.
</li>

<li><code>[X]</code> A model to predict the probability of a runner winning a
marathon. Data from all runners in the Boston Marathon was used to
build the model, and we want use the model to predict the
probability of winning for all people who run marathons.
</li>
</ul>
</div>

<div id="outline-container-sec-2-8-1" class="outline-4">
<h4 id="sec-2-8-1"><span class="section-number-4">2.8.1</span> Answer</h4>
<div class="outline-text-4" id="text-2-8-1">
<p>
<b>Explanation</b>
</p>

<p>
In the first and third models, we are using a <b>special sub-population</b>
to build the model. While we can use the model for that
sub-population, we should use external validation to test the model on
other populations. The second model uses data from a special
sub-population, but the model is only intended for that
sub-population, so external validation is not necessary.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-9" class="outline-3">
<h3 id="sec-2-9"><span class="section-number-3">2.9</span> Video 5: Interventions</h3>
<div class="outline-text-3" id="text-2-9">
<p>
We next discuss interventions suggested by the model developed for the
Framingham Heart Study. The first intervention has to do with drugs to
lower blood pressure.
</p>


<div class="figure">
<p><img src="../graphs/Interventions.png" alt="Interventions.png" />
</p>
</div>

<p>
In FDR's time, hypertension drugs were too toxic for practical
use. But in the 1950s, the diuretic chlorothiazide was developed, and
the Framingham Heart Study gave Ed Freis the evidence needed to argue
for testing effects for blood pressure drugs.
</p>
</div>

<div id="outline-container-sec-2-9-1" class="outline-4">
<h4 id="sec-2-9-1"><span class="section-number-4">2.9.1</span> First intervention and business</h4>
<div class="outline-text-4" id="text-2-9-1">

<div class="figure">
<p><img src="../graphs/Interventions01.png" alt="Interventions01.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-9-2" class="outline-4">
<h4 id="sec-2-9-2"><span class="section-number-4">2.9.2</span> Second intervention</h4>
<div class="outline-text-4" id="text-2-9-2">
<p>
Another intervention had to do with&#x2013; to lower cholesterol.
</p>


<div class="figure">
<p><img src="../graphs/Interventions02.png" alt="Interventions02.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-2-10" class="outline-3">
<h3 id="sec-2-10"><span class="section-number-3">2.10</span> Quick Question (1 point possible)</h3>
<div class="outline-text-3" id="text-2-10">
<p>
In Video 3, we built a logistic regression model and found that the
following variables were significant (or almost significant) for
predicting ten year risk of CHD: male, age, number of cigarettes per
day, whether or not the patient previously had a stroke, whether or
not the patient is currently hypertensive, total cholesterol level,
systolic blood pressure, and blood glucose level. Which one of the
following variables would be the most dramatically affected by a
behavioral intervention? HINT: Think about how much control the
patient has over each of the variables.
</p>

<ul class="org-ul">
<li><code>[&#xa0;]</code> Male
</li>
<li><code>[&#xa0;]</code> Age
</li>
<li><code>[X]</code> Number of Cigarettes per day
</li>
<li><code>[&#xa0;]</code> Previously had a Stroke
</li>
<li><code>[&#xa0;]</code> Hypertensive
</li>
<li><code>[&#xa0;]</code> Total Cholesterol Level
</li>
<li><code>[&#xa0;]</code> Systolic Blood Pressure
</li>
<li><code>[&#xa0;]</code> Blood Glucose Level
</li>
</ul>
</div>

<div id="outline-container-sec-2-10-1" class="outline-4">
<h4 id="sec-2-10-1"><span class="section-number-4">2.10.1</span> Answer</h4>
<div class="outline-text-4" id="text-2-10-1">
<p>
<b>Explanation</b>
</p>

<p>
The number of cigarettes smoked per day would be the most dramatically
affected by a behavioral intervention. This is a variable that the
patient has the ability to control the most.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-11" class="outline-3">
<h3 id="sec-2-11"><span class="section-number-3">2.11</span> Video 6: Overall Impact</h3>
<div class="outline-text-3" id="text-2-11">
<p>
Let us next examine the impact that the Framingham Heart Study had
through the years. So the graph on the right shows the number of
papers written every year using data from the Framingham Study as a
function of time, and we observe the very significant increase in the
number of such publications.
</p>

<p>
Altogether, there has been 2,400 studies written using the Framingham
data. During the years, many other risk factors were
evaluated. Obesity, exercise, psychological, and social issues. In
fact, the Texas Heart Institute Journal named the Framingham Heart
Study as the top 10 cardiology advance of the 20th century.
</p>


<div class="figure">
<p><img src="../graphs/HeartStudyVsTime.png" alt="HeartStudyVsTime.png" />
</p>
</div>
</div>

<div id="outline-container-sec-2-11-1" class="outline-4">
<h4 id="sec-2-11-1"><span class="section-number-4">2.11.1</span> Online tool</h4>
<div class="outline-text-4" id="text-2-11-1">
<p>
In addition to the study, there has been an online tool that assesses
the risk for your 10-year risk of having a heart attack.
</p>


<div class="figure">
<p><img src="../graphs/CHDOnlineTool.png" alt="CHDOnlineTool.png" />
</p>
</div>

<p>
So how about new research directions and challenges that the study is
facing?
</p>


<div class="figure">
<p><img src="../graphs/ResearchDirections.png" alt="ResearchDirections.png" />
</p>
</div>

<p>
A very important impact of the Framingham Heart Study is the
development of clinical decision rules.
</p>


<div class="figure">
<p><img src="../graphs/ClinicalDecisionRules.png" alt="ClinicalDecisionRules.png" />
</p>
</div>

<p>
And the graph shows the clinical prediction rules published as a
function of the year from 1960s to today. And you observe that more
than 70,000 published rules, clinical decision rules, have been
published across medicine, and you observe that the rate of
publication is increasing.
</p>

<p>
So these clinical decision rules are developed using patient and
disease characteristics, and then observed test results from patients
that can assess the effectiveness of such rules.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> Election Forecasting: Predicting the Winner Before any Votes are Cast (Recitation)</h2>
<div class="outline-text-2" id="text-3">
<p>
We'll be using polling data from the months leading up to a
presidential election to predict that election's winner. We'll go over
how to build logistic regression models in R, how to select the
variables to include in those models, and how to evaluate the model
predictions.
</p>
</div>

<div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1"><span class="section-number-3">3.1</span> Video 1: Election Prediction</h3>
<div class="outline-text-3" id="text-3-1">
<p>
The topic of this recitation is election forecasting, which is the art
and science of predicting the winner of an election before any votes
are actually cast using polling data from likely voters. In this
recitation, we are going to look at the United States presidential
election. In the United States, a president is elected every four
years.
</p>

<p>
And while there are a number of different political parties in the US,
generally there are only two competitive candidates.
</p>

<p>
There's the Republican candidate, who tends to be more conservative,
and the Democratic candidate, who's more liberal. So for instance a
recent Republican president was George W. Bush, and a recent
Democratic president was Barack Obama.
</p>

<p>
Now while in many countries the leader of the country is elected using
the simple candidate who receives the largest number of votes across
the entire country is elected, in the United States it's significantly
more complicated.
</p>

<p>
There are 50 states in the United States, and each is assigned a
number of electoral votes based on its population.
</p>


<div class="figure">
<p><img src="../graphs/ElectoralCollege.png" alt="ElectoralCollege.png" />
</p>
</div>

<p>
So for instance, the most populous state, California, in 2012 had
nearly 20 times the number of electoral votes as the least populous
states.
</p>

<p>
And these number of electoral votes are reassigned periodically based
on changes of populations between states. Within a given state in
general, the system is winner take all in the sense that the candidate
who receives the most vote in that state gets all of its electoral
votes.
</p>

<p>
And then across the entire country, the candidate who receives the
most electoral votes wins the entire presidential election.
</p>

<p>
Now while it seems like a somewhat subtle distinction, the electoral
college versus the simple popular vote model, it can have very
significant consequences on the outcome of the election.
</p>

<p>
As an example, let's look at the 2000 presidential election between
George W. Bush and Al Gore.
</p>


<div class="figure">
<p><img src="../graphs/Election2000.png" alt="Election2000.png" />
</p>
</div>

<p>
As we can see on the right here, Al Gore received more than 500,000
more votes across the entire country than George W. Bush in terms of
the popular vote. But in terms of the electoral vote, because of how
those votes were distributed, George Bush actually won the election
because he received five more electoral votes than Al Gore.
</p>

<p>
So our goal will be to use polling data that's collected from likely
voters before the election to predict the winner in each state, and
therefore to enable us to predict the winner of the entire election in
the electoral college system.
</p>


<div class="figure">
<p><img src="../graphs/ElectionPrediction.png" alt="ElectionPrediction.png" />
</p>
</div>

<p>
While election prediction has long attracted some attention, there's
been a particular interest in the problem for the 2012 presidential
election, when then-New York Times columnist Nate Silver took on the
task of predicting the winner in each state.
</p>
</div>

<div id="outline-container-sec-3-1-1" class="outline-4">
<h4 id="sec-3-1-1"><span class="section-number-4">3.1.1</span> The data set</h4>
<div class="outline-text-4" id="text-3-1-1">

<div class="figure">
<p><img src="../graphs/ElectionDS.png" alt="ElectionDS.png" />
</p>
</div>

<p>
Each row in the data set represents a state in a particular election
year. And the dependent variable, which is called Republican, is a
binary outcome. It's \(1\) if the Republican won that state in that
particular election year, and a \(0\) if a Democrat won.
</p>

<p>
The independent variables, again, are related to polling data in that
state. So for instance, the <b>Rasmussen</b> and <b>SurveyUSA</b> variables are
related to two major polls that are assigned across many different
states in the United States.
</p>

<p>
And it represents the percentage of voters who said they were likely
to vote Republican minus the percentage who said they were likely to
vote Democrat.
</p>

<p>
So for instance, if the variable SurveyUSA in our data set has value
\(-6\), it means that \(6\%\) more voters said they were likely to vote
Democrat than said they were likely to vote Republican in that state.
</p>

<p>
<b>DiffCount</b> counts the number of all the polls leading up to the
election that predicted a Republican winner in the state, minus the
number of polls that predicted a Democratic winner.
</p>

<p>
And <b>PropR</b>, or proportion Republican, has the proportion of all those
polls leading up to the election that predicted a Republican winner.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-3-2" class="outline-3">
<h3 id="sec-3-2"><span class="section-number-3">3.2</span> Video 2: Dealing with Missing Data</h3>
<div class="outline-text-3" id="text-3-2">
<p>
In this recitation, we will be using the dataset
<a href="https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/PollingData.csv">PollingData.csv</a>. Please download this dataset to your computer, and
save it in a location that you can easily navigate to in R. This data
comes from <a href="http://www.realclearpolitics.com">RealClearPolitics.com</a>.
</p>

<p>
An R script file with all of the commands used in this lecture can be
downloaded <a href="https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/Unit3_Recitation.R">here</a>.
</p>

<p>
IMPORTANT NOTE: On some operating systems, the imputed results will be
slightly different even if you set the random seed. This is just due
to the randomess involved in the multiple imputation process. We've
provided the imputed data here: <a href="https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/PollingData_Imputed.csv">PollingData_Imputed.csv</a>. If your
results are not matching after the imputation, you can use this
dataset instead.
</p>
</div>

<div id="outline-container-sec-3-2-1" class="outline-4">
<h4 id="sec-3-2-1"><span class="section-number-4">3.2.1</span> Download the data sets</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
In this part we can download the data
</p>

<div class="org-src-container">

<pre class="src src-R"><span style="color: #db7093;">library</span>(parallel)

<span style="color: #4682b4;">if</span>(!file.exists(<span style="color: #ff6a6a;">"../data"</span>)) {
        dir.create(<span style="color: #ff6a6a;">"../data"</span>)
}

fileUrl <span style="color: #db7093;">&lt;-</span> <span style="color: #ff6a6a;">"https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/PollingData.csv"</span>

fileName <span style="color: #db7093;">&lt;-</span> <span style="color: #ff6a6a;">"PollingData.csv"</span>

dataPath <span style="color: #db7093;">&lt;-</span> <span style="color: #ff6a6a;">"../data"</span>

filePath <span style="color: #db7093;">&lt;-</span> paste(dataPath, fileName, sep = <span style="color: #ff6a6a;">"/"</span>)

<span style="color: #4682b4;">if</span>(!file.exists(filePath)) {
        download.file(fileUrl, destfile = filePath, method = <span style="color: #ff6a6a;">"curl"</span>)
}

list.files(<span style="color: #ff6a6a;">"../data"</span>)
</pre>
</div>

<pre class="example">
 [1] "AirlinesCluster.csv"       "AnonymityPoll.csv"
 [3] "baseball.csv"              "BoeingStock.csv"
 [5] "boston.csv"                "ClaimsData.csv"
 [7] "ClaimsData.csv.zip"        "climate_change.csv"
 [9] "clinical_trial.csv"        "ClusterMeans.ods"
[11] "CocaColaStock.csv"         "CountryCodes.csv"
[13] "CPSData.csv"               "dailykos.csv"
[15] "eBayiPadTest.csv"          "eBayiPadTrain.csv"
[17] "edges.csv"                 "emails.csv"
[19] "energy_bids.csv"           "flower.csv"
[21] "FluTest.csv"               "FluTrain.csv"
[23] "framingham.csv"            "gerber.csv"
[25] "GEStock.csv"               "healthy.csv"
[27] "households.csv"            "IBMStock.csv"
[29] "intl.csv"                  "intlall.csv"
[31] "loans_imputed.csv"         "loans.csv"
[33] "MetroAreaCodes.csv"        "movieLens.txt"
[35] "murders.csv"               "mvt.csv"
[37] "mvtWeek1.csv"              "NBA_test.csv"
[39] "NBA_train.csv"             "parole.csv"
[41] "pisa2009test.csv"          "pisa2009train.csv"
[43] "PollingData_Imputed.csv"   "PollingData.csv"
[45] "PollingImputed.csv"        "ProcterGambleStock.csv"
[47] "quality.csv"               "README.md"
[49] "SampleSubmission.csv"      "songs.csv"
[51] "stevens.csv"               "StocksCluster.csv"
[53] "stopwords.txt"             "SubmissionLR2.csv"
[55] "SubmissionSimpleLogV1.csv" "tumor.csv"
[57] "tweets.csv"                "tweetsU7.csv"
[59] "USDA.csv"                  "users.csv"
[61] "WHO_Europe.csv"            "WHO.csv"
[63] "WHOu7.csv"                 "wiki.csv"
[65] "wine_test.csv"             "wine.csv"
</pre>
</div>
</div>

<div id="outline-container-sec-3-2-2" class="outline-4">
<h4 id="sec-3-2-2"><span class="section-number-4">3.2.2</span> Load the data set</h4>
<div class="outline-text-4" id="text-3-2-2">
<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"    Loading data into their data frames."</span>)
polling <span style="color: #db7093;">&lt;-</span> read.table(<span style="color: #ff6a6a;">"../data/PollingData.csv"</span>, sep = <span style="color: #ff6a6a;">","</span>, header = <span style="color: #3cb371;">TRUE</span>)
str(polling)
table(polling$Year)
summary(polling)
</pre>
</div>

<pre class="example">
    Loading data into their data frames.
'data.frame':	145 obs. of  7 variables:
 $ State     : Factor w/ 50 levels "Alabama","Alaska",..: 1 1 2 2 3 3 3 4 4 4 ...
 $ Year      : int  2004 2008 2004 2008 2004 2008 2012 2004 2008 2012 ...
 $ Rasmussen : int  11 21 NA 16 5 5 8 7 10 NA ...
 $ SurveyUSA : int  18 25 NA NA 15 NA NA 5 NA NA ...
 $ DiffCount : int  5 5 1 6 8 9 4 8 5 2 ...
 $ PropR     : num  1 1 1 1 1 ...
 $ Republican: int  1 1 1 1 1 1 1 1 1 1 ...

2004 2008 2012
  50   50   45
         State          Year        Rasmussen          SurveyUSA
 Arizona    :  3   Min.   :2004   Min.   :-41.0000   Min.   :-33.0000
 Arkansas   :  3   1st Qu.:2004   1st Qu.: -8.0000   1st Qu.:-11.7500
 California :  3   Median :2008   Median :  1.0000   Median : -2.0000
 Colorado   :  3   Mean   :2008   Mean   :  0.0404   Mean   : -0.8243
 Connecticut:  3   3rd Qu.:2012   3rd Qu.:  8.5000   3rd Qu.:  8.0000
 Florida    :  3   Max.   :2012   Max.   : 39.0000   Max.   : 30.0000
 (Other)    :127                  NA's   :46         NA's   :71
   DiffCount           PropR          Republican
 Min.   :-19.000   Min.   :0.0000   Min.   :0.0000
 1st Qu.: -6.000   1st Qu.:0.0000   1st Qu.:0.0000
 Median :  1.000   Median :0.6250   Median :1.0000
 Mean   : -1.269   Mean   :0.5259   Mean   :0.5103
 3rd Qu.:  4.000   3rd Qu.:1.0000   3rd Qu.:1.0000
 Max.   : 11.000   Max.   :1.0000   Max.   :1.0000
</pre>

<p>
What we see is that while in the 2004 and 2008 elections, all 50
states have data reported, in 2012, only 45 of the 50 states have
data. And actually, what happened here is that pollsters were so sure
about the five missing states that they didn't perform any polls in
the months leading up to the 2012 election.
</p>

<p>
So since these states are particularly easy to predict, we feel pretty
comfortable moving forward, making predictions just for the 45
remaining states.
</p>

<p>
So the second thing that we notice is that there are these NA values,
which signify missing data. So to get a handle on just how many values
are missing, we can use our summary function on the polling data
frame.
</p>

<p>
We see that for the <b>Rasmussen</b> polling data and also for the <b>SurveyUSA</b>
polling data, there are a decent number of missing values.
</p>
</div>
</div>

<div id="outline-container-sec-3-2-3" class="outline-4">
<h4 id="sec-3-2-3"><span class="section-number-4">3.2.3</span> Simple approaches to deal with missing data</h4>
<div class="outline-text-4" id="text-3-2-3">

<div class="figure">
<p><img src="../graphs/DealWithNA.png" alt="DealWithNA.png" />
</p>
</div>

<p>
There are a number of simple approaches to dealing with missing
data.
</p>

<ul class="org-ul">
<li>One would be to delete observations that are missing at least one
variable value. Unfortunately, in this case, that would result in
throwing away more than \(50\%\) of the observations. And further, we
want to be able to make predictions for all states, not just for the
ones that report all of their variable values.
</li>

<li>Another observation would be to remove the variables that have
missing values, in this case, the Rasmussen and SurveyUSA
variables. However, we expect Rasmussen and SurveyUSA to be
qualitatively different from aggregate variables, such as DiffCount
and PropR, so we want to retain them in our data set.
</li>

<li>A third approach would be to fill the missing data points with
average values. So for Rasmussen and SurveyUSA, the average value
for a poll would be very close to zero across all the times with it
reported, which is roughly a tie between the Democrat and Republican
candidate. However, if PropR is very close to one or zero, we would
expect the Rasmussen or SurveyUSA values that are currently missing
to be positive or negative, respectively.
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-3-2-4" class="outline-4">
<h4 id="sec-3-2-4"><span class="section-number-4">3.2.4</span> Multiple Imputation</h4>
<div class="outline-text-4" id="text-3-2-4">
<p>
This leads to a more complicated approach called multiple imputation
in which we fill in the missing values based on the non-missing
values for an observation. So for instance, if the Rasmussen
variable is reported and is very negative, then a missing SurveyUSA
value would likely be filled in as a negative value as well.
</p>


<div class="figure">
<p><img src="../graphs/MultipleImputation.png" alt="MultipleImputation.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-3-2-5" class="outline-4">
<h4 id="sec-3-2-5"><span class="section-number-4">3.2.5</span> Multiple imputation procedure</h4>
<div class="outline-text-4" id="text-3-2-5">
<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Install and load mice package (Only once)"</span>)
<span style="color: #9932cc;">## </span><span style="color: #ba55d3;">install.packages('mice', repos='http://cran.rstudio.com/')</span>
<span style="color: #db7093;">library</span>(mice)
</pre>
</div>

<pre class="example">
 :: Install and load mice package (Only once)
Loading required package: Rcpp
Loading required package: lattice
mice 2.22 2014-06-10
</pre>

<p>
So for our multiple imputation to be useful, we have to be able to
find out the values of our missing variables without using the outcome
of Republican. So, what we're going to do here is we're going to limit
our data frame to just the four polling related variables before we
actually perform multiple imputation.
</p>

<p>
So we're going to create a new data frame called <b>simple</b>, and that's
just going to be our original polling data frame limited to <b>Rasmussen</b>,
<b>SurveyUSA</b>, <b>PropR</b>, and <b>DiffCount</b>.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Multiple imputation"</span>)
simple <span style="color: #db7093;">&lt;-</span> polling[c(<span style="color: #ff6a6a;">"Rasmussen"</span>, <span style="color: #ff6a6a;">"SurveyUSA"</span>, <span style="color: #ff6a6a;">"PropR"</span>, <span style="color: #ff6a6a;">"DiffCount"</span>)]
summary(simple)
</pre>
</div>

<pre class="example">
:: Multiple imputation
  Rasmussen          SurveyUSA            PropR          DiffCount
Min.   :-41.0000   Min.   :-33.0000   Min.   :0.0000   Min.   :-19.000
1st Qu.: -8.0000   1st Qu.:-11.7500   1st Qu.:0.0000   1st Qu.: -6.000
Median :  1.0000   Median : -2.0000   Median :0.6250   Median :  1.000
Mean   :  0.0404   Mean   : -0.8243   Mean   :0.5259   Mean   : -1.269
3rd Qu.:  8.5000   3rd Qu.:  8.0000   3rd Qu.:1.0000   3rd Qu.:  4.000
Max.   : 39.0000   Max.   : 30.0000   Max.   :1.0000   Max.   : 11.000
NA's   :46         NA's   :71
</pre>

<p>
So again, multiple imputation, if you ran it twice, you would get
different values that were filled in. So, to make sure that everybody
following along gets the same results from imputation, we're going to
set the random seed to a value. It doesn't really matter what value we
pick, so we'll just pick my favorite number, 144.
</p>

<div class="org-src-container">

<pre class="src src-R">set.seed(144)
imputed <span style="color: #db7093;">&lt;-</span> complete(mice(simple))
summary(imputed)
</pre>
</div>

<pre class="example">
iter imp variable
 1   1  Rasmussen  SurveyUSA
 1   2  Rasmussen  SurveyUSA
 1   3  Rasmussen  SurveyUSA
 1   4  Rasmussen  SurveyUSA
 1   5  Rasmussen  SurveyUSA
 2   1  Rasmussen  SurveyUSA
 2   2  Rasmussen  SurveyUSA
 2   3  Rasmussen  SurveyUSA
 2   4  Rasmussen  SurveyUSA
 2   5  Rasmussen  SurveyUSA
 3   1  Rasmussen  SurveyUSA
 3   2  Rasmussen  SurveyUSA
 3   3  Rasmussen  SurveyUSA
 3   4  Rasmussen  SurveyUSA
 3   5  Rasmussen  SurveyUSA
 4   1  Rasmussen  SurveyUSA
 4   2  Rasmussen  SurveyUSA
 4   3  Rasmussen  SurveyUSA
 4   4  Rasmussen  SurveyUSA
 4   5  Rasmussen  SurveyUSA
 5   1  Rasmussen  SurveyUSA
 5   2  Rasmussen  SurveyUSA
 5   3  Rasmussen  SurveyUSA
 5   4  Rasmussen  SurveyUSA
 5   5  Rasmussen  SurveyUSA
  Rasmussen         SurveyUSA           PropR          DiffCount
Min.   :-41.000   Min.   :-33.000   Min.   :0.0000   Min.   :-19.000
1st Qu.: -8.000   1st Qu.:-11.000   1st Qu.:0.0000   1st Qu.: -6.000
Median :  3.000   Median :  1.000   Median :0.6250   Median :  1.000
Mean   :  1.731   Mean   :  1.517   Mean   :0.5259   Mean   : -1.269
3rd Qu.: 11.000   3rd Qu.: 18.000   3rd Qu.:1.0000   3rd Qu.:  4.000
Max.   : 39.000   Max.   : 30.000   Max.   :1.0000   Max.   : 11.000
</pre>

<p>
So the output here shows us that five rounds of imputation have been
run, and now all of the variables have been filled in.
</p>

<p>
So <b>Rasmussen</b> and <b>SurveyUSA</b> both have no more of those NA or
missing values.
</p>

<p>
So the last step in this imputation process is to actually copy the
<b>Rasmussen</b> and <b>SurveyUSA</b> variables back into our original polling data
frame, which has all the variables for the problem.
</p>

<div class="org-src-container">

<pre class="src src-R">polling$Rasmussen <span style="color: #db7093;">&lt;-</span> imputed$Rasmussen
polling$SurveyUSA <span style="color: #db7093;">&lt;-</span> imputed$SurveyUSA
summary(polling)
</pre>
</div>

<pre class="example">
        State          Year        Rasmussen         SurveyUSA
Arizona    :  3   Min.   :2004   Min.   :-41.000   Min.   :-33.000
Arkansas   :  3   1st Qu.:2004   1st Qu.: -8.000   1st Qu.:-11.000
California :  3   Median :2008   Median :  3.000   Median :  1.000
Colorado   :  3   Mean   :2008   Mean   :  1.731   Mean   :  1.517
Connecticut:  3   3rd Qu.:2012   3rd Qu.: 11.000   3rd Qu.: 18.000
Florida    :  3   Max.   :2012   Max.   : 39.000   Max.   : 30.000
(Other)    :127
  DiffCount           PropR          Republican
Min.   :-19.000   Min.   :0.0000   Min.   :0.0000
1st Qu.: -6.000   1st Qu.:0.0000   1st Qu.:0.0000
Median :  1.000   Median :0.6250   Median :1.0000
Mean   : -1.269   Mean   :0.5259   Mean   :0.5103
3rd Qu.:  4.000   3rd Qu.:1.0000   3rd Qu.:1.0000
Max.   : 11.000   Max.   :1.0000   Max.   :1.0000
</pre>

<p>
And as we can see, Rasmussen* and <b>SurveyUSA</b> are no longer missing
values.
</p>

<p>
Because the difference in the way that the architecture works with the
seed, we had detected a difference with the recitation imputed data
set, we will correct this in order to follow the exact behavior.
</p>
</div>
</div>

<div id="outline-container-sec-3-2-6" class="outline-4">
<h4 id="sec-3-2-6"><span class="section-number-4">3.2.6</span> Download the data sets</h4>
<div class="outline-text-4" id="text-3-2-6">
<p>
In this part we can download the data
</p>

<div class="org-src-container">

<pre class="src src-R"><span style="color: #db7093;">library</span>(parallel)

<span style="color: #4682b4;">if</span>(!file.exists(<span style="color: #ff6a6a;">"../data"</span>)) {
        dir.create(<span style="color: #ff6a6a;">"../data"</span>)
}

fileUrl <span style="color: #db7093;">&lt;-</span> <span style="color: #ff6a6a;">"https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/PollingData_Imputed.csv"</span>

fileName <span style="color: #db7093;">&lt;-</span> <span style="color: #ff6a6a;">"PollingData_Imputed.csv"</span>

dataPath <span style="color: #db7093;">&lt;-</span> <span style="color: #ff6a6a;">"../data"</span>

filePath <span style="color: #db7093;">&lt;-</span> paste(dataPath, fileName, sep = <span style="color: #ff6a6a;">"/"</span>)

<span style="color: #4682b4;">if</span>(!file.exists(filePath)) {
        download.file(fileUrl, destfile = filePath, method = <span style="color: #ff6a6a;">"curl"</span>)
}

list.files(<span style="color: #ff6a6a;">"../data"</span>)
</pre>
</div>

<pre class="example">
 [1] "AirlinesCluster.csv"       "AnonymityPoll.csv"
 [3] "baseball.csv"              "BoeingStock.csv"
 [5] "boston.csv"                "ClaimsData.csv"
 [7] "ClaimsData.csv.zip"        "climate_change.csv"
 [9] "clinical_trial.csv"        "ClusterMeans.ods"
[11] "CocaColaStock.csv"         "CountryCodes.csv"
[13] "CPSData.csv"               "dailykos.csv"
[15] "eBayiPadTest.csv"          "eBayiPadTrain.csv"
[17] "edges.csv"                 "emails.csv"
[19] "energy_bids.csv"           "flower.csv"
[21] "FluTest.csv"               "FluTrain.csv"
[23] "framingham.csv"            "gerber.csv"
[25] "GEStock.csv"               "healthy.csv"
[27] "households.csv"            "IBMStock.csv"
[29] "intl.csv"                  "intlall.csv"
[31] "loans_imputed.csv"         "loans.csv"
[33] "MetroAreaCodes.csv"        "movieLens.txt"
[35] "murders.csv"               "mvt.csv"
[37] "mvtWeek1.csv"              "NBA_test.csv"
[39] "NBA_train.csv"             "parole.csv"
[41] "pisa2009test.csv"          "pisa2009train.csv"
[43] "PollingData_Imputed.csv"   "PollingData.csv"
[45] "PollingImputed.csv"        "ProcterGambleStock.csv"
[47] "quality.csv"               "README.md"
[49] "SampleSubmission.csv"      "songs.csv"
[51] "stevens.csv"               "StocksCluster.csv"
[53] "stopwords.txt"             "SubmissionLR2.csv"
[55] "SubmissionSimpleLogV1.csv" "tumor.csv"
[57] "tweets.csv"                "tweetsU7.csv"
[59] "USDA.csv"                  "users.csv"
[61] "WHO_Europe.csv"            "WHO.csv"
[63] "WHOu7.csv"                 "wiki.csv"
[65] "wine_test.csv"             "wine.csv"
</pre>
</div>
</div>

<div id="outline-container-sec-3-2-7" class="outline-4">
<h4 id="sec-3-2-7"><span class="section-number-4">3.2.7</span> Load the data set</h4>
<div class="outline-text-4" id="text-3-2-7">
<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"    Loading the imputed data into their data frame."</span>)
polling <span style="color: #db7093;">&lt;-</span> read.table(<span style="color: #ff6a6a;">"../data/PollingData_Imputed.csv"</span>, sep = <span style="color: #ff6a6a;">","</span>, header = <span style="color: #3cb371;">TRUE</span>)
str(polling)
table(polling$Year)
summary(polling)
</pre>
</div>

<pre class="example">
    Loading the imputed data into their data frame.
'data.frame':	145 obs. of  7 variables:
 $ State     : Factor w/ 50 levels "Alabama","Alaska",..: 1 1 2 2 3 3 3 4 4 4 ...
 $ Year      : int  2004 2008 2004 2008 2004 2008 2012 2004 2008 2012 ...
 $ Rasmussen : int  11 21 19 16 5 5 8 7 10 13 ...
 $ SurveyUSA : int  18 25 21 18 15 3 5 5 7 21 ...
 $ DiffCount : int  5 5 1 6 8 9 4 8 5 2 ...
 $ PropR     : num  1 1 1 1 1 ...
 $ Republican: int  1 1 1 1 1 1 1 1 1 1 ...

2004 2008 2012
  50   50   45
         State          Year        Rasmussen         SurveyUSA
 Arizona    :  3   Min.   :2004   Min.   :-41.000   Min.   :-33.000
 Arkansas   :  3   1st Qu.:2004   1st Qu.:-10.000   1st Qu.:-11.000
 California :  3   Median :2008   Median :  3.000   Median :  1.000
 Colorado   :  3   Mean   :2008   Mean   :  2.048   Mean   :  1.359
 Connecticut:  3   3rd Qu.:2012   3rd Qu.: 12.000   3rd Qu.: 16.000
 Florida    :  3   Max.   :2012   Max.   : 39.000   Max.   : 30.000
 (Other)    :127
   DiffCount           PropR          Republican
 Min.   :-19.000   Min.   :0.0000   Min.   :0.0000
 1st Qu.: -6.000   1st Qu.:0.0000   1st Qu.:0.0000
 Median :  1.000   Median :0.6250   Median :1.0000
 Mean   : -1.269   Mean   :0.5259   Mean   :0.5103
 3rd Qu.:  4.000   3rd Qu.:1.0000   3rd Qu.:1.0000
 Max.   : 11.000   Max.   :1.0000   Max.   :1.0000
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-3-3" class="outline-3">
<h3 id="sec-3-3"><span class="section-number-3">3.3</span> Video 3: A Sophisticated Baseline Method</h3>
<div class="outline-text-3" id="text-3-3">
<p>
So as usual, the first thing we're going to do is split our data into
a training and a testing set. And for this problem, we're actually
going to train on data from the 2004 and 2008 elections, and we're
going to test on data from the 2012 presidential election.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Subset data into training set and test set"</span>)
Train <span style="color: #db7093;">&lt;-</span> subset(polling, Year == 2004 | Year == 2008)
Test <span style="color: #db7093;">&lt;-</span> subset(polling, Year == 2012)
</pre>
</div>

<pre class="example">
:: Subset data into training set and test set
</pre>

<p>
So now that we've broken it down into a training and a testing set, we
want to understand the prediction of our baseline model against which
we want to compare a later logistic regression model.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Naive Baseline"</span>)
table(Train$Republican)
</pre>
</div>

<pre class="example">
 :: Naive Baseline

 0  1
47 53
</pre>

<p>
What we can see here is that in \(47\) of the \(100\) training observations,
the <b>Democrat</b> won the state, and in \(53\) of the observations, the
<b>Republican</b> won the state. So our <b>simple baseline model is always
going to predict the more common outcome</b>, which is that the
<b>Republican is going to win the state</b>.
</p>

<p>
And we see that the simple baseline model will have accuracy of \(53\%\) on
the training set. Now, unfortunately, this is a pretty weak model. It
always predicts Republican, even for a very landslide Democratic
state, where the Democrat was polling by \(15\%\) or \(20\%\) ahead of the
Republican. So nobody would really consider this to be a credible
model.
</p>

<p>
So we need to think of a smarter baseline model against which we can
compare our logistic regression models that we're going to develop
later.
</p>

<p>
So a reasonable smart baseline would be to just take one of the
polls&#x2013; in our case, we'll take <b>Rasmussen</b>&#x2013; and make a prediction
based on who poll said was winning in the state.
</p>

<p>
So for instance, if the <b>Republican</b> is polling ahead, the <b>Rasmussen</b>
smart baseline would just pick the <b>Republican</b> to be the winner. If the
<b>Democrat</b> was ahead, it would pick the <b>Democrat</b>. And if they were
tied, the model would not know which one to select.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: The Sign function:"</span>)
sign(20)
sign(-10)
sign(0)

writeLines(<span style="color: #ff6a6a;">"\n :: Applying to the training set"</span>)
table(sign(Train$Rasmussen))
</pre>
</div>

<pre class="example">
 :: The Sign function:
[1] 1
[1] -1
[1] 0

 :: Applying to the training set

-1  0  1
42  2 56
</pre>

<p>
So if we passed the <b>Rasmussen</b> variable into <code>sign</code>, whenever the
Republican was winning the state, meaning <b>Rasmussen</b> is positive,
it's going to return a \(1\). So for instance, if the value \(20\) is
passed, meaning the Republican is polling \(20\) ahead, it returns
\(1\). So \(1\) signifies that the <b>Republican</b> is predicted to win.
</p>

<p>
If the <b>Democrat</b> is leading in the <b>Rasmussen</b> poll, it'll take on a
negative value.
</p>

<p>
So if we took for instance the sign of \(-10\), we get \(-1\). So -1 means
this smart baseline is predicting that the Democrat won the state. And
finally, if we took the sign of \(0\), meaning that the <b>Rasmussen</b> poll had
a <b>tie</b>, it returns \(0\), saying that the model is inconclusive about who's
going to win the state.
</p>

<p>
And what we can see is that in \(56\) of the \(100\) training set
observations, the smart baseline predicted that the <b>Republican</b> was
going to win.
</p>

<p>
In \(42\) instances, it predicted the Democrat. And in two instances, it
was inconclusive. So what we really want to do is to see the breakdown
of how the smart baseline model does, compared to the actual result &#x2013;
who actually won the state.
</p>

<p>
So what we really want to do is to see the breakdown of how the smart
baseline model does, compared to the actual result &#x2013; who actually won
the state.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Comparing the smart baseline with the actual result"</span>)
table(Train$Republican, sign(Train$Rasmussen))
</pre>
</div>

<pre class="example">
:: Comparing the smart baseline with the actual result

   -1  0  1
 0 42  1  4
 1  0  1 52
</pre>

<p>
So in this table, the rows are the true outcome \(1\) is for
<b>Republican</b>, \(0\) is for <b>Democrat</b>, and the columns are the smart
baseline predictions, \(-1\), \(0\), or \(1\).
</p>

<p>
We have \(42\) observations where the <b>Rasmussen</b> smart baseline
predicted the <b>Democrat</b> would win, and the <b>Democrat</b> actually did
win.
</p>

<p>
There were \(52\) observations where the smart baseline predicted the
<b>Republican</b> would win, and the <b>Republican</b> actually did win.
</p>

<p>
Again, there were those two inconclusive observations. And finally,
there were four mistakes. There were four times where the smart
baseline model predicted that the <b>Republican</b> would win, but actually
the <b>Democrat</b> won the state.
</p>

<p>
So as we can see, this model, with four mistakes and two inconclusive
results out of the \(100\) training set observations is doing much, much
better than the naive baseline, which simply was always predicting the
<b>Republican</b> would win and made \(47\) mistakes on the same data.
</p>

<p>
Our \(4\) mistakes smart model against the naive model with \(53\)
mistakes.
</p>

<p>
So we see that this is a much more reasonable baseline model to carry
forward, against which we can compare our logistic regression-based
approach.
</p>
</div>
</div>

<div id="outline-container-sec-3-4" class="outline-3">
<h3 id="sec-3-4"><span class="section-number-3">3.4</span> Video 4: Logistic Regression Models</h3>
<div class="outline-text-3" id="text-3-4">
<p>
We need to consider the possibility that there is multicollinearity
within the independent variables.
</p>

<p>
And there's a good reason to suspect that there would be
multicollinearity amongst the variables, because in some sense,
they're all measuring the same thing, which is how strong the
Republican candidate is performing  in the particular state.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Multicollinearity"</span>)
<span style="color: #9932cc;">## </span><span style="color: #ba55d3;">cor(Train)                              # Error because is not numeric</span>
</pre>
</div>

<pre class="example">
:: Multicollinearity
</pre>

<p>
So while normally, we would run the correlation function on the
training set, in this case, it doesn't work. It says, x must be
numeric. And if we go back and look at the structure of the training
set, it jumps out why we're getting this issue. It's because we're
trying to take the correlations of the names of states, which doesn't
make any sense.
</p>

<p>
So to compute the correlation, we're going to want to take the
correlation amongst just the independent variables that we're going to
be using to predict, and we can also add in the dependent variable to
this correlation matrix.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Eliminate the issue:"</span>)
str(Train)

writeLines(<span style="color: #ff6a6a;">"\n :: The correlation between variables:"</span>)
cor(Train[c(<span style="color: #ff6a6a;">"Rasmussen"</span>, <span style="color: #ff6a6a;">"SurveyUSA"</span>, <span style="color: #ff6a6a;">"PropR"</span>, <span style="color: #ff6a6a;">"DiffCount"</span>, <span style="color: #ff6a6a;">"Republican"</span>)])
</pre>
</div>

<pre class="example">
 :: Eliminate the issue:
'data.frame':	100 obs. of  7 variables:
 $ State     : Factor w/ 50 levels "Alabama","Alaska",..: 1 1 2 2 3 3 4 4 5 5 ...
 $ Year      : int  2004 2008 2004 2008 2004 2008 2004 2008 2004 2008 ...
 $ Rasmussen : int  11 21 19 16 5 5 7 10 -11 -27 ...
 $ SurveyUSA : int  18 25 21 18 15 3 5 7 -11 -24 ...
 $ DiffCount : int  5 5 1 6 8 9 8 5 -8 -5 ...
 $ PropR     : num  1 1 1 1 1 1 1 1 0 0 ...
 $ Republican: int  1 1 1 1 1 1 1 1 0 0 ...

 :: The correlation between variables:
           Rasmussen SurveyUSA     PropR DiffCount Republican
Rasmussen  1.0000000 0.9365837 0.8431180 0.5109169  0.7929252
SurveyUSA  0.9365837 1.0000000 0.8616478 0.5222585  0.8101645
PropR      0.8431180 0.8616478 1.0000000 0.8273785  0.9484204
DiffCount  0.5109169 0.5222585 0.8273785 1.0000000  0.8092777
Republican 0.7929252 0.8101645 0.9484204 0.8092777  1.0000000
</pre>

<p>
We're seeing a lot of big values here. For instance, <b>SurveyUSA</b> and
<b>Rasmussen</b> are independent variables that have a correlation of \(0.94\),
which is very, very large and something that would be concerning.
</p>

<p>
So let's first consider the case where we want to build a logistic
regression model with just one variable.
</p>

<p>
So in this case, it stands to reason that the variable we'd want to
add would be the one that is most highly correlated with the outcome,
<b>Republican</b>.
</p>

<p>
So if we read the bottom row, which is the correlation of each
variable to <b>Republican</b>, we see that <b>PropR</b> is probably the best
candidate to include in our single-variable model, because it's so
highly correlated, meaning it's going to do a good job of predicting
the <b>Republican</b> status.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Logistic Regression Model 1:"</span>)
mod1 <span style="color: #db7093;">&lt;-</span> glm(Republican ~ PropR, data = Train, family = <span style="color: #ff6a6a;">"binomial"</span>)
summary(mod1)
</pre>
</div>

<pre class="example">
 :: Logistic Regression Model 1:

Call:
glm(formula = Republican ~ PropR, family = "binomial", data = Train)

Deviance Residuals:
     Min        1Q    Median        3Q       Max
-2.22880  -0.06541   0.10260   0.10260   1.37392

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   -6.146      1.977  -3.108 0.001882 **
PropR         11.390      3.153   3.613 0.000303 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.269  on 99  degrees of freedom
Residual deviance:  15.772  on 98  degrees of freedom
AIC: 19.772

Number of Fisher Scoring iterations: 8
</pre>

<p>
And we can see that it looks pretty nice in terms of its significance
and the sign of the coefficients. PropR is the proportion of the polls
that said the Republican won.
</p>

<p>
We see that that has a very high coefficient in terms of predicting
that the Republican will win in the state, which makes a lot of
sense.
</p>

<p>
And we'll note down that the AIC measuring the strength of the model
is \(19.8\). So this seems like a very reasonable model. Let's see how it
does in terms of actually predicting the <b>Republican</b> outcome on the
training set.
</p>

<p>
we want to compute the predictions, the predicted probabilities that
the Republican is going to win on the training set. We do need to pass
it type = "response" to get probabilities out as the predictions.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Training set predictions"</span>)
pred1 <span style="color: #db7093;">&lt;-</span> predict(mod1, type=<span style="color: #ff6a6a;">"response"</span>)
table(Train$Republican, pred1 &gt;= 0.5)
</pre>
</div>

<pre class="example">
:: Training set predictions

   FALSE TRUE
 0    45    2
 1     2   51
</pre>

<p>
The rows, as usual, are the outcome \(1\) is <b>Republican</b>, \(0\) is
Democrat. And the columns <b>TRUE</b> means that we predicted <b>Republican</b>,
<b>FALSE</b> means we predicted <b>Democrat</b>. So we see that on the training
set, this model with one variable as a prediction makes four mistakes,
which is just about the same as our smart baseline model.
</p>

<p>
So now, let's see if we can improve on this performance by adding in
another variable.
</p>

<p>
We might be searching for a pair of variables that has a relatively
lower correlation with each other, because they might kind of work
together to improve the prediction overall of the <b>Republican</b>
outcome.
</p>

<p>
The basically the least correlated pairs of variables are either
<b>Rasmussen and DiffCount</b>, or <b>SurveyUSA and DiffCount</b>.
</p>

<p>
So the idea would be to try out one of these pairs in our two-variable
model. So we'll go ahead and try out <b>SurveyUSA and DiffCount</b>
together in our second model. So to save ourselves some typing, we can
hit up a few times until we get to the model definition for model
one. And then we can just change the variables. In this case, we're
now using <b>SurveyUSA plus DiffCount</b>.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Two-variable model"</span>)
mod2 <span style="color: #db7093;">&lt;-</span> glm(Republican ~ SurveyUSA + DiffCount, data = Train, family = <span style="color: #ff6a6a;">"binomial"</span>)
pred2 <span style="color: #db7093;">&lt;-</span> predict(mod2, type = <span style="color: #ff6a6a;">"response"</span>)
table(Train$Republican, pred2 &gt;= 0.5)
summary(mod2)
</pre>
</div>

<pre class="example">
 :: Two-variable model

    FALSE TRUE
  0    45    2
  1     1   52

Call:
glm(formula = Republican ~ SurveyUSA + DiffCount, family = "binomial",
    data = Train)

Deviance Residuals:
     Min        1Q    Median        3Q       Max
-2.04741  -0.00977   0.00561   0.03751   1.32999

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -0.6827     1.0468  -0.652   0.5143
SurveyUSA     0.3309     0.2226   1.487   0.1371
DiffCount     0.6619     0.3663   1.807   0.0708 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.269  on 99  degrees of freedom
Residual deviance:  11.154  on 97  degrees of freedom
AIC: 17.154

Number of Fisher Scoring iterations: 9
</pre>

<p>
We can see that we made one less mistake. We made three mistakes
instead of four on the training set so a little better than the
smart baseline but nothing too impressive.
</p>

<ul class="org-ul">
<li>The AIC has a smaller value, which suggests a stronger model.
</li>

<li><b>SurveyUSA and DiffCount</b> both have positive coefficients in
predicting if the Republican wins the state, which makes sense.
</li>

<li>But a weakness of this model is that neither of these variables has
a significance of a star or better, which means that they are less
significant statistically.
</li>
</ul>

<p>
So there are definitely some strengths and weaknesses between the
two-variable and the one-variable model. We'll go ahead and use the
two-variable model when we make our predictions on the testing set.
</p>
</div>
</div>

<div id="outline-container-sec-3-5" class="outline-3">
<h3 id="sec-3-5"><span class="section-number-3">3.5</span> Video 5: Test Set Predictions</h3>
<div class="outline-text-3" id="text-3-5">
</div><div id="outline-container-sec-3-5-1" class="outline-4">
<h4 id="sec-3-5-1"><span class="section-number-4">3.5.1</span> Smart baseline model testing</h4>
<div class="outline-text-4" id="text-3-5-1">
<p>
The first model we're going to want to look at is that smart baseline
model that basically just took a look at the polling results from the
<b>Rasmussen</b> poll and used those to determine who was predicted to win
the election.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Smart baseline accuracy:"</span>)
table(Test$Republican, sign(Test$Rasmussen))
</pre>
</div>

<pre class="example">
:: Smart baseline accuracy:

   -1  0  1
 0 18  2  4
 1  0  0 21
</pre>

<p>
We can see that for these results, there are 18 times where the smart
baseline predicted that the <b>Democrat</b> would win and it's correct, \(21\)
where it predicted the <b>Republican</b> would win and was correct, two times
when it was inconclusive, and four times where it predicted <b>Republican</b>
but the <b>Democrat</b> actually won. So that's four mistakes and two
inconclusive results on the testing set.
</p>

<p>
So this is going to be what we're going to compare our logistic
regression-based model against. So we need to obtain final testing set
prediction from our model.
</p>
</div>
</div>

<div id="outline-container-sec-3-5-2" class="outline-4">
<h4 id="sec-3-5-2"><span class="section-number-4">3.5.2</span> Logistic Regression Model testing</h4>
<div class="outline-text-4" id="text-3-5-2">
<p>
So we need to obtain final testing set prediction from our model. So
we selected mod2, which was the two variable model.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Test set predictions:"</span>)
TestPrediction <span style="color: #db7093;">&lt;-</span> predict(mod2, newdata = Test, type = <span style="color: #ff6a6a;">"response"</span>)
table(Test$Republican, TestPrediction &gt;= 0.5)
</pre>
</div>

<pre class="example">
:: Test set predictions:

   FALSE TRUE
 0    23    1
 1     0   21
</pre>

<p>
And the moment of truth, we're finally going to table the test set
<b>Republican</b> value against the test prediction being greater than or
equal to \(0.5\), at least a \(50\%\) probability of the <b>Republican</b>
winning.
</p>

<p>
<b>And we see that for this particular case, in all but one of the \(45\)
observations in the testing set, we're correct</b>.
</p>

<p>
Now, we could have tried changing this threshold from \(0.5\) to other
values and computed out an ROC curve, but that doesn't quite make as
much sense in this setting where we're just trying to accurately
predict the outcome of each state and we don't care more about one
sort of error&#x2013; when we predicted <b>Republican</b> and it was <b>actually
Democrat</b> &#x2013; than the other, where we predicted <b>Democrat</b> and it was
actually <b>Republican</b>.
</p>

<p>
<b>So in this particular case, we feel OK just using the cutoff of 0.5
 to evaluate our model</b>.
</p>
</div>
</div>

<div id="outline-container-sec-3-5-3" class="outline-4">
<h4 id="sec-3-5-3"><span class="section-number-4">3.5.3</span> Analyze mistake</h4>
<div class="outline-text-4" id="text-3-5-3">
<p>
So let's take a look now at the <b>mistake</b> we made and see if we can
understand what's going on. So to actually pull out the mistake we
made, we can just take a subset of the testing set and limit it to
when we predicted true, but actually the Democrat won, which is the
case when that one failed.
</p>

<div class="org-src-container">

<pre class="src src-R">writeLines(<span style="color: #ff6a6a;">"\n :: Analyze mistake:"</span>)
subset(Test, TestPrediction &gt;= 0.5 &amp; Republican == 0)
</pre>
</div>

<pre class="example">
 :: Analyze mistake:
     State Year Rasmussen SurveyUSA DiffCount     PropR Republican
24 Florida 2012         2         0         6 0.6666667          0
</pre>

<p>
So here is that subset, which just has one observation since we made
just one mistake. So this was for the year 2012, the testing set
year. This was the state of Florida.
</p>

<p>
And looking through these predictor variables, we see why we made the
mistake. The <b>Rasmussen</b> poll gave the <b>Republican</b> a two percentage point
lead, <b>SurveyUSA</b> called a <b>tie</b>, <b>DiffCount</b> said there were six more polls
that predicted <b>Republican</b> than <b>Democrat</b>, and two thirds of the polls
predicted the Republican was going to win. But actually in this case,
the Republican didn't win. Barack Obama won the state of Florida in
2012 over Mitt Romney.
</p>

<p>
The models here are not magic, and given this sort of data, it's
pretty unsurprising that our model actually didn't get Florida correct
in this case and made the mistake.
</p>

<p>
However, overall, it seems to be <b>outperforming the smart baseline</b>
that we selected, and so we think that maybe this would be a nice
model to use in the election prediction.
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 26/06/2015</p>
<p class="author">Author: Sergio-Feliciano Mendoza-Barrera</p>
<p class="date">Created: 2015-07-30 Thu 07:25</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 24.5.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
