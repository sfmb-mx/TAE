#+TITLE:         Unit 6 - Clustering
#+AUTHOR:        Sergio-Feliciano Mendoza-Barrera
#+DRAWERS:       sfmb
#+EMAIL:         smendoza.barrera@gmail.com
#+DATE:          14/07/2015
#+DESCRIPTION:   Clustering methods in Machine Learning
#+KEYWORDS:      R, data science, emacs, ESS, org-mode, machine learning, clustering
#+LANGUAGE:      en
#+OPTIONS:       H:10 num:t toc:nil \n:nil @:t ::t |:t ^:{} -:t f:t *:t <:t d:HIDDEN
#+OPTIONS:       TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+OPTIONS:       LaTeX:dvipng
#+INFOJS_OPT:    view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:
#+LINK_HOME:
#+XSLT:
#+STYLE: <link rel="stylesheet" type="text/css" href="dft.css"/>

#+LaTeX_CLASS: IEEEtran
#+LATEX_CLASS_OPTIONS: [letterpaper, 9pt, onecolumn, twoside, technote, final]
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage{makeidx}

#+LATEX_HEADER: \usepackage[lining,tabular]{fbb} % so math uses tabular lining figures
#+LATEX_HEADER: \usepackage[scaled=.95,type1]{cabin} % sans serif in style of Gill Sans
#+LATEX_HEADER: \usepackage[varqu,varl]{zi4}% inconsolata typewriter
#+LATEX_HEADER: \usepackage[T1]{fontenc} % LY1 also works
#+LATEX_HEADER: \usepackage[libertine,bigdelims]{newtxmath}
#+LATEX_HEADER: \usepackage[cal=boondoxo,bb=boondox,frak=boondox]{mathalfa}
#+LATEX_HEADER: \useosf % change normal text to use proportional oldstyle figures

#+LATEX_HEADER: \markboth{Unit - Clustering. July 2015.}%
#+LATEX_HEADER: {Sergio-Feliciano Mendoza-Barrera}

#+LATEX_HEADER: \newcommand{\degC}{$^\circ$C{}}

#+STYLE: <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script>

#+ATTR_HTML: width="500px"

# -*- mode: org; -*-
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/htmlize.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/bigblow.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/hideshow.css"/>

#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery-1.11.0.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery-ui-1.10.2.min.js"></script>

#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.localscroll-min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.scrollTo-1.4.3.1-min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.zclip.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/bigblow.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/hideshow.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.min.js"></script>

#+BEGIN_ABSTRACT
This document concentrate the work of clustering in the TEA course.
#+END_ABSTRACT

* Introduction

This week we'll discuss how analytics can be used to make
recommendations for movies and for health. In the first lecture, we
discuss how Netflix offered the million dollar prize to improve their
movie recommendation system.

In the second lecture, we discuss how health care claims data can be
used to predict the occurrence of a heart attack Through these
examples, we'll discuss the method of clustering, which is used to
find similarities and patterns in data.

* Recommendations Worth a Million: An Introduction to Clustering

[[../graphs/Netflix.png]]

** What is Netflix?

[[../graphs/Netflix02.png]]

The recommendation systems in the Netflix business permit to the
executives take some decisions about the tendencies of the market. The
operating people is able to decide what kind of movies are potentially
more interesting to their customers.

Have a movie theater for each customer. In order to manage the big
volume of customers, Netflix needs an automated recommendation
system. This system allow the company to select and propose the best
titles for an specific customer.

The initial algorithm proposed could be improved, then the Netflix
price was open in order to develop a better algorithm.

[[../graphs/NetflixPrize.png]]

The contest rules:

[[../graphs/NetflixPrize02.png]]

Some initial and early results.

[[../graphs/NF-InitialResults.png]]

Then the history of the contest.

[[../graphs/NF-Progress.png]]

One of the best teams.

[[../graphs/NF-Progress02.png]]

The winner!

[[../graphs/NF-LastCall.png]]

Netflix was willing to pay over one million dollars for the best user
rating algorithm, which shows how critical the recommendation system
is to their business.

We will discuss how recommendation systems work. Let's start by
thinking about the data. When predicting user ratings, what data could
be useful? There are two main types of data that we could use. The
first is that for every movie in Netflix's database, we have a ranking
from all users who have ranked that movie. The second is that we know
facts about the movie itself-- the actors in the movie, the director,
the genre classifications of the movie, the year it was released,
etc.

[[../graphs/NF-PredictingBestUserRatings.png]]

As an example, suppose we have the following user ratings for four
users and four movies.

The ratings are on a one to five scale, where one is the lowest rating
and five is the highest rating. The blank entries mean that the user
has not rated the movie. We could suggest to Carl that he watch Men in
Black, since Amy rated it highly. She gave it a rating of five, and
*Amy and Carl seem to have similar ratings for the other movies*.

This technique of using other user's ratings to make predictions is
called collaborative filtering.

[[../graphs/NF-CollaborativeFiltering.png]]

Note that we're not using any information about the movie itself here,
just the similarity between users.

We saw in the table that Amy liked Men in Black. She gave it a rating
of five. We know that this movie was directed by Barry Sonnenfeld, is
classified in the genres of action, adventure, sci-fi, and comedy, and
it stars actor Will Smith. Based on this information, we could make
recommendations to Amy.

We could recommend to Amy another movie by the same director, Barry
Sonnenfeld's movie, Get Shorty. We can instead recommend the movie
Jurassic Park, which is also classified in the genres of action,
adventure, and sci-fi. Or we could recommend to Amy another movie
starring Will Smith-- Hitch.

Note that we're not using the ratings of other users at all here, just
information about the movie. This technique is called content
filtering.

[[../graphs/NF-ContentFiltering.png]]

There are strengths and weaknesses to both types of recommendation
systems.

Collaborative filtering can accurately suggest complex items without
understanding the nature of the items. It didn't matter at all that
our items were movies in the collaborative filtering example.

We were just comparing user ratings. However, this requires a lot of
data about the user to make accurate recommendations. Also, when there
are millions of items, it needs a lot of computing power to compute
the user similarities.

[[../graphs/NF-StrengthsWeaknesses.png]]

On the other hand, content filtering requires very little data to get
started. But the major weakness of content filtering is that it can be
limited in scope.

You're only recommending similar things to what the user has already
liked. So the recommendations are often not surprising or particularly
insightful.

Netflix actually uses what's called a hybrid recommendation
system. They use both collaborative and content filtering.

[[../graphs/NF-HybridRecommendationSystems.png]]

If we were only doing collaborative filtering, one of them would have
had to have seen it before. And if we were only doing content
filtering, we would only be recommending to one user at a time.

So by combining the two methods, the algorithm can be much more
efficient and accurate. In the next video, we'll see how we can do
content filtering by using a method called clustering.

** Quick Question (2/2 points)

Let's consider a recommendation system on Amazon.com, an online retail
site.

*** Question a
If Amazon.com constructs a recommendation system for books, and would
like to use the same exact algorithm for shoes, what type would it
have to be?

**** Answer

Collaborative Filtering.

*** Question b

If Amazon.com would like to suggest books to users based on the
previous books they have purchased, what type of recommendation system
would it be?

**** Answer

Content Filtering.

*Explanation*

In the first case, the recommendation system would have to be
collaborative filtering, since it can't use information about the
items. In the second case, the recommendation system would be content
filtering since other users are not involved.

** Video 3: Movie Data and Clustering

We will be using data from *MovieLens* to explain clustering and
perform content filtering.

*Movielens.org* is a movie recommendation website run by the GroupLens
research lab at the University of Minnesota. They collect user
preferences about movies and do collaborative filtering to make
recommendations to users, based on the similarities between users.

We'll use their movie database to do content filtering using a
technique called clustering.

[[../graphs/MovieLens-Data.png]]

First, let's discuss what data we have. Movies in the MovieLens data
set are categorized as belonging to different genres.

There are 18 different genres as well as an unknown category. The
genres include crime, musical, mystery, and children's. Each movie may
belong to many different genres. So a movie could be classified as
drama, adventure, and sci-fi.

The question we want to answer is, can we systematically find groups
of movies with similar sets of genres? To answer this question, we'll
use a method called clustering.

[[../graphs/MovieLens-ItemDataset.png]]

To answer this question, we'll use a method called
clustering. Clustering is different from the other analytics methods
we've covered so far. It's called an unsupervised learning
method. This means that we're just trying to segment the data into
similar groups, instead of trying to predict an outcome. In this image
on the slide, based on the locations of points, we've divided them
into three clusters-- a blue cluster, a red cluster, and a yellow
cluster.

[[../graphs/ML-WhyClustering.png]]

This is the goal of clustering-- to put each data point into a group
with similar values in the data. A clustering algorithm does not
predict anything. However, clustering can be used to improve
predictive methods.

You can cluster the data into similar groups and then build a
predictive model for each group. This can often improve the accuracy
of predictive methods. But as a warning, be careful not to over-fit
your model to the training set. This works best for large data sets.

There are many different algorithms for clustering. They differ in
what makes a cluster and how the clusters are found.

[[../graphs/ML-ClusteringMethods.png]]

You'll learn how to create clusters using either method in R. There
are other clustering methods also, but hierarchical and K-means are
two of the most popular methods. To cluster data points, we need to
compute how similar the points are. This is done by computing the
distance between points.

** Quick Question (1 point possible)

In the previous video, we discussed how clustering is used to split
the data into similar groups. Which of the following tasks do you
think are appropriate for clustering? Select all that apply.

*** Answer [2/3]

- [X] Dividing search results on Google into categories based on the
  topic.

- [X] Grouping players into different "types" of basketball players
  that make it to the NBA.

- [ ] Predicting the winner of the Major League Baseball World
  Series.

*Explanation*

The first two options are appropriate tasks for clustering. Clustering
probably wouldn't help us predict the winner of the World Series.

** Video 4: Computing Distances

*So how does clustering work?* The first step in clustering is to
define the distance between two data points. The most popular way to
compute the distance is what's called Euclidean distance. This is the
standard way to compute distance that you might have seen before.

[[../graphs/ML-Distance.png]]

The distance between the two points, which we'll call dij, is equal to
the square root of the difference between the two points in the first
component, squared, plus the difference between the two points in the
second component, squared, all the way up to the difference between
the two points in the k-th component, squared, where k here is the
number of attributes or independent variables.

Let's see how this works by looking at an example. In our movie lens
dataset, we have binary vectors for each movie, classifying that movie
into genres. The movie Toy Story is categorized as an animation,
comedy, and children's movie.

So the data for Toy Story has a 1 in the spot for these three genres
and a 0 everywhere else.

[[../graphs/ML-DistanceExample.png]]

The movie Batman Forever is categorized as an action, adventure,
comedy, and crime movie. So Batman Forever has a 1 in the spot for
these four genres and a 0 everywhere else.

So given these two data observations, let's compute the distance
between them.

[[../graphs/ML-DistanceExample02.png]]

In addition to Euclidean distance, there are many other popular
distance metrics that could be used. One is called Manhattan distance,
where the distance is computed to be the sum of the absolute values
instead of the sum of squares.

Another is called maximum coordinate distance, where we only consider
the measurement for which the data points deviate the most.

Another important distance that we have to calculate for clustering is
the distance between clusters, when a cluster is a group of data
points.

We just discussed how to compute the distance between two individual
points, but how do we compute the distance between groups of points?

One way of doing this is by using what's called the *minimum
distance*. This defines the *distance between clusters* as the
distance between the two data points in the clusters that are closest
together.

[[../graphs/NF-DistanceBetweenClusters.png]]

For example, we would define the distance between the yellow and red
clusters by computing the Euclidean distance between these two
(marked) points.

The other points in the clusters could be really far away, but it
doesn't matter if we use minimum distance. The only thing we care
about is how close together the closest points are.

Alternatively, we could use maximum distance. This one computes the
distance between the two clusters as the distance between the two
points that are the farthest apart.

So for example, we would compute the distance between the yellow and
red clusters by looking at these two points. Here, it doesn't matter
how close together the other points are. All we care about is how
close together the furthest points are.

[[../graphs/NF-DistanceBetweenClusters02.png]]

The most common distance metric between clusters is called centroid
distance. And this is what we'll use. It defines the distance between
clusters by computing the centroid of the clusters.

[[../graphs/NF-DistanceBetweenClusters03.png]]

The centroid is just the data point that takes the average of all data
points in each component. This takes all data points in each cluster
into account and can be thought of as the middle data point.

In our example, the centroids between yellow and red are here, and we
would compute the distance between the clusters by computing the
Euclidean distance between those two points.

When we are computing distances, it's highly influenced by the scale
of the variables.

[[../graphs/NF-NormalizeData.png]]

As an example, suppose you're computing the distance between two data
points, where one variable is the revenue of a company in thousands of
dollars, and another is the age of the company in years. The revenue
variable would really dominate in the distance calculation.

The differences between the data points for revenue would be in the
thousands. Whereas the differences between the year variable would
probably be less than 10.

To handle this, it's *customary to normalize the data first*. We can
normalize by subtracting the mean of the data and dividing by the
standard deviation.

In our movie data set, all of our genre variables are on the same
scale. So we don't have to worry about normalizing. But if we wanted
to add a variable, like box office revenue, we would need to normalize
so that this variable didn't dominate all of the others.

Now that we've defined how we'll compute the distances, we'll talk
about a specific clustering algorithm: *hierarchical clustering*.

** Quick Question (1 point possible)

The movie "The Godfather" is in the genres action, crime, and drama,
and is defined by the vector: (0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0)

The movie "Titanic" is in the genres action, drama, and romance, and
is defined by the vector: (0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0)

What is the distance between "The Godfather" and "Titanic", using
euclidean distance?

#+begin_src R :session :results output :exports all
  writeLines("\n :: Euclidean distance:")
  v1 <- c(0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0)
  v2 <- c(0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0)
  d <- sqrt(sum((v1 - v2)^2))
  d
#+end_src

#+RESULTS:
:
:  :: Euclidean distance:
: [1] 1.414214

*** Answer

*Explanation*

The distance between these two movies is $\sqrt{2}$. They have a
difference of 1 in two genres - crime and romance.

** Video 5: Hierarchical Clustering

In hierarchical clustering, the clusters are formed by each data point
starting in its own cluster. As a small example, suppose we have five
data points. Each data point is labeled as belonging in its own
cluster. So this data point is in the red cluster, this one's in the
blue cluster, this one's in the purple cluster, this one's in the
green cluster, and this one's in the yellow cluster.

[[../graphs/NF-Hierarchical.png]]

Then hierarchical clustering combines the two nearest clusters into
one cluster. We'll use Euclidean and Centroid distances to decide
which two clusters are the closest. In our example, the green and
yellow clusters are closest together.

[[../graphs/NF-Hierarchical02.png]]

So we would combine these two clusters into one cluster. So now the
green cluster has two points, and the yellow cluster is gone.

[[../graphs/NF-Hierarchical03.png]]

Now this process repeats. We again find the two nearest clusters,
which this time are the green cluster and the purple cluster, and we
combine them into one cluster.

[[../graphs/NF-Hierarchical04.png]]

Now the green cluster has three points, and the purple cluster is
gone.

[[../graphs/NF-Hierarchical05.png]]

Now the two nearest clusters are the red and blue clusters. So we
would combine these two clusters into one cluster, the red cluster.

[[../graphs/NF-Hierarchical06.png]]

So now we have just two clusters, the red one and the green one.

[[../graphs/NF-Hierarchical07.png]]

So now the final step is to combine these two clusters into one
cluster.

[[../graphs/NF-Hierarchical08.png]]

So at the end of hierarchical clustering, all of our data points are
in a single cluster.

[[../graphs/NF-Hierarchical09.png]]

The hierarchical cluster process can be displayed through what's
called a *dendrogram*. The data points are listed along the bottom, and
the lines show how the clusters were combined. The height of the lines
represents how far apart the clusters were when they were combined.

So points 1 and 4 were pretty close together when they were
combined. But when we combined the two clusters at the end, they were
significantly farther apart. We can use a dendrogram to decide how
many clusters we want for our final clustering model.

[[../graphs/NF-DisplayClusterProcess.png]]

This dendrogram shows the clustering process with ten data points. The
easiest way to pick the number of clusters you want is to draw a
horizontal line across the dendrogram.

The number of vertical lines that line crosses is the number of
clusters there will be. In this case, our line crosses two vertical
lines, meaning that we will have two clusters-- one cluster with
points 5, 2, and 7, and one cluster with the remaining points.

[[../graphs/NF-SelectClusters.png]]

The farthest this horizontal line can move up and down in the
dendrogram without hitting one of the horizontal lines of the
dendrogram, the better that choice of the number of clusters is.

If we instead selected three clusters, this line can't move as far up
and down without hitting horizontal lines in the dendrogram. This
probably means that the *two cluster choice is better*.

But when picking the number of clusters, *you should also consider how
many clusters make sense* for the particular *application you're
working with*.

After selecting the number of clusters you want, you should analyze
your clusters to see *if they're meaningful*.

You can also check to see if the clusters have a feature in common
that was not used in the clustering, like an outcome variable. This
often indicates that your clusters might help improve a predictive
model.

[[../graphs/NF-MeaningfulClusters.png]]

** Quick Question (2 points possible)

Suppose you are running the Hierarchical clustering algorithm with 212
observations.

*** Question a

How many clusters will there be at the start of the algorithm?

**** Answer

212

*** Question b

How many clusters will there be at the end of the algorithm?

*** Answer

1

*Explanation*

The Hierarchical clustering algorithm always starts with each data
point in its own cluster, and ends with all data points in the same
cluster. So there will be 212 clusters at the beginning of the
algorithm, and 1 cluster at the end of the algorithm.

** Video 6: Getting the Data

We'll be downloading our dataset from the MovieLens website. Please
open the following link in a new window or tab of your browser to
access the [[http://files.grouplens.org/datasets/movielens/ml-100k/u.item][data]]:

This video will show you how to load the data into R.

An R script file with all of the commands used in this Lecture can be
downloaded [[https://courses.edx.org/asset-v1:MITx%2B15.071x_2a%2B2T2015%2Btype@asset%2Bblock/Unit6_Netflix.R][here]].

IMPORTANT NOTE: We'll be using a text editor in this video to get the
data into R. If you are on a Mac and are using TextEdit, the default
file type is .rtf, so you will need to change the file type to txt. To
do this, just go to Format --> Make Plain Text, and the file will
re-save as a txt file. Alternatively, depending on your operating
system and web browser, you might just be able to save the file
directly from the browser as a txt file.

*** Download the data sets

In this part we can download the data

#+BEGIN_SRC R :session :results output :exports all
  library(parallel)

  if(!file.exists("../data")) {
          dir.create("../data")
  }

  fileUrl <- "http://files.grouplens.org/datasets/movielens/ml-100k/u.item"

  fileName <- "movieLens.txt"

  dataPath <- "../data"

  filePath <- paste(dataPath, fileName, sep = "/")

  if(!file.exists(filePath)) {
          download.file(fileUrl, destfile = filePath, method = "curl")
  }

  list.files("../data")
#+END_SRC

#+RESULTS:
#+begin_example
 [1] "AnonymityPoll.csv"       "baseball.csv"
 [3] "BoeingStock.csv"         "boston.csv"
 [5] "ClaimsData.csv"          "ClaimsData.csv.zip"
 [7] "climate_change.csv"      "clinical_trial.csv"
 [9] "ClusterMeans.ods"        "CocaColaStock.csv"
[11] "CountryCodes.csv"        "CPSData.csv"
[13] "emails.csv"              "energy_bids.csv"
[15] "flower.csv"              "FluTest.csv"
[17] "FluTrain.csv"            "framingham.csv"
[19] "gerber.csv"              "GEStock.csv"
[21] "healthy.csv"             "IBMStock.csv"
[23] "loans_imputed.csv"       "loans.csv"
[25] "MetroAreaCodes.csv"      "movieLens.txt"
[27] "mvtWeek1.csv"            "NBA_test.csv"
[29] "NBA_train.csv"           "parole.csv"
[31] "pisa2009test.csv"        "pisa2009train.csv"
[33] "PollingData_Imputed.csv" "PollingData.csv"
[35] "ProcterGambleStock.csv"  "quality.csv"
[37] "README.md"               "songs.csv"
[39] "stevens.csv"             "stopwords.txt"
[41] "tumor.csv"               "tweets.csv"
[43] "USDA.csv"                "WHO_Europe.csv"
[45] "WHO.csv"                 "wiki.csv"
[47] "wine_test.csv"           "wine.csv"
#+end_example

*** Load the data set

#+BEGIN_SRC R :session :results output :exports all
  writeLines("    Loading data into their data frames.")
  movies <- read.table("../data/movieLens.txt", header = FALSE,
                       sep = "|", quote = "\"")
  str(movies)
#+END_SRC

#+RESULTS:
#+begin_example
    Loading data into their data frames.
'data.frame':	1682 obs. of  24 variables:
 $ V1 : int  1 2 3 4 5 6 7 8 9 10 ...
 $ V2 : Factor w/ 1664 levels "'Til There Was You (1997)",..: 1525 618 555 594 344 1318 1545 111 391 1240 ...
 $ V3 : Factor w/ 241 levels "","01-Aug-1997",..: 71 71 71 71 71 71 71 71 71 182 ...
 $ V4 : logi  NA NA NA NA NA NA ...
 $ V5 : Factor w/ 1661 levels "","http://us.imdb.com/M/title-exact?%22Langoliers,%20The%22%20(1995)%20(mini)",..: 1430 564 504 542 309 1661 1452 102 356 1182 ...
 $ V6 : int  0 0 0 0 0 0 0 0 0 0 ...
 $ V7 : int  0 1 0 1 0 0 0 0 0 0 ...
 $ V8 : int  0 1 0 0 0 0 0 0 0 0 ...
 $ V9 : int  1 0 0 0 0 0 0 0 0 0 ...
 $ V10: int  1 0 0 0 0 0 0 1 0 0 ...
 $ V11: int  1 0 0 1 0 0 0 1 0 0 ...
 $ V12: int  0 0 0 0 1 0 0 0 0 0 ...
 $ V13: int  0 0 0 0 0 0 0 0 0 0 ...
 $ V14: int  0 0 0 1 1 1 1 1 1 1 ...
 $ V15: int  0 0 0 0 0 0 0 0 0 0 ...
 $ V16: int  0 0 0 0 0 0 0 0 0 0 ...
 $ V17: int  0 0 0 0 0 0 0 0 0 0 ...
 $ V18: int  0 0 0 0 0 0 0 0 0 0 ...
 $ V19: int  0 0 0 0 0 0 0 0 0 0 ...
 $ V20: int  0 0 0 0 0 0 0 0 0 0 ...
 $ V21: int  0 0 0 0 0 0 1 0 0 0 ...
 $ V22: int  0 1 1 0 1 0 0 0 0 0 ...
 $ V23: int  0 0 0 0 0 0 0 0 0 1 ...
 $ V24: int  0 0 0 0 0 0 0 0 0 0 ...
#+end_example

We need one more argument, which is quote="\"". Close the parentheses,
and hit Enter. That last argument just made sure that our text was
read in properly.

Let's take a look at the structure of our data using the ~str~
function. We have 1,682 observations of 24 different variables.

Since our variables didn't have names, header equaled false, R just
labeled them with V1, V2, V3, etc.

But from the Movie Lens documentation, we know what these variables
are. So we'll go ahead and add in the column names ourselves.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Add column names:")
  colnames(movies) = c("ID", "Title", "ReleaseDate", "VideoReleaseDate",
                       "IMDB", "Unknown", "Action", "Adventure",
                       "Animation", "Childrens", "Comedy", "Crime",
                       "Documentary", "Drama", "Fantasy", "FilmNoir",
                       "Horror", "Musical", "Mystery", "Romance",
                       "SciFi", "Thriller", "War", "Western")

  names(movies)
#+end_src

#+RESULTS:
:
:  :: Add column names:
:  [1] "ID"               "Title"            "ReleaseDate"      "VideoReleaseDate"
:  [5] "IMDB"             "Unknown"          "Action"           "Adventure"
:  [9] "Animation"        "Childrens"        "Comedy"           "Crime"
: [13] "Documentary"      "Drama"            "Fantasy"          "FilmNoir"
: [17] "Horror"           "Musical"          "Mystery"          "Romance"
: [21] "SciFi"            "Thriller"         "War"              "Western"

We won't be using the ID, release date, video release date, or IMDB
variables, so let's go ahead and remove them.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Remove unnecessary variables:")
  movies$ID = NULL
  movies$ReleaseDate = NULL
  movies$VideoReleaseDate = NULL
  movies$IMDB = NULL

  names(movies)
#+end_src

#+RESULTS:
:
:  :: Remove unnecessary variables:
:  [1] "Title"       "Unknown"     "Action"      "Adventure"   "Animation"
:  [6] "Childrens"   "Comedy"      "Crime"       "Documentary" "Drama"
: [11] "Fantasy"     "FilmNoir"    "Horror"      "Musical"     "Mystery"
: [16] "Romance"     "SciFi"       "Thriller"    "War"         "Western"

And there are a few duplicate entries in our data set, so we'll go
ahead and remove them with the ~unique~ function.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Remove duplicates:")
  movies = unique(movies)

  writeLines("\n :: Take a look at our data again:")
  str(movies)
#+end_src

#+RESULTS:
#+begin_example

 :: Remove duplicates:

 :: Take a look at our data again:
'data.frame':	1664 obs. of  20 variables:
 $ Title      : Factor w/ 1664 levels "'Til There Was You (1997)",..: 1525 618 555 594 344 1318 1545 111 391 1240 ...
 $ Unknown    : int  0 0 0 0 0 0 0 0 0 0 ...
 $ Action     : int  0 1 0 1 0 0 0 0 0 0 ...
 $ Adventure  : int  0 1 0 0 0 0 0 0 0 0 ...
 $ Animation  : int  1 0 0 0 0 0 0 0 0 0 ...
 $ Childrens  : int  1 0 0 0 0 0 0 1 0 0 ...
 $ Comedy     : int  1 0 0 1 0 0 0 1 0 0 ...
 $ Crime      : int  0 0 0 0 1 0 0 0 0 0 ...
 $ Documentary: int  0 0 0 0 0 0 0 0 0 0 ...
 $ Drama      : int  0 0 0 1 1 1 1 1 1 1 ...
 $ Fantasy    : int  0 0 0 0 0 0 0 0 0 0 ...
 $ FilmNoir   : int  0 0 0 0 0 0 0 0 0 0 ...
 $ Horror     : int  0 0 0 0 0 0 0 0 0 0 ...
 $ Musical    : int  0 0 0 0 0 0 0 0 0 0 ...
 $ Mystery    : int  0 0 0 0 0 0 0 0 0 0 ...
 $ Romance    : int  0 0 0 0 0 0 0 0 0 0 ...
 $ SciFi      : int  0 0 0 0 0 0 1 0 0 0 ...
 $ Thriller   : int  0 1 1 0 1 0 0 0 0 0 ...
 $ War        : int  0 0 0 0 0 0 0 0 0 1 ...
 $ Western    : int  0 0 0 0 0 0 0 0 0 0 ...
#+end_example

Let's take a look at our data one more time. Now, we have 1,664
observations, a few less than before, and 20 variables-- the title of
the movie, the unknown genre label, and then the 18 other genre
labels.

** Quick Question (3 points possible)

Using the table function in R, please answer the following questions
about the dataset *movies*.

*** Question a

How many movies are classified as comedies?

**** Answer

#+begin_src R :session :results output :exports all
  writeLines("\n :: Number of movies classified as comedy:")
  nrow(subset(movies, movies$Comedy == 1))
#+end_src

#+RESULTS:
:
:  :: Number of movies classified as comedy:
: [1] 502

*** Question b

How many movies are classified as westerns?

**** Answer

#+begin_src R :session :results output :exports all
    writeLines("\n :: Number of movies classified as Western:")
    nrow(subset(movies, movies$Western == 1))
#+end_src

#+RESULTS:
:
:  :: Number of movies classified as Western:
: [1] 27

*** Question c

How many movies are classified as romance AND drama?

**** Answer

#+begin_src R :session :results output :exports all
  writeLines("\n :: Number of movies classified as Romance and Drama:")
  nrow(subset(movies, movies$Romance == 1 & movies$Drama == 1))

  writeLines("\n :: Other way to answer this question:")
  nrow(movies[movies$Romance == 1 & movies$Drama == 1, ])
#+end_src

#+RESULTS:
:
:  :: Number of movies classified as Romance and Drama:
: [1] 97
:
:  :: Other way to answer this question:
: [1] 97

*Explanation*

You can answer these questions by using the following commands:

~table(movies$Comedy)~

~table(movies$Western)~

~table(movies$Romance, movies$Drama)~

** Video 7: Hierarchical Clustering in R

*Important Note*

In this video, we use the "ward" method to do hierarchical
clustering. This method was recently renamed in R to "ward.D". If you
are following along in R while watching the video, you will need to
use the following command when doing the hierarchical clustering
("ward" is replaced with "ward.D"):

~clusterMovies = hclust(distances, method = "ward.D")~

We'll use hierarchical clustering to cluster the movies in the Movie
Lens data set by genre. After we make our clusters, we'll see how they
can be used to make recommendations.

There are two steps to hierarchical clustering. First we have to
compute the distances between all data points, and then we need to
cluster the points.

To compute the distances we can use the ~dist~ function. We only want to
cluster our movies on the genre variable, not on the title variable,
so we'll cluster on columns two through 20.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Compute distances...")
  distances <- dist(movies[2:20], method = "euclidean")
#+end_src

#+RESULTS:
:
:  :: Compute distances...

Now let's cluster our movies using the ~hclust~ function for
hierarchical clustering.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Hierarchical clustering...")
  clusterMovies <- hclust(distances, method = "ward.D")
#+end_src

#+RESULTS:
:
:  :: Hierarchical clustering...

The second argument is ~method="ward"~. The ward method cares about
the distance between clusters using centroid distance, and also the
variance in each of the clusters.

Now let's plot the dendrogram of our clustering algorithm:

#+BEGIN_SRC R :var basename="MoviesDendrogram" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  # Plot the dendrogram
  plot(clusterMovies)
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Movies dendrogram of clustering method
#+NAME:     fig:MoviesDendrogram
#+ATTR_LaTeX: placement: [H]
[[../graphs/MoviesDendrogram.png]]

This dendrogram might look a little strange. We have all this black
along the bottom. Remember that the dendrogram lists all of the data
points along the bottom. But when there are over 1,000 data points
it's impossible to read.

We'll see later how to assign our clusters to groups so that we can
analyze which data points are in which cluster.

So looking at this dendrogram, *how many clusters would you pick?* It
looks like maybe three or four clusters would be a good choice
according to the dendrogram, but let's keep our application in mind,
too.

We probably want more than two, three, or even four clusters of movies
to make recommendations to users.

It looks like there's a nice spot down here where there's 10
clusters. This is probably better for our application. We could select
even more clusters if we want to have very specific genre groups.

If you want a lot of clusters it's hard to pick the right number from
the dendrogram. You need to use your understanding of the problem to
pick the number of clusters. Let's stick with 10 clusters for now,
combining what we learned from the dendrogram with our understanding
of the problem.

We can label each of the data points according to what cluster it
belongs to using the cutree function.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Assign points to clusters...")
  clusterGroups <- cutree(clusterMovies, k = 10)
#+end_src

#+RESULTS:
:
:  :: Assign points to clusters...

Now let's figure out what the clusters are like. We'll use the tapply
function to compute the percentage of movies in each genre and
cluster.

So let's type tapply, and then give as the first argument,
movies$Action-- we'll start with the action genre-- and then
clusterGroups, and then mean.

*So what does this do?*

It divides our data points into the 10 clusters and then computes the
average value of the action variable for each cluster. Remember that
the action variable is a binary variable with value 0 or 1.

So by computing the average of this variable we're computing the
percentage of movies in that cluster that belong in that genre.

#+begin_src R :session :results output :exports all
  tapply(movies$Action, clusterGroups, mean)
  tapply(movies$Romance, clusterGroups, mean)
#+end_src

#+RESULTS:
:         1         2         3         4         5         6         7         8
: 0.1784512 0.7839196 0.1238532 0.0000000 0.0000000 0.1015625 0.0000000 0.0000000
:         9        10
: 0.0000000 0.0000000
:          1          2          3          4          5          6          7
: 0.10437710 0.04522613 0.03669725 0.00000000 0.00000000 1.00000000 1.00000000
:          8          9         10
: 0.00000000 0.00000000 0.00000000

So we can see here that in cluster 2, about $78\%$ of the movies have the
action genre label, whereas in cluster 4 none of the movies are
labeled as action movies.

Let's try this again, but this time let's look at the *romance*
genre. Here we can see that all of the movies in clusters six and
seven are labeled as romance movies, whereas only $4\%$ of the movies
in cluster two are labeled as romance movies.

| Cluster 1   |              Cluster 2 |              Cluster 3 | Cluster 4 | Cluster 5 | Cluster 6 |         Cluster 7 |   Cluster 8 |         Cluster 9 | Cluster 10 |      |
|-------------+------------------------+------------------------+-----------+-----------+-----------+-------------------+-------------+-------------------+------------+------|
| Action      |                   0.18 |                   0.78 |      0.12 |      0.00 |      0.00 |              0.10 |        0.00 |              0.00 |       0.00 | 0.00 |
| Adventure   |                   0.19 |                   0.35 |      0.04 |      0.00 |      0.00 |              0.00 |        0.00 |              0.00 |       0.00 | 0.00 |
| Animation   |                   0.13 |                   0.01 |      0.00 |      0.00 |      0.00 |              0.00 |        0.00 |              0.00 |       0.00 | 0.00 |
| Childrens   |                   0.39 |                   0.01 |      0.01 |      0.00 |      0.00 |              0.00 |        0.00 |              0.00 |       0.00 | 0.00 |
| Comedy      |                   0.36 |                   0.07 |      0.06 |      0.00 |      1.00 |              0.11 |        1.00 |              0.02 |       1.00 | 0.16 |
| Crime       |                   0.03 |                   0.01 |      0.41 |      0.00 |      0.00 |              0.05 |        0.00 |              0.00 |       0.00 | 0.00 |
| Documentary |                   0.01 |                   0.00 |      0.00 |      0.00 |      0.00 |              0.00 |        0.00 |              1.00 |       0.00 | 0.00 |
| Drama       |                   0.31 |                   0.11 |      0.38 |      1.00 |      0.00 |              0.66 |        0.00 |              0.00 |       1.00 | 0.00 |
| Fantasy     |                   0.07 |                   0.00 |      0.00 |      0.00 |      0.00 |              0.00 |        0.00 |              0.00 |       0.00 | 0.00 |
| Film Noir   |                   0.00 |                   0.00 |      0.11 |      0.00 |      0.00 |              0.01 |        0.00 |              0.00 |       0.00 | 0.00 |
| Horror      |                   0.02 |                   0.08 |      0.02 |      0.00 |      0.00 |              0.02 |        0.00 |              0.00 |       0.00 | 1.00 |
| Musical     |                   0.19 |                   0.00 |      0.00 |      0.00 |      0.00 |              0.00 |        0.00 |              0.00 |       0.00 | 0.00 |
| Mystery     |                   0.00 |                   0.00 |      0.28 |      0.00 |      0.00 |              0.00 |        0.00 |              0.00 |       0.00 | 0.00 |
| Romance     |                   0.10 |                   0.05 |      0.04 |      0.00 |      0.00 |              1.00 |        1.00 |              0.00 |       0.00 | 0.00 |
| Sci-Fi      |                   0.07 |                   0.35 |      0.04 |      0.00 |      0.00 |              0.00 |        0.00 |              0.00 |       0.00 | 0.00 |
| Thriller    |                   0.04 |                   0.38 |      0.61 |      0.00 |      0.00 |              0.14 |        0.00 |              0.00 |       0.00 | 0.16 |
| War         |                   0.23 |                   0.02 |      0.00 |      0.00 |      0.00 |              0.00 |        0.00 |              0.02 |       0.00 | 0.00 |
| Western     |                   0.09 |                   0.00 |      0.00 |      0.00 |      0.00 |              0.00 |        0.00 |              0.00 |       0.00 | 0.00 |
| Misc        | Action-Adventure-SciFi | Crime-Mystery-Thriller |     Drama |    Comedy |   Romance | Romantic Comedies | Documentary | Dramatic Comedies |     Horror |      |

[[../graphs/MoviesClustering.png]]

Knowing common movie genres, these clusters seem to make a lot of
sense.

*Let's see how these clusters could be used in a recommendation system.*

Remember that Amy liked the movie Men in Black. Let's figure out what
cluster Men in Black is in.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Id of the movie:")
  subset(movies, Title == "Men in Black (1997)")

  writeLines("\n :: Find which cluster Men in Black is in:")
  clusterGroups[257]
#+end_src

#+RESULTS:
#+begin_example

 :: Id of the movie:
                  Title Unknown Action Adventure Animation Childrens Comedy
257 Men in Black (1997)       0      1         1         0         0      1
    Crime Documentary Drama Fantasy FilmNoir Horror Musical Mystery Romance
257     0           0     0       0        0      0       0       0       0
    SciFi Thriller War Western
257     1        0   0       0

 :: Find which cluster Men in Black is in:
257
  2
#+end_example

I knew that this is the title of Men in Black because I looked it up
in our data set. So it looks like Men in Black is the 257th row in our
data. So which cluster did the 257th movie go into?

It looks like Men in Black went into cluster 2. That make sense since
we just saw that *cluster 2 is the action, adventure, sci-fi cluster*.

So let's create a new data set with just the movies from cluster two.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Create a new data set with just the movies from cluster 2:")
  cluster2 <- subset(movies, clusterGroups == 2)

  writeLines("\n :: Look at the first 10 titles in this cluster:")
  cluster2$Title[1:10]
#+end_src

#+RESULTS:
#+begin_example

 :: Create a new data set with just the movies from cluster 2:

 :: Look at the first 10 titles in this cluster:
 [1] GoldenEye (1995)
 [2] Bad Boys (1995)
 [3] Apollo 13 (1995)
 [4] Net, The (1995)
 [5] Natural Born Killers (1994)
 [6] Outbreak (1995)
 [7] Stargate (1994)
 [8] Fugitive, The (1993)
 [9] Jurassic Park (1993)
[10] Robert A. Heinlein's The Puppet Masters (1994)
1664 Levels: 'Til There Was You (1997) ...
#+end_example

So it looks like good movies to recommend to Amy, according to our
clustering algorithm, would be movies like Apollo 13 and Jurassic
Park.

In this video we saw how clustering can be applied to create a movie
recommendation system.

*** An Advanced Approach to Finding Cluster Centroids

In this video, we explain how you can find the cluster centroids by
using the function *tapply* for each variable in the dataset. While
this approach works and is familiar to us, it can be a little tedious
when there are a lot of variables.

An alternative approach is to use the colMeans function. With this
approach, you only have one command for each cluster instead of one
command for each variable. If you run the following command in your R
console, you can get all of the column (variable) means for cluster 1:

~colMeans(subset(movies[2:20], clusterGroups == 1))~

You can repeat this for each cluster by changing the *clusterGroups*
number. However, if you also have a lot of clusters, this approach is
not that much more efficient than just using the tapply function.

A more advanced approach uses the ~split~ and ~lapply~ functions. The
following command will split the data into subsets based on the
clusters:

~spl = split(movies[2:20], clusterGroups)~

Then you can use spl to access the different clusters, because

~spl[ [1] ]~

is the same as

~subset(movies[2:20], clusterGroups == 1)~

so ~colMeans(spl[ [1] ])~ will output the centroid of cluster 1. But
an even easier approach uses the lapply function. The following
command will output the cluster centroids for all clusters:

~lapply(spl, colMeans)~

The lapply function runs the second argument (colMeans) on each
element of the first argument (each cluster subset in spl). So instead
of using 19 tapply commands, or 10 colMeans commands, we can output
our centroids with just two commands: one to define spl, and then the
~lapply~ command.

Note that if you have a variable called "split" in your current R
session, you will need to remove it with rm(split) so that you can use
the split function.


** Quick Question (1 point possible)

Run the ~cutree~ function again to create the cluster groups, but this
time pick ~k = 2~ clusters. It turns out that the algorithm groups all
of the movies that only belong to one specific genre in one cluster
(cluster 2), and puts all of the other movies in the other cluster
(cluster 1).

What is the genre that all of the movies in cluster 2 belong to?

#+begin_src R :session :results output :exports all
  writeLines("\n :: Assign points to clusters...")
  clusterGroups2 <- cutree(clusterMovies, k = 2)

  spl = split(movies[2:20], clusterGroups2)

  writeLines("\n :: Percentage of movies in each cluster:")
  lapply(spl, colMeans)
#+end_src

#+RESULTS:
#+begin_example

 :: Assign points to clusters...

 :: Percentage of movies in each cluster:
$`1`
    Unknown      Action   Adventure   Animation   Childrens      Comedy
0.001545595 0.192426584 0.102782071 0.032457496 0.092735703 0.387944359
      Crime Documentary       Drama     Fantasy    FilmNoir      Horror
0.082689335 0.038639876 0.267387944 0.017001546 0.018547141 0.069551777
    Musical     Mystery     Romance       SciFi    Thriller         War
0.043276662 0.046367852 0.188562597 0.077279753 0.191653787 0.054868624
    Western
0.020865533

$`2`
    Unknown      Action   Adventure   Animation   Childrens      Comedy
          0           0           0           0           0           0
      Crime Documentary       Drama     Fantasy    FilmNoir      Horror
          0           0           1           0           0           0
    Musical     Mystery     Romance       SciFi    Thriller         War
          0           0           0           0           0           0
    Western
          0
#+end_example

*** Answer

Drama

*Explanation*

You can redo the cluster grouping with just two clusters by running
the following command:

~clusterGroups = cutree(clusterMovies, k = 2)~

Then, by using the tapply function just like we did in the video, you
can see the average value in each genre and cluster. It turns out that
all of the movies in the second cluster belong to the drama genre.

Alternatively, you can use colMeans or lapply as explained below
Video 7.

** Video 8: The Analytics Edge of Recommendation Systems

Recommendation systems are used in many different areas other than
movies.

Jeff Bezos, the CEO of Amazon, said that, "If I have 3 million
customers on the web, I should have 3 million stores on the web."

The internet allows for mass personalization, and recommendation
systems are a key part of that. Recommendation systems build models
about users' preferences to personalize the user experience.

[[../graphs/RecommendationSystems-MassPersonalization.png]]

Recommendation systems are a cornerstone of these top
businesses. Social networking sites, like Facebook, music streaming
sites, like Pandora, and retail companies, like Amazon, all provide
recommendation systems for their users.

[[../graphs/RS-Cornerstone.png]]

Both collaborative filtering and content filtering are used in
practice. Collaborative filtering is used by companies like Amazon,
Facebook, and Google News. Content filtering is used by companies like
Pandora, Rotten Tomatoes, and See This Next. And Netflix uses both
collaborative filtering and content filtering.

[[../graphs/RS-MethodUsed.png]]

So now let's go back to the Netflix prize.

To really test the algorithms, Netflix tested them on a private test
set that the teams had never seen before. This is the true test of
predictive ability.

[[../graphs/NF-NetflixPrize.png]]

On September 18, 2009, Netflix announced that the winning team was
Bellkor's Pragmatic Chaos. They won the competition and the $1 million
grand prize.

[[../graphs/NF-Winners.png]]

Recommendation systems provide a significant edge to many
companies. In today's digital age, businesses often have hundreds of
thousands of items to offer their customers, whether they're movies,
songs , or people they might know on Facebook.

[[../graphs/TEA-RecommendationSystems.png]]

Excellent recommendation systems can make or break these
businesses. Clustering algorithms, which are tailored to find similar
customers or similar items, form the backbone of many of these
recommendation systems.

Clustering also has many other interesting applications. In the next
lecture, we'll see how clustering can be used to improve the
predictive ability of classification methods.

* Predictive Diagnosis: Discovering Patterns for Disease Detection

** Video 1: Heart Attacks

We discuss the idea of predictive analytics in medicine. Specifically,
we introduce the idea of using clustering methods for better
predicting heart attacks.

Heart attacks are a common complication of coronary heart disease,
resulting from the interruption of blood supply to part of the
heart. Heart attack is the number one cause of death for both men and
women in the United States. About one in every four deaths is due to
heart attack.

[[../graphs/PredictiveDiagnosis.png]]

A 2012 report from the American Heart Association estimates about
715,000 Americans have a heart attack every year. To put this number
into perspective, this means that every 20 seconds, a person has a
heart attack in the United States.

It is also equivalent of September the 11th repeating itself every 24
hours, 365 days a year.

Nearly half of these attacks occur without prior warning signs. In
fact, 250,000 Americans die of sudden cardiac death yearly, which
means 680 people every day die of sudden cardiac death.

[[../graphs/PD-HeartAttacks.png]]

A heart attack has well-known symptoms: chest pain, shortness of
breath, upper body pain, nausea.  The nature of heart attacks,
however, makes it hard to predict, prevent, and even diagnose. Here
are some statistics.

[[../graphs/PD-HeartAttacks02.png]]

*How can analytics help?* The key to helping patients is to understand
the clinical characteristics of patients in whom heart attacks was
missed.

We need to better understand the patterns in a patient's diagnostic
history that link to heart attack and to predicting whether a patient
is at risk for a heart attack.

We'll see, how analytics helps to understand patterns of heart attacks
and to provide good predictions that in turn lead to improved
monitoring and taking action early and effectively.

[[../graphs/PD-AnalyticsHelpsMonitoring.png]]

** Quick Question (1/1 point)

In this class, we've learned many different methods for predicting
outcomes. Which of the following methods is designed to be used to
predict an outcome like whether or not someone will experience a heart
attack? Select all that apply.

*** Answer [3/4]

- [ ] Linear Regression
- [X] Logistic Regression
- [X] CART
- [X] Random Forest

*Explanation*

*Logistic Regression*, *CART*, and *Random Forest* are all designed to
be used to predict whether or not someone has a heart attack, since
this is a classification problem. Linear regression would be
appropriate for a problem with a continuous outcome, such as the
amount of time until someone has a heart attack. In this lecture,
we'll use random forest, but the other methods could be used too.

** Video 2: The Data

Claims data offers an expansive view of the patient's health
history. Specifically, claims data include information on
demographics, medical history, and medications. They offer insights
regarding a patient's risk. And as I will demonstrate, may reveal
indicative signals and patterns.

[[../graphs/PD-ClaimsData.png]]

We concentrated on members with the following attributes. These
selections yield patients with a high risk of heart attack, and a
reasonably rich medical history with continuous coverage.

[[../graphs/PD-ClaimsData02.png]]

Let us discuss how we aggregated this data. The resulting data sets
includes about 20 million health insurance entries, including
individual, medical, and pharmaceutical records.

[[../graphs/PD-DataAggregation.png]]

Let us discuss how we view the data over time. It is important in this
study to view the medical records chronologically, and to represent a
patient's diagnosis profile over time.

So we record the cost and number of medical claims and hospital visits
by a diagnosis. All the observations we have span over five years of
data. They were split into 21 periods, each 90 days in length.

[[../graphs/PD-DiagnosticHistory.png]]

What was the target variable we were trying to predict? The target
prediction variable is the occurrence of a heart attack. We define
this from a combination of several claims. Namely, diagnosis of a
heart attack, alongside a trip to the emergency room, followed by
subsequent hospitalization.

We define this from a combination of several claims. Namely, diagnosis
of a heart attack, alongside a trip to the emergency room, followed by
subsequent hospitalization. Only considering heart attack diagnosis
that are associated with a visit to the emergency room, and following
hospitalization helps ensure that the target outcome is in fact a
heart attack event.

The target variable is binary. It is denoted by plus 1 or minus 1 for
the occurrence or non-occurrence of a heart attack in the targeted
period of 90 days.

[[../graphs/PD-TargetVariable.png]]

*How is the data organized?* There were 147 variables.

[[../graphs/PD-DatasetCompilation.png]]

*Cost of medical care* is a good summary of a person's health. In our
database, the total cost of medical care in the three 90 day periods
preceding the heart attack target event ranged from $0 to $636,000 and
approximately 70% of the overall cost was generated by only 11% of the
population.

This means that the highest patients with high medical expenses are a
very small proportion of the data, and could skew our final results.

According to the American Medical Association, only 10% of individuals
have projected medical expenses of approximately $10,000 or greater
per year, which is more than four times greater than the average
projected medical expenses of $2,400 per year.

To lessen the effects of these high-cost outliers, we divided the data
into different cost buckets, based on the findings of the American
Medical Association.

[[../graphs/PD-CostBucketPartitioning.png]]

We did not want to have too many cost bins because the size of the
data set. The table in the slide gives a summary of the cost bucket
partitions. Patients with expenses over $10,000 in the nine month
period were allocated to cost bucket 3.

Patients with less than $2,000 in expenses were allocated to cost
bucket 1. And the remaining patients with costs between $2,000 and
$10,000 to cost bucket 2.

Please note that the majority of patients, 4,400 out of 6,500, or
67.5% of all patients fell into the first bucket of low expenses.

** Quick Question (2 points possible)

In the previous video, we discussed how we split the data into three
groups, or buckets, according to cost.

*** Question a

Which bucket has the most data, in terms of number of patients?

**** Answer [1/3]

- [X] Cost Bucket 1
- [ ] Cost Bucket 2
- [ ] Cost Bucket 3

*** Question b

Which bucket probably has the densest data, in terms of number of
claims per person?

**** Answer [1/3]

- [ ] Cost Bucket 1
- [ ] Cost Bucket 2
- [X] Cost Bucket 3

*Explanation*

Cost Bucket 1 contains the most patients (see slide 7 of the previous
video), and Cost Bucket 3 probably has the densest data, since these
are the patients with the highest cost in terms of claims.

** Video 3: Predicting Heart Attacks using Clustering

Let us discuss the performance of a benchmark algorithm. The Random
Forest algorithm is known for its attractive property of detecting
variable interactions and excellent performance as a learning
algorithm. For this reason, we're selecting the Random Forest
algorithm as a benchmark-- initially, we randomly partitioned the full
data set into two separate parts, where the split was 50-50, and the
partitioning was done evenly within each cost bin.

[[../graphs/PD-PredictingHeartAttacks.png]]

The first part, the training set, was used to develop the method. The
second part, the test set, was used to evaluate the model's
performance. The table in this slide reports the accuracy of the
Random Forest algorithm on each of the three buckets.

Let us now introduce the idea of clustering. Patients in each bucket
may have different characteristics. For this reason, we create
clusters for each cost bucket and make predictions for each cluster
using the Random Forest algorithm.

[[../graphs/PD-IncorporatingClustering.png]]

For this reason, we create clusters for each cost bucket and make
predictions for each cluster using the Random Forest algorithm.

[[../graphs/PD-IncorporatingClustering02.png]]

*Clustering* is mostly used in the absence of a target variable to
search for relationships among input variables or to organize data
into meaningful groups.

In this study, although the target variable is well-defined as a heart
attack or not a heart attack, there are many different trajectories
that are associated with the target.

There's not one set pattern of health or diagnostic combination that
leads a person to heart attack. Instead, we'll show that there are
many different dynamic health patterns and time series diagnostic
relations preceding a heart attack.

[[../graphs/PD-ClusteringCostBuckets.png]]

The clustering methods we used were spectral clustering and k-means
clustering. We focus, in the lecture, on the k-means clustering. The
broad description of the algorithm is as follows.

[[../graphs/PD-ClusteringCostBuckets02.png]]

Let us illustrate the k-means algorithm in action. We specify the
desired number of clusters k. In this case, we use ~k = 2~.

We then randomly assign each data point to a cluster.

[[../graphs/PD-k-MeansClustering.png]]

Randomly assign each data point to a cluster:

[[../graphs/PD-k-MeansClustering02.png]]

In this case, we have the three points in red, and the two points in
black.

[[../graphs/PD-k-MeansClustering03.png]]

We then compute the cluster centroids, of the points showed.

[[../graphs/PD-k-MeansClustering04.png]]

When we compute the cluster centroids, indicated by the red x and the
grey x. We re-assign each point to the closest cluster centroid.

[[../graphs/PD-k-MeansClustering05.png]]

and now you observe that this point changes from a red to a gray.

[[../graphs/PD-k-MeansClustering06.png]]

We re-compute the cluster centroids, and we repeat the previous steps,
4 and 5 until no improvement is made. You can see the initial
centroids:

[[../graphs/PD-k-MeansClustering07.png]]

And now the new calculated centroids. We observe that, in this case,
the k-means clustering is done, and this is our final clustering.

[[../graphs/PD-k-MeansClustering08.png]]

Let us discuss some practical considerations.

[[../graphs/PD-PracticalConsiderations.png]]

*So how do we measure performance?* After we construct the clusters in
the training set, we assign new observations to clusters by proximity
to the centroid of each cluster.

[[../graphs/PD-RandomForestWithClustering.png]]

We measure performance by recording the average performance rate in
each cluster.

Let us now discuss the performance of the clustering methods. We
perform clustering on each bucket using k=10 clusters.

In the table we record the average prediction rate of each cost
bucket. We observe a very visible improvement when we use clustering--
from 49% to 64%, from 56% to 73%, from 58% to 78%.

[[../graphs/PD-PredictingHeartAttacks02.png]]

** Quick Question (1 point possible)

K-means clustering differs from Hierarchical clustering in a couple
important ways. Which of the following statements is true?

*** Answer [1/2]

- [X] In k-means clustering, you have to pick the number of clusters
  you want before you run the algorithm.

- [ ] In k-means clustering, you can pick the number of clusters you
  want after the algorithm is done, just like in Hierarchical
  clustering.

*Explanation*

In k-means clustering, you have to pick the number of clusters before
you run the algorithm, but the computational effort needed is much
less than that for hierarchical clustering (we'll see this in more
detail during the recitation).

** Video 4: Understanding Cluster Patterns

Let us see what we learned about the patterns that emerge. We will
show that the clusters are interpretable and reveal unique patterns of
diagnostic history among the population.

We selected six patterns to present in this lecture-- Cluster 1, 6,
and 7, in Cost Bucket 2, and Clusters 4, 5, and 10, in Cost Bucket 3.

[[../graphs/PD-UnderstandingClusterPatterns.png]]

The first pattern shows the occurrence of chest pain three months
before the heart attack. Note that the red dots depict the visits per
diagnosis for patients in Cluster 1-- this is, we think, Bucket 2--
and the blue dots depict the visits per diagnosis for patients in
Bucket 2 throughout.

[[../graphs/PD-OccurrenceOfChestPain.png]]

Note the very significant increase for visits related to chest pains
three months before the event. About 17, three months before for the
red patients, and about 1 and 1/2 visits for the blue patients.

The next pattern reveals an increasing occurrence of chronic
obstructive pulmonary disease, COPD, for short. Patients from Cluster
7 in Bucket 2 have regular doctor visits for COPD.

Note that nine months before, we have 4 and 1/2 visits (red) versus
0.5 (blue) visits. Six months before, we have almost 7 visits versus
1/2 a visit, and three months before, we have 9 visits versus 1/2 a
visit for COPD, so *a clear increasing pattern*.

[[../graphs/PD-COPD.png]]

The next pattern shows gradually increasing occurrence of anemia. The
red line shows the patients in Cluster 4 increasingly visit the doctor
for anemia from nine months on before the event.

Nine months before, members have an average of 9 visits to the doctor
for anemia. This increases to an average of 11 visits six months
before the event, and then an average of 15 visits three months before
the event, a *clear increasing pattern*.

[[../graphs/PD-Anemia.png]]

The final pattern shows the occurrence of diabetes as a pattern for
heart attacks. It is well known that both types 1 and 2 diabetes are
associated with accelerated atherosclerosis, one of the main causes of
myocardial infarction-- heart attacks, that is.

Well known diagnoses associated with heart attacks, such as diabetes,
hypertension, and hyperlipidemia, characterize many of the patterns of
the consistency of care throughout all of the cost buckets and
clustering models.

[[../graphs/PD-Diabetes.png]]

You observe a difference, here, of the number of visits for diabetes
for the population that had the event versus the average population.

** Quick Question (1 point possible)

As we saw in the previous video, the clusters can be used to find
interesting patterns of health in addition to being used to improve
predictive models. By changing the number of clusters, you can find
more general or more specific patterns.

If you wanted to find more unusual patterns shared by a small number
of people, would you increase or decrease the number of clusters?

*** Answer [1/2]

- [X] Increase
- [ ] Decrease

*Explanation*

If you wanted to find more unusual patterns, you would increase the
number of clusters since the clusters would become smaller and more
patterns would probably emerge.

** Video 5: The Analytics Edge

What is the impact of clustering?

[[../graphs/PD-ImpactOfClustering.png]]

The approach shows that using analytics for early heart failure
detection through pattern recognition can lead to interesting new
insights.

The findings here are reinforced by results from our research. IBM,
Sutter Health, and Geisinger Health Systems partnered in 2009 to
research analytics tools in view of early detection.

*Important insights*

[[../graphs/PD-AnalyticsForEarlyDetection.png]]

Steve Steinhubl, a cardiologist from Geisinger, wrote, "our early
research showed the signs and symptoms of heart failure in patients
are often documented years before diagnosis.

The pattern of documentation can offer clinically useful signals for
early detection of this deadly disease."

* Seeing the Big Picture: Segmenting Images to Create Data (Recitation)

** Welcome to Recitation 6

We will review hierarchical and k-means clustering techniques and
apply them to segment gray scale images-- in particular, MRI brain
images.

Then we will conclude by overviewing all the analytics tools we have
seen so far in this class to prepare for the class competition.

[[../graphs/BP-SeeingBigPicture.png]]

** Video 1: Image Segmentation

We will see how to apply clustering techniques to segment images, with
the main application being geared towards medical image segmentation.

At the end of this recitation, you will get a head start on how to
cluster an MRI brain image by tissue substances and locate
pathological anatomies.

Image segmentation is the process of partitioning digital images into
regions, or segments, that share the same visual characteristics, such
as color, intensity, or texture.

The segments should also be meaningful, as in they should correspond
to particular surfaces, objects, or even parts of an object. Think of
having an image of a water pond, a mountain chain in the backdrop, and
the sky.

[[../graphs/BP-ImageSegmentation.png]]

Segmenting this image should ideally detect the three different
objects and assign their corresponding pixels to three different
regions. In few words, the goal of image segmentation is to modify the
representation of an image from pixel data into something meaningful
to us and easier to analyze.

Image segmentation has a wide applicability. A major practical
application is in the field of medical imaging, where image segments
often correspond to different tissues, organs, pathologies, or tumors.

Image segmentation helps locate these geometrically complex objects
and measure their volume.

Another application is detecting instances of semantic objects such as
humans, buildings, and others. The two major domains that have seen
much attention recently include face and pedestrian detection.

The main uses of facial detection, for instance, include the
development of the auto-focus in digital cameras and face recognition
commonly used in video surveillance.

[[../graphs/BP-Applications.png]]

Other important applications are fingerprint and iris recognition. For
instance, fingerprint recognition tries to identify print patterns,
including aggregate characteristics of ridges and minutiae points.

Various methods have been proposed to segment images. Clustering
methods are used to group the points into clusters according to their
characteristic features, for instance, intensity values.

These clusters are then mapped back to the original spatial domain to
produce a segmentation of the image.

Another technique is edge detection, which is based on detecting
discontinuities or boundaries. For instance, in a gray-scale image, a
boundary would correspond to an abrupt change in the gray level.

[[../graphs/BP-VariousMethods.png]]

Instead of finding boundaries of regions in the image, there are other
techniques called region growing methods, which start dividing the
image into small regions. Then, they sequentially merge these regions
together if they are sufficiently similar.

In particular, we will review hierarchical and k-means clustering
techniques and how to use them in R. We will restrict ourselves to
gray-scale images. Our first example is a low-resolution flower image
whose pixel intensity information is given the data set ~flower.csv~.

[[../graphs/BP-ClusteringReview.png]]

Our second and major example involves two *weighted MRI images of the
brain*. One image corresponds to a healthy patient, and the other one
corresponds to a patient with a tumor called *oligodendroglioma*. The
pixel intensity information of these two images are given in the data
sets healthy and ~tumor.csv~.

The last video will compare the use, pros, and cons of all the
analytics tools that we have seen so far. I hope that this will help
you synthesize all that you've learned to give you an edge in the
class competition.

** Video 2: Clustering Pixels

In this Recitation, we'll be using the following data files:
[[https://courses.edx.org/asset-v1:MITx%2B15.071x_2a%2B2T2015%2Btype@asset%2Bblock/flower.csv][flower.csv]], [[https://courses.edx.org/asset-v1:MITx%2B15.071x_2a%2B2T2015%2Btype@asset%2Bblock/healthy.csv][healthy.csv]], and [[https://courses.edx.org/asset-v1:MITx%2B15.071x_2a%2B2T2015%2Btype@asset%2Bblock/tumor.csv][tumor.csv]]. Please download these files to
your computer to following along.

An R script file with all of the commands used in this Recitation can
be downloaded [[https://courses.edx.org/asset-v1:MITx%2B15.071x_2a%2B2T2015%2Btype@asset%2Bblock/Unit6_Recitation.R][here]].

*** Download the data sets

In this part we can download the data

#+BEGIN_SRC R :session :results output :exports all
  library(parallel)

  if(!file.exists("../data")) {
          dir.create("../data")
  }

  fileUrl <-
          c("https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/flower.csv",
"https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/healthy.csv",
"https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/tumor.csv")

  fileName <- c("flower.csv", "healthy.csv", "tumor.csv")

  dataPath <- "../data"

  for(i in 1:3) {
          filePath <- paste(dataPath, fileName[i], sep = "/")

          if(!file.exists(filePath)) {
                  download.file(fileUrl[i], destfile = filePath, method = "curl")
          }
  }
  list.files("../data")
#+END_SRC

#+RESULTS:
#+begin_example
 [1] "AnonymityPoll.csv"       "baseball.csv"
 [3] "BoeingStock.csv"         "boston.csv"
 [5] "ClaimsData.csv"          "ClaimsData.csv.zip"
 [7] "climate_change.csv"      "clinical_trial.csv"
 [9] "ClusterMeans.ods"        "CocaColaStock.csv"
[11] "CountryCodes.csv"        "CPSData.csv"
[13] "emails.csv"              "energy_bids.csv"
[15] "flower.csv"              "FluTest.csv"
[17] "FluTrain.csv"            "framingham.csv"
[19] "gerber.csv"              "GEStock.csv"
[21] "healthy.csv"             "IBMStock.csv"
[23] "loans_imputed.csv"       "loans.csv"
[25] "MetroAreaCodes.csv"      "movieLens.txt"
[27] "mvtWeek1.csv"            "NBA_test.csv"
[29] "NBA_train.csv"           "parole.csv"
[31] "pisa2009test.csv"        "pisa2009train.csv"
[33] "PollingData_Imputed.csv" "PollingData.csv"
[35] "ProcterGambleStock.csv"  "quality.csv"
[37] "README.md"               "songs.csv"
[39] "stevens.csv"             "stopwords.txt"
[41] "tumor.csv"               "tweets.csv"
[43] "USDA.csv"                "WHO_Europe.csv"
[45] "WHO.csv"                 "wiki.csv"
[47] "wine_test.csv"           "wine.csv"
#+end_example

** Video 2: Clustering Pixels

Let us try to understand the format of the data handed to us in the
CSV files. *Grayscale* images are represented as a matrix of pixel
intensity values that range from zero to one. The intensity value zero
corresponds to the absence of color, or black, and the value one
corresponds to white. For 8 bits per pixel images, we have 256 color
levels ranging from zero to one.

For instance, if we have the following grayscale image, the pixel
information can be translated to a matrix of values between zero and
one. It is exactly this matrix that we are given in our datasets.

In other words, the datasets contain a table of values between zero
and one. And the number of columns corresponds to the width of the
image, whereas the number of rows corresponds to the height of the
image.

In this example, the resolution is 7 by 7 pixels. We have to be
careful when reading the dataset in R. We need to make sure that R
reads in the matrix appropriately.

[[../graphs/BP-GrayscaleImages.png]]

Until now in this class, our datasets were structured in a way where
the rows refer to observations and the columns refer to variables. But
this is not the case for the intensity matrix. So keep in mind that we
need to do some maneuvering to make sure that R recognizes the data as
a matrix.

*Grayscale* image segmentation can be done by clustering pixels
according to their intensity values. So we can think of our clustering
algorithm as trying to divide the spectrum of intensity values from
zero to one into intervals, or clusters.

For instance, the red cluster corresponds to the darkest shades, and
the green cluster to the lightest. Now, what should the input be to
the clustering algorithm?

[[../graphs/BP-GrayscaleImages02.png]]

Well, our observations should be all of the 7 by 7 intensity
values. Hence, we should have 49 observations. And we only have one
variable, which is the pixel intensity value.

So in other words, the input to the clustering algorithm should be a
vector containing 49 elements, or intensity values. But what we have
is a 7 by 7 matrix. A crucial step before feeding the intensity values
to the clustering algorithm is *morphing our data*.

We should modify the matrix structure and lump all the intensity
values into a single vector. We will see that we can do this in R
using the as.vector function. Now, once we have the vector, we can
simply feed it into the clustering algorithm and assign each element
in the vector to a cluster.

Let us first use hierarchical clustering since we are familiar with
it. The first step is to calculate the distance matrix, which computes
the pairwise distances among the elements of the intensity vector.

*How many such distances do we need to calculate?* Well, for each
element in the intensity vector, we need to calculate its distance
from the other 48 elements.

So this makes 48 calculations per element. And we have 49 such
elements in the intensity vector. In total, we should compute 49 times
48 pairwise distances. But due to symmetry, we really need to
calculate half of them. So the number of pairwise distance
calculations is actually $\frac{(49*48)}{2}$.

In general, if we call the size of the intensity vector $n$, then we
need to compute $\frac{n*(n-1)}{2}$ pairwise distances and store them
in the distance matrix.

Now we should be ready to go to R.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Reading the flower dataset:")
  flower <- read.csv("../data/flower.csv", header = FALSE)
  str(flower)
#+end_src

#+RESULTS:
#+begin_example

 :: Reading the flower dataset:
'data.frame':	50 obs. of  50 variables:
 $ V1 : num  0.0991 0.0991 0.1034 0.1034 0.1034 ...
 $ V2 : num  0.112 0.108 0.112 0.116 0.108 ...
 $ V3 : num  0.134 0.116 0.121 0.116 0.112 ...
 $ V4 : num  0.138 0.138 0.121 0.121 0.112 ...
 $ V5 : num  0.138 0.134 0.125 0.116 0.112 ...
 $ V6 : num  0.138 0.129 0.121 0.108 0.112 ...
 $ V7 : num  0.129 0.116 0.103 0.108 0.112 ...
 $ V8 : num  0.116 0.103 0.103 0.103 0.116 ...
 $ V9 : num  0.1121 0.0991 0.1078 0.1121 0.1164 ...
 $ V10: num  0.121 0.108 0.112 0.116 0.125 ...
 $ V11: num  0.134 0.125 0.129 0.134 0.129 ...
 $ V12: num  0.147 0.134 0.138 0.129 0.138 ...
 $ V13: num  0.000862 0.146552 0.142241 0.142241 0.133621 ...
 $ V14: num  0.000862 0.000862 0.142241 0.133621 0.12931 ...
 $ V15: num  0.142 0.142 0.134 0.121 0.116 ...
 $ V16: num  0.125 0.125 0.116 0.108 0.108 ...
 $ V17: num  0.1121 0.1164 0.1078 0.0991 0.0991 ...
 $ V18: num  0.108 0.112 0.108 0.108 0.108 ...
 $ V19: num  0.121 0.129 0.125 0.116 0.116 ...
 $ V20: num  0.138 0.129 0.125 0.116 0.116 ...
 $ V21: num  0.138 0.134 0.121 0.125 0.125 ...
 $ V22: num  0.134 0.129 0.125 0.121 0.103 ...
 $ V23: num  0.125 0.1207 0.1164 0.1164 0.0819 ...
 $ V24: num  0.1034 0.1034 0.0991 0.0991 0.1034 ...
 $ V25: num  0.0948 0.0905 0.0905 0.1034 0.125 ...
 $ V26: num  0.0862 0.0862 0.0991 0.125 0.1422 ...
 $ V27: num  0.086207 0.086207 0.103448 0.12931 0.000862 ...
 $ V28: num  0.0991 0.1078 0.1164 0.1293 0.1466 ...
 $ V29: num  0.116 0.134 0.134 0.121 0.142 ...
 $ V30: num  0.121 0.138 0.142 0.129 0.138 ...
 $ V31: num  0.121 0.134 0.142 0.134 0.129 ...
 $ V32: num  0.116 0.134 0.129 0.116 0.112 ...
 $ V33: num  0.108 0.112 0.116 0.108 0.108 ...
 $ V34: num  0.1078 0.1078 0.1034 0.0991 0.1034 ...
 $ V35: num  0.1078 0.1034 0.0991 0.0991 0.0991 ...
 $ V36: num  0.1078 0.1034 0.1034 0.0905 0.0862 ...
 $ V37: num  0.1078 0.1078 0.1034 0.0819 0.0733 ...
 $ V38: num  0.0948 0.0991 0.0776 0.069 0.0733 ...
 $ V39: num  0.0733 0.056 0.0474 0.0474 0.056 ...
 $ V40: num  0.0474 0.0388 0.0431 0.0474 0.0603 ...
 $ V41: num  0.0345 0.0345 0.0388 0.0474 0.0647 ...
 $ V42: num  0.0259 0.0259 0.0345 0.0431 0.056 ...
 $ V43: num  0.0259 0.0259 0.0388 0.0517 0.0603 ...
 $ V44: num  0.0302 0.0302 0.0345 0.0517 0.0603 ...
 $ V45: num  0.0259 0.0259 0.0259 0.0388 0.0474 ...
 $ V46: num  0.0259 0.0172 0.0172 0.0259 0.0345 ...
 $ V47: num  0.01724 0.01724 0.00862 0.02155 0.02586 ...
 $ V48: num  0.0216 0.0129 0.0129 0.0172 0.0302 ...
 $ V49: num  0.0216 0.0216 0.0216 0.0345 0.0603 ...
 $ V50: num  0.0302 0.0345 0.0388 0.0603 0.0776 ...
#+end_example

Note that the default in R assumes that the first row in the dataset
is the header. So if we didn't specify that we have no headers in this
case, we would have lost the information from the first row of the
pixel intensity matrix.

We realize that the way the data is stored does not reflect that this
is a matrix of intensity values. Actually, R treats the rows as
observations and the columns as variables.

Let's try to change the data type to a matrix by using the as.matrix
function.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Change the data type to matrix:")
  flowerMatrix <- as.matrix(flower)
  str(flowerMatrix)
#+end_src

#+RESULTS:
:
:  :: Change the data type to matrix:
:  num [1:50, 1:50] 0.0991 0.0991 0.1034 0.1034 0.1034 ...
:  - attr(*, "dimnames")=List of 2
:   ..$ : NULL
:   ..$ : chr [1:50] "V1" "V2" "V3" "V4" ...

And now if we look at the structure of the flower matrix, we realize
that we have 50 rows and 50 columns. What this suggests is that the
resolution of the image is 50 pixels in width and 50 pixels in
height. This is actually a very, very small picture.

I am very curious to see how this image looks like, but lets hold off
now and do our clustering first. We do not want to be influenced by
how the image looks like in our decision of the numbers of clusters we
want to pick.

To perform any type of clustering, we saw earlier that we would need
to convert the matrix of pixel intensities to a vector that contains
all the intensity values ranging from zero to one.

And the clustering algorithm divides the intensity spectrum, the
interval zero to one, into these joint clusters or intervals.

So let us define the vector ~flowerVector~, and then now we're going to
use the function as.vector, which takes as an input the
~flowerMatrix~.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Turn matrix into a vector:")
  flowerVector <- as.vector(flowerMatrix)
  head(str(flowerVector))
#+end_src

#+RESULTS:
:
:  :: Turn matrix into a vector:
:  num [1:2500] 0.0991 0.0991 0.1034 0.1034 0.1034 ...
: NULL

And now if we look at the structure of the ~flowerVector~, we realize
that we have 2,500 numerical values, which range between zero and
one. And this totally makes sense because this reflects the 50 times
50 intensity values that we had in our matrix.

Now you might be wondering why we can't immediately convert the data
frame flower to a vector. Let's try to do this.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Turn matrix into a vector:")
  flowerVector2 <- as.vector(flower)
  head(str(flowerVector2))
#+end_src

#+RESULTS:
#+begin_example

 :: Turn matrix into a vector:
'data.frame':	50 obs. of  50 variables:
 $ V1 : num  0.0991 0.0991 0.1034 0.1034 0.1034 ...
 $ V2 : num  0.112 0.108 0.112 0.116 0.108 ...
 $ V3 : num  0.134 0.116 0.121 0.116 0.112 ...
 $ V4 : num  0.138 0.138 0.121 0.121 0.112 ...
 $ V5 : num  0.138 0.134 0.125 0.116 0.112 ...
 $ V6 : num  0.138 0.129 0.121 0.108 0.112 ...
 $ V7 : num  0.129 0.116 0.103 0.108 0.112 ...
 $ V8 : num  0.116 0.103 0.103 0.103 0.116 ...
 $ V9 : num  0.1121 0.0991 0.1078 0.1121 0.1164 ...
 $ V10: num  0.121 0.108 0.112 0.116 0.125 ...
 $ V11: num  0.134 0.125 0.129 0.134 0.129 ...
 $ V12: num  0.147 0.134 0.138 0.129 0.138 ...
 $ V13: num  0.000862 0.146552 0.142241 0.142241 0.133621 ...
 $ V14: num  0.000862 0.000862 0.142241 0.133621 0.12931 ...
 $ V15: num  0.142 0.142 0.134 0.121 0.116 ...
 $ V16: num  0.125 0.125 0.116 0.108 0.108 ...
 $ V17: num  0.1121 0.1164 0.1078 0.0991 0.0991 ...
 $ V18: num  0.108 0.112 0.108 0.108 0.108 ...
 $ V19: num  0.121 0.129 0.125 0.116 0.116 ...
 $ V20: num  0.138 0.129 0.125 0.116 0.116 ...
 $ V21: num  0.138 0.134 0.121 0.125 0.125 ...
 $ V22: num  0.134 0.129 0.125 0.121 0.103 ...
 $ V23: num  0.125 0.1207 0.1164 0.1164 0.0819 ...
 $ V24: num  0.1034 0.1034 0.0991 0.0991 0.1034 ...
 $ V25: num  0.0948 0.0905 0.0905 0.1034 0.125 ...
 $ V26: num  0.0862 0.0862 0.0991 0.125 0.1422 ...
 $ V27: num  0.086207 0.086207 0.103448 0.12931 0.000862 ...
 $ V28: num  0.0991 0.1078 0.1164 0.1293 0.1466 ...
 $ V29: num  0.116 0.134 0.134 0.121 0.142 ...
 $ V30: num  0.121 0.138 0.142 0.129 0.138 ...
 $ V31: num  0.121 0.134 0.142 0.134 0.129 ...
 $ V32: num  0.116 0.134 0.129 0.116 0.112 ...
 $ V33: num  0.108 0.112 0.116 0.108 0.108 ...
 $ V34: num  0.1078 0.1078 0.1034 0.0991 0.1034 ...
 $ V35: num  0.1078 0.1034 0.0991 0.0991 0.0991 ...
 $ V36: num  0.1078 0.1034 0.1034 0.0905 0.0862 ...
 $ V37: num  0.1078 0.1078 0.1034 0.0819 0.0733 ...
 $ V38: num  0.0948 0.0991 0.0776 0.069 0.0733 ...
 $ V39: num  0.0733 0.056 0.0474 0.0474 0.056 ...
 $ V40: num  0.0474 0.0388 0.0431 0.0474 0.0603 ...
 $ V41: num  0.0345 0.0345 0.0388 0.0474 0.0647 ...
 $ V42: num  0.0259 0.0259 0.0345 0.0431 0.056 ...
 $ V43: num  0.0259 0.0259 0.0388 0.0517 0.0603 ...
 $ V44: num  0.0302 0.0302 0.0345 0.0517 0.0603 ...
 $ V45: num  0.0259 0.0259 0.0259 0.0388 0.0474 ...
 $ V46: num  0.0259 0.0172 0.0172 0.0259 0.0345 ...
 $ V47: num  0.01724 0.01724 0.00862 0.02155 0.02586 ...
 $ V48: num  0.0216 0.0129 0.0129 0.0172 0.0302 ...
 $ V49: num  0.0216 0.0216 0.0216 0.0345 0.0603 ...
 $ V50: num  0.0302 0.0345 0.0388 0.0603 0.0776 ...
NULL
#+end_example

And now let's look at its structure. It seems that R reads it exactly
like the flower data frame and sees it as 50 observations and 50
variables.

So converting the data to a matrix and then to the vector is a crucial
step. Now we should be ready to start our hierarchical clustering. The
first step is to create the distance matrix, as you already know,
which in this case computes the difference between every two intensity
values in our flower vector.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Compute distances...")
  distance <- dist(flowerVector, method = "euclidean")
#+end_src

#+RESULTS:
:
:  :: Compute distances...

Now that we have the distance, next we will be computing the
hierarchical clusters.

** Video 3: Hierarchical Clustering

We found the distance matrix, which computes the pairwise distances
between all the intensity values in the flower vector. Now we can
cluster the intensity values using hierarchical clustering.

As a reminder, the Wards method is a minimum variance method, which
tries to find compact and spherical clusters. We can think about it as
trying to minimize the variance within each cluster and the distance
among clusters.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Hierarchical clustering...")
  clusterIntensity <- hclust(distance, method = "ward.D")
#+end_src

#+RESULTS:
:
:  :: Hierarchical clustering...

Now we can plot the cluster dendrogram.

#+BEGIN_SRC R :var basename="FlowerDendrogram" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  # Plot the dendrogram
  plot(clusterIntensity)
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Flower dendrogram
#+NAME:     fig:FlowerDendrogram
#+ATTR_LaTeX: placement: [H]
[[../graphs/FlowerDendrogram.png]]

Let's have here a little aside or a quick reminder about how to read a
dendrogram and make sense of it. Let us first consider this toy
dendrogram example. The lowest row of nodes represent the data or the
individual observations, and the remaining nodes represent the
clusters. The vertical lines depict the distance between two nodes or
clusters. The taller the line, the more dissimilar the clusters are.

For instance, cluster D-E-F is closer to cluster B-C-D-E-F than
cluster B-C is. And this is well depicted by the height of the lines
connecting each of clusters B-C and D-E-F to their parent node.

[[../graphs/BP-DendrogramExample.png]]

Now cutting the dendrogram at a given level yields a certain
partitioning of the data. For instance, if we cut the tree between
levels two and three, we obtain four clusters, A, B-C, D-E, and F.

[[../graphs/BP-DendrogramExample02.png]]

If we cut the dendrogram between levels three and four, then we obtain
three clusters, A, B-C, and D-E-F. And if we were to cut the
dendrogram between levels four and five, then we obtain two clusters,
A and B-C-D-E-F.

*What to choose, two, three, or four clusters?* Well, the smaller the
number of clusters, the coarser the clustering is. But at the same
time, having many clusters may be too much of a stretch.

*We should always have this trade-off in mind.* Now the distance
information between clusters can guide our choice of the number of
clusters. A good partition belongs to a cut that has a good enough
room to move up and down.

For instance, the cut between levels two and three can go up until it
reaches cluster D-E-F. The cut between levels three and four has more
room to move until it reaches the cluster B-C-D-E-F. And the cut
between levels four and five has the least room.

[[../graphs/BP-DendrogramExample03.png]]

So it seems like choosing three clusters is reasonable in this
case. Going back to our dendrogram, it seems that having two clusters
or three clusters is reasonable in our case. We can actually visualize
the cuts by plotting rectangles around the clusters on this tree.

To do so, we can use the ~rect.hclust~ function, which takes as an input
clusterIntensity, which is our tree. And then we can specify the
number of clusters that we want. So let's set ~k = 3~.

#+BEGIN_SRC R :var basename="FlowerDendrogram02" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  writeLines("\n :: Select 3 clusters:")
  plot(clusterIntensity)
  rect.hclust(clusterIntensity, k = 3, border = "red")
  flowerClusters <- cutree(clusterIntensity, k = 3)
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Cluster selection in the Dendrogram
#+NAME:     fig:FlowerDendrogram02
#+ATTR_LaTeX: placement: [H]
[[../graphs/FlowerDendrogram02.png]]

The input would be ~clusterIntensity~. And then we have to specify ~k=3~,
because we would like to have three clusters. Now let us output the
~flowerClusters~ variable to see how it looks. So ~flowerClusters~.

[[../graphs/BP-FlowerDendrogram.png]]

And we see that ~flowerClusters~ is actually a vector that assigns each
intensity value in the flower vector to a cluster. It actually has the
same length, which is 2,500, and has values 1, 2, and 3, which
correspond to each cluster.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Summary of the flowerClusters vector:")
  str(flowerClusters)
#+end_src

#+RESULTS:
:
:  :: Summary of the flowerClusters vector:
:  int [1:2500] 1 1 1 1 1 1 1 1 1 1 ...

To find the mean intensity value of each of our clusters:

#+begin_src R :session :results output :exports all
  writeLines("\n :: Find mean intensity values:")
  tapply(flowerVector, flowerClusters, mean)
#+end_src

#+RESULTS:
:
:  :: Find mean intensity values:
:          1          2          3
: 0.08574315 0.50826255 0.93147713

What we obtain is that the first cluster has a mean intensity value of
0.08, which is closest to zero, and this means that it corresponds to
the darkest shape in our image.  And then the third cluster here,
which is closest to 1, corresponds to the fairest shade.

And now the fun part. Let us see how the image was segmented. To
output an image, we can use the image function in R, which takes a
matrix as an input. But the variable ~flowerClusters~, as we just saw,
is a vector.

So we need to convert it into a matrix. We can do this by setting the
dimension of this variable by using the dimension function.

#+BEGIN_SRC R :var basename="SegmentedImageCluster" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  writeLines("\n :: Plot the image and the clusters:")
  dim(flowerClusters) <- c(50,50)
  image(flowerClusters, axes = FALSE)
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Segmented flower image using clusters.
#+NAME:     fig:SegmentedImageCluster
#+ATTR_LaTeX: placement: [H]
[[../graphs/SegmentedImageCluster.png]]

The darkest shade corresponds to the background, and this is actually
associated with the first cluster. The one in the middle is the core
of the flower, and this is cluster 2.

And then the petals correspond to cluster 3, which has the fairest
shade in our image. Let us now check how the original image looked.

Let's go back to the console and then maximize it here. So let's go
back to our image function, but now this time the input is the flower
matrix. And then let's keep the axis as false. But now, how about we
add an additional argument regarding the color scheme? Let's make it
grayscale.

So we're going to take the color, and it's going to take the function
gray. And the input to this function is a sequence of values that goes
from 0 to 1, which actually is from black to white.

And then we have to also specify its length, and that's specified as
256, because this corresponds to the convention for grayscale.

#+BEGIN_SRC R :var basename="FlowerOriginalImage" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  # Original image
  image(flowerMatrix,axes=FALSE,col=grey(seq(0,1,length=256)))
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Original image.
#+NAME:     fig:FlowerOriginalImage
#+ATTR_LaTeX: placement: [H]
[[../graphs/FlowerOriginalImage.png]]

Now we can see our original grayscale image here. You can see that it
has a very, very low resolution. But in our next video, we will try to
segment an MRI image of the brain that has a much, much higher
resolution.

** Video 4: MRI Image

We will try to segment an MRI brain image of a healthy patient using
hierarchical clustering. And remember that this healthy data set
consists of a matrix of intensity values, so let's set the header to
false.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Let's try this with an MRI image of the brain:")

  healthy <- read.csv("../data/healthy.csv", header = FALSE)
  healthyMatrix <- as.matrix(healthy)
  str(healthyMatrix)
#+end_src

#+RESULTS:
:
:  :: Let's try this with an MRI image of the brain:
:  num [1:566, 1:646] 0.00427 0.00855 0.01282 0.01282 0.01282 ...
:  - attr(*, "dimnames")=List of 2
:   ..$ : NULL
:   ..$ : chr [1:646] "V1" "V2" "V3" "V4" ...

And then we realize that we have 566 by 646 pixel resolution for our
image.  So this MRI image is considerably larger than the little
flower image that we saw, and we worked with in the previous section.

To see the MRI image, we can use the image function:

#+BEGIN_SRC R :var basename="RMIoriginalImage" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  # Plot image
  image(healthyMatrix, axes = FALSE, col = grey(seq(0, 1, length = 256)))
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  RMI original image
#+NAME:     fig:RMIoriginalImage
#+ATTR_LaTeX: placement: [H]
[[../graphs/RMIoriginalImage.png]]

We see that what we have is the T2-weighted MRI imaging of a top
section of the brain. And it shows different substances, such as the
gray matter, the white matter, and the cerebrospinal fluid.

Now let us see if we can isolate these substances via hierarchical
clustering. We first need to convert the healthy matrix to a vector,
and let's call it ~healthyVector~.

We first need to convert the healthy matrix to a vector:

#+begin_src R :session :results output :exports all
  writeLines("\n :: Hierarchial clustering (Memory error)")
  healthyVector <- as.vector(healthyMatrix)
  ## distance = dist(healthyVector, method = "euclidean")

  writeLines("\n :: Search for memory alternative, run on Ithaca!")
#+end_src

#+RESULTS:
:
:  :: Hierarchial clustering (Memory error)
:
:  :: Search for memory alternative, run on Ithaca!

R gives us an error that seems to tell us that our vector is huge, and
R cannot allocate enough memory.

Well let us see how big is our vector. So we're going to go and use
the structure function over the healthy vector, and let's see what we
obtain.

#+begin_src R :session :results output :exports all
  writeLines("\n :: We have an error - why?")
  str(healthyVector)
#+end_src

#+RESULTS:
:
:  :: We have an error - why?
:  num [1:365636] 0.00427 0.00855 0.01282 0.01282 0.01282 ...

The healthy vector has 365,636 elements. Let's call this number n.

#+begin_src R :session :results output :exports all
  n <- 365636

  writeLines("\n :: The number of calculations:")
  (n*(n - 1)) / 2
#+end_src

#+RESULTS:
:
:  :: The number of calculations:
: [1] 66844659430

Of course R would complain. It's 67 billion values that we're asking R
to store in a matrix. The bad news now is that we cannot use
hierarchical clustering.

Lets try another clustering method is k-means. Let us review it first,
and see if it could work on our high resolution image. The k-means
clustering algorithm aims at partitioning the data into k clusters, in
a way that each data point belongs to the cluster whose mean is the
nearest to it.

[[../graphs/BP-k-MeansClustering.png]]

Let's go over the algorithm step-by-step. In this example we have five
data points. The first step is to specify the number of clusters. And
suppose we wish to find two clusters, so set ~k = 2~.

[[../graphs/PD-k-MeansClustering02.png]]

Then we start by randomly grouping the data into two clusters. For
instance, three points in the red cluster, and the remaining two
points in the grey cluster.

[[../graphs/PD-k-MeansClustering03.png]]

Here we see the two remainder black points grouped in the gray
cluster:

[[../graphs/PD-k-MeansClustering04.png]]

The next step is to compute the cluster means or centroids. Let's
first compute the mean of the red cluster, and then the mean of the
grey cluster is simply the midpoint.

[[../graphs/PD-k-MeansClustering05.png]]

Now remember that the k-means clustering algorithm tries to cluster
points according to the nearest mean. But this red point over here
seems to be closer to the mean of the grey cluster, then to the mean
of the red cluster to which it was assigned in the previous step.

So intuitively, the next step in the k-means algorithm is to re-assign
the data points to the closest cluster mean. As a result, now this red
point should be in the grey cluster.

[[../graphs/PD-k-MeansClustering06.png]]

Now that we moved one point from the red cluster over to the grey
cluster, we need to update the means. This is exactly the next step in
the k-means algorithm.

[[../graphs/PD-k-MeansClustering07.png]]

So let's recompute the mean of the red cluster, and then re-compute
the mean of the grey cluster.

[[../graphs/PD-k-MeansClustering08.png]]

Now we go back to Step 4. Is there any point here that seems to be
cluster to a cluster mean that it does not belong to? If so, we need
to re-assign it to the other cluster.

However, in this case, all points are closest to their cluster mean,
so the algorithm is done, and we can stop.

** Video 5: K-Means Clustering

*SCREE plots*

While dendrograms can be used to select the final number of clusters
for Hierarchical Clustering, we can't use dendrograms for k-means
clustering. However, there are several other ways that the number of
clusters can be selected. One common way to select the number of
clusters is by using a scree plot, which works for any clustering
algorithm.

A standard scree plot has the number of clusters on the x-axis, and
the sum of the within-cluster sum of squares on the y-axis. The
within-cluster sum of squares for a cluster is the sum, across all
points in the cluster, of the squared distance between each point and
the centroid of the cluster.  We ideally want very small
within-cluster sum of squares, since this means that the points are
all very close to their centroid.

To create the scree plot, the clustering algorithm is run with a range
of values for the number of clusters. For each number of clusters, the
within-cluster sum of squares can easily be extracted when using
k-means clustering. For example, suppose that we want to cluster the
MRI image from this video into two clusters. We can first run the
k-means algorithm with two clusters:

~KMC2 = kmeans(healthyVector, centers = 2, iter.max = 1000)~

Then, the within-cluster sum of squares is just an element of ~KMC2~:

~KMC2$withinss~

This gives a vector of the within-cluster sum of squares for each
cluster (in this case, there should be two numbers).

Now suppose we want to determine the best number of clusters for this
dataset. We would first repeat the kmeans function call above with
centers = 3, centers = 4, etc. to create KMC3, KMC4, and so on. Then,
we could generate the following plot:

~NumClusters = seq(2,10,1)~

~SumWithinss = c(sum(KMC2$withinss), sum(KMC3$withinss), sum(KMC4$withinss), sum(KMC5$withinss), sum(KMC6$withinss), sum(KMC7$withinss), sum(KMC8$withinss), sum(KMC9$withinss), sum(KMC10$withinss))~

~plot(NumClusters, SumWithinss, type="b")~

The plot looks like this (the type="b" argument just told the plot
command to give us points and lines):

[[../graphs/ScreePlot.jpg]]

To determine the best number of clusters using this plot, we want to
look for a bend, or elbow, in the plot. This means that we want to
find the number of clusters for which increasing the number of
clusters further does not significantly help to reduce the
within-cluster sum of squares. For this particular dataset, it looks
like 4 or 5 clusters is a good choice. Beyond 5, increasing the number
of clusters does not really reduce the within-cluster sum of squares
too much.

Note: You may have noticed it took a lot of typing to generate
SumWithinss; this is because we limited ourselves to R functions we've
learned so far in the course. In fact, R has powerful functions for
repeating tasks with a different input (in this case running kmeans
with different cluster sizes). For instance, we could generate
SumWithinss with:

~SumWithinss = sapply(2:10, function(x) sum(kmeans(healthyVector, centers=x, iter.max=1000)$withinss))~

We won't be teaching more advanced R functions like sapply in this
course, but if you're interested you could read more [[http://www.r-bloggers.com/using-apply-sapply-lapply-in-r/][here]].

*RECALL*

Recall from our last video that it was impossible for us to use
hierarchical clustering because of the high resolution of our image.

The first step in k-means clustering involves specifying the number of
clusters, k. *But how do we select k?* Well, our clusters would ideally
assign each point in the image to a tissue class or a particular
substance, for instance, grey matter or white matter, and so on.

And these substances are known to the medical community. So setting
the number of clusters depends on exactly what you're trying to
extract from the image.

For the sake of our example, let's set the number of clusters here, k,
to five. And since the k-means clustering algorithm starts by randomly
assigning points to clusters, we should set the seed, so that we all
obtain the same clusters.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Specify number of clusters k = 5...")
  k <- 5

  writeLines("\n :: Run k-means...")
  set.seed(1)
  KMC <- kmeans(healthyVector, centers = k, iter.max = 1000)
  str(KMC)
#+end_src

#+RESULTS:
#+begin_example

 :: Specify number of clusters k = 5...

 :: Run k-means...
 ..$ : chr [1:5] "1" "2" "3" "4" ...
  .. ..$ : NULL
 $ totss       : num 5775
 $ withinss    : num [1:5] 96.6 47.2 39.2 57.5 62.3
 $ tot.withinss: num 303
 $ betweenss   : num 5472
 $ size        : int [1:5] 20556 101085 133162 31555 79278
 $ iter        : int 2
 $ ifault      : int 0
 - attr(*, "class")= chr "kmeans"
#+end_example

And then finally, since the k-means is an iterative method that could
take very long to converge, we need to set a maximum number of
iterations. And we can do this by typing iter.max, and give it, for
instance, the value 1,000.

The k-means algorithm is actually quite fast, even though we have a
high resolution image. Now to see the result of the k-means clustering
algorithm, we can output the structure of the KMC variable.

The first, and most important, piece of information that we get, is
the cluster vector. Which assigns each intensity value in the healthy
vector to a cluster. In this case, it will be giving them values 1
through 5, since we have 5 clusters.

Now recall that to output the segmented image, we need to extract this
vector. The way to do this is by using the dollar notation.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Extract clusters:")
  healthyClusters <- KMC$cluster
#+end_src

#+RESULTS:
:
:  :: Extract clusters:

Now how can we obtain the mean intensity value within each of our 5
clusters? In hierarchical clustering, we needed to do some manual
work, and use the tapply function to extract this information. In this
case, we have the answers ready, under the vector centers.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Extract clusters centers intensities:")
  KMC$centers
#+end_src

#+RESULTS:
:
:  :: Extract clusters centers intensities:
:         [,1]
: 1 0.48177191
: 2 0.10619450
: 3 0.01961886
: 4 0.30942825
: 5 0.18420578

In fact, for instance, the mean intensity value of the first cluster
is 0.48, and the mean intensity value of the last cluster is 0.18.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Extract cluster 2 center intensity:")
  KMC$centers[2]
#+end_src

#+RESULTS:
:
:  :: Extract cluster 2 center intensity:
: [1] 0.1061945

For instance, ~KMC$centers[2]~. This should give us the mean intensity
value of the second cluster, which is $0.1$. And indeed, this is what we
obtain.

Before we move on, I would like to point your attention to one last
interesting piece of information that we can get here. And that is the
size of the cluster.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Extract clusters sizes:")
  KMC$size
#+end_src

#+RESULTS:
:
:  :: Extract clusters sizes:
: [1]  20556 101085 133162  31555  79278

For instance, the largest cluster that we have is the third one, which
combines $133000$ values in it. And interestingly, it's the one that
has the smallest mean intensity value, which means that it corresponds
to the darkest shade in our image.

Actually, if we look at all the mean intensity values, we can see that
they are all less than $0.5$. So they're all pretty close to $0$. And
this means that our images is pretty dark. If we look at our image
again, it's indeed very dark. And we have very few points that are
actually white.

Now the exciting part. Let us output the segmented image and see what
we get. Recall that we first need to convert the vector healthy
clusters to a matrix.

To do this, we will use the dimension function, that takes as an input
the ~healthyClusters~ vector. And now we're going to turn it into a
matrix. So we have to specify using the combine function, the number
of rows, and the number of columns that we want.

We should make sure that it corresponds to the same size as the
healthy matrix. And since we've forgot the number of rows and the
number columns in the healthy matrix, we can simply use the ~nrow~ and
~ncol~ function to get them.

So the first input right now would be ~nrow~ of ~healthyMatrix~. And
then the second input would be the number of columns of the healthy
matrix.

And then let's be creative and use a fancy color scheme. We're going
to invoke for color here, the rainbow palette in R. And the rainbow
palette, or the function rainbow, takes as an input the number of
colors that we want. In this case, the number of colors would
correspond to the number of clusters. So the input would be ~k~.

#+BEGIN_SRC R :var basename="RMIprocessedImage" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  # Plot the image with the clusters
  dim(healthyClusters) = c(nrow(healthyMatrix), ncol(healthyMatrix))

  image(healthyClusters, axes = FALSE, col=rainbow(k))
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  RMI result image after k-means clustering
#+NAME:     fig:RMIprocessedImage
#+ATTR_LaTeX: placement: [H]
[[../graphs/RMIprocessedImage.png]]

We see that k-means algorithm was able to segment the image in 5
different clusters. More refinement maybe needs to be made to our
clustering algorithm to appropriately capture all the anatomical
structures. But this seems like a good starting point.

The question now is, can we use the clusters, or the classes, found by
our k-means algorithm on the healthy MRI image to identify tumors in
another MRI image of a *sick patient*?

** Video 6: Detecting Tumors

In the previous section we identified clusters, or tissue substances,
in a healthy brain image. It would be really helpful if we can use
these clusters to automatically detect tumors in MRI images of sick
patients.

The ~tumor.csv~ file corresponds to an MRI brain image of a patient
with *oligodendroglioma*, a tumor that commonly occurs in the front
lobe of the brain.

Since brain biopsy is the only definite diagnosis of this tumor, MRI
guidance is key in determining its location and geometry.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Apply to a test image. Loading the test data...")

  tumor <- read.csv("../data/tumor.csv", header = FALSE)
  tumorMatrix <- as.matrix(tumor)
  tumorMatrix <- t(tumorMatrix)[,nrow(tumorMatrix):1]
  tumorVector <- as.vector(tumorMatrix)
#+end_src

#+RESULTS:
:
:  :: Apply to a test image. Loading the test data...

Now, we will not run the k-means algorithm again on the tumor
vector. Instead, we will apply the k-means clustering results that we
found using the healthy brain image on the tumor vector.

In other words, we treat the *healthy vector as training set* and the
*tumor vector as a testing set*.

To do this, we first need to install a new package that is called
~flexclust~.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Install new package: flexclust ...")
  ## install.packages('flexclust', repos='http://cran.rstudio.com/')
  writeLines("\n :: NOTE: Please comment after install once...")

  library(flexclust)
  writeLines("\n :: Library flexclust loaded...")
#+end_src

#+RESULTS:
:
:  :: Install new package: flexclust ...
:
:  :: NOTE: Please comment after install once...
:
:  :: Library flexclust loaded...

The ~flexclust~ package contains the object class ~KCCA~, which stands
for K-Centroids Cluster Analysis.  We need to convert the information
from the clustering algorithm to an object of the class ~KCCA~.

And this conversion is needed before we can use the predict function
on the test set ~tumorVector~.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Apply clusters from before to new image, using the flexclust package...")
  KMC.kcca <- as.kcca(KMC, healthyVector)
#+end_src

#+RESULTS:
:
:  :: Apply clusters from before to new image, using the flexclust package...

We can cluster the pixels in the ~tumorVector~ using the predict
function.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Clustering with the testing data...")
  tumorClusters <- predict(KMC.kcca, newdata = tumorVector)
#+end_src

#+RESULTS:
:
:  :: Clustering with the testing data...

And now, the tumorClusters is a vector that assigns a value 1 through
5 to each of the intensity values in the tumorVector, as predicted by
the k-means algorithm.

To output the segmented image, we first need to convert the tumor
clusters to a matrix.

#+BEGIN_SRC R :var basename="RMISegmentedImage" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  # Visualize the clusters
  dim(tumorClusters) = c(nrow(tumorMatrix), ncol(tumorMatrix))

  image(tumorClusters, axes = FALSE, col=rainbow(k))
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  RMI segmented image
#+NAME:     fig:RMISegmentedImage
#+ATTR_LaTeX: placement: [H]
[[../graphs/RMISegmentedImage.png]]

Oh, and yes, we do! It is this abnormal substance here that is
highlighted in *red* that was not present in the healthy MRI image. So
we were successfully able to identify, more or less, the geometry of
the malignant structure.

We see that we did a good job capturing the major tissue substances of
the brain. The grey matter is highlighted in purple and the white
matter in yellow. For the sick patient, the substance highlighted in
*red* is the oligodendroglioma tumor.

Now we can see the original test image to compare:

#+BEGIN_SRC R :var basename="RMIOriginalTestingImage" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  ## Plot test image
  image(tumorMatrix, axes = FALSE, col = grey(seq(0, 1, length = 256)))
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  RMI original testing image
#+NAME:     fig:RMIOriginalTestingImage
#+ATTR_LaTeX: placement: [H]
[[../graphs/RMIOriginalTestingImage.png]]

We see that we did a good job capturing the major tissue substances of
the brain. The grey matter is highlighted in purple and the white
matter in yellow.

For the sick patient, the substance highlighted in blue is the
oligodendroglioma tumor (video and slides colors).

[[../graphs/BP-SegmentedMRIimages.png]]

Notice that we do not see substantial blue regions in the healthy
brain image, apart from the region around the eyes. Actually, looking
at the eyes regions, we notice that the two images were not taken
precisely at the same section of the brain.

This might explain some differences in shapes between the two images.

Let's see how the images look like originally. We see that the tumor
region has a lighter color intensity, which is very similar to the
region around the eyes in the healthy brain image. This might explain
highlighting this region in blue.

[[../graphs/BP-T2WeightedMRIimages.png]]

Of course, we cannot claim that we did a wonderful job obtaining the
exact geometries of all the tissue substances, but we are definitely
on the right track. In fact, to do so, we need to use more advanced
algorithms and fine-tune our clustering technique.

MRI image segmentation is an ongoing field of research. While k-means
clustering is a good starting point, more advanced techniques have
been proposed in the literature, such as the modified fuzzy k-means
clustering method.

Also, if you are interested, R has packages that are specialized for
analyzing medical images.

[[../graphs/BP-MRIimageSegmentation.png]]

Now, if we had MRI axial images taken at different sections of the
brain, we could segment each image and capture the geometries of the
substances at different levels. Then, by interpolating between the
segmented images, we can estimate the missing slices, and we can then
obtain a 3D reconstruction of the anatomy of the brain from 2D MRI
cross-sections.

[[../graphs/BP-3DReconstruction.png]]

In fact, 3D reconstruction is particularly important in the medical
field for diagnosis, surgical planning, and biological research
purposes. I hope that this recitation gave you a flavor of this
fascinating field of image segmentation.

** Video 7: Comparing Methods

We will compare all the different methods we have seen so far in this
course and review what they are used for, their benefits, and
limitations.

Linear regression is used to predict a continuous outcome. If we have
a nonlinear relationship, we need to add variables to our analysis.

[[../graphs/BP-Comparison.png]]

Logistic regression is used to predict a categorical outcome. We
mainly focused on binary outcomes, like yes or no, sell or buy, accept
or reject, and so on.

We have seen it applied to predict the quality of care, good or bad;
the winner of the US presidential election, Republican or Democrat;
parole violation and loan payment, yes or no.

In the trees week we learned CART, which is used to predict a
categorical outcome, with possibly more than two categories, like
quality rating, from one to five, and three decisions, say, buy, sell,
or hold.

It can also predict a continuous outcome, such as salary or price. We
have seen it applied to predict life expectancy, earnings from census
data, and letter recognition.

The power of CART lies in the fact that it can handle nonlinear
relationships between variables. The tree representation makes it easy
to visualize and interpret the results.

[[../graphs/BP-Comparison02.png]]

Random forest is also used to predict categorical outcomes or
continuous outcomes. Its benefit over CART is that it can improve the
prediction accuracy.

This week, we learned hierarchical clustering, which is used to find
similar groups. An important aspect of clustering data into smaller
groups is that we can improve our prediction accuracy by applying our
predictive methods, like logistic regression for instance, on each
cluster.

[[../graphs/BP-Comparison03.png]]

*Hierarchical clustering* is an attractive technique, because we do
 not need to select the number of clusters before running the
 algorithm.

An alternative method is k-means clustering, which works well on data
sets of any size. However, k-means requires selecting the number of
clusters before running the algorithm.

This may not be a limitation if we have an intuition of the number of
clusters we want to look at, as in the medical image segmentation
example.
