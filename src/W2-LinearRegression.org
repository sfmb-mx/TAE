#+TITLE:         Unit 2 - Linear Regression
#+AUTHOR:        Sergio-Feliciano Mendoza-Barrera
#+DRAWERS:       Jaalkab
#+EMAIL:         smendoza.barrera@gmail.com
#+DATE:          11/06/2015
#+DESCRIPTION:   R introduction, remembering the syntax and some useful examples
#+KEYWORDS:      R, data science, emacs, ESS, org-mode
#+LANGUAGE:      en
#+OPTIONS:       H:10 num:t toc:nil \n:nil @:t ::t |:t ^:{} -:t f:t *:t <:t d:HIDDEN
#+OPTIONS:       TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+OPTIONS:       LaTeX:dvipng
#+INFOJS_OPT:    view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:
#+LINK_HOME:
#+XSLT:
#+STYLE: <link rel="stylesheet" type="text/css" href="dft.css"/>

#+LaTeX_CLASS: IEEEtran
#+LATEX_CLASS_OPTIONS: [letterpaper, 9pt, onecolumn, twoside, technote, final]
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage{makeidx}

#+LATEX_HEADER: \usepackage[lining,tabular]{fbb} % so math uses tabular lining figures
#+LATEX_HEADER: \usepackage[scaled=.95,type1]{cabin} % sans serif in style of Gill Sans
#+LATEX_HEADER: \usepackage[varqu,varl]{zi4}% inconsolata typewriter
#+LATEX_HEADER: \usepackage[T1]{fontenc} % LY1 also works
#+LATEX_HEADER: \usepackage[libertine,bigdelims]{newtxmath}
#+LATEX_HEADER: \usepackage[cal=boondoxo,bb=boondox,frak=boondox]{mathalfa}
#+LATEX_HEADER: \useosf % change normal text to use proportional oldstyle figures

#+LATEX_HEADER: \markboth{Reporte de gastos Febrero - Abril, 2015}%
#+LATEX_HEADER: {Sergio-Feliciano Mendoza-Barrera - CEO Global Labs Mexico}

#+LATEX_HEADER: \newcommand{\degC}{$^\circ$C{}}

#+STYLE: <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script>

#+ATTR_HTML: width="500px"

# -*- mode: org; -*-
#+OPTIONS:   toc:2

#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>

#+HTML_HEAD: <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
#+HTML_HEAD: <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>

#+BEGIN_ABSTRACT
Linear Regression topics. For the course "MITx: 15.071x The Analytics Edge".
#+END_ABSTRACT

* The Statistical Sommelier: An Introduction to Linear Regression

** Quick Question (1 point possible)

The plots below show the relationship between two of the independent
variables considered by Ashenfelter and the price of wine.

[[../graphs/Wine_QQ1_Plot1.png]]

[[../graphs/Wine_QQ1_Plot2.png]]

What is the correct relationship between harvest rain, average growing
season temperature, and wine prices?

*** Answer

More harvest rain is associated with a lower price, and higher
temperatures is associated with a higher price

**** Explanation

The plots show a positive trend between average growing season
temperature and the wine price. While the trend is less clear between
harvest rain and price, there is a slight negative association.

** Quick Question (4 points possible)

The following figure shows three data points and the best fit line

y = 3x + 2.

The x-coordinate, or "x", is our independent variable and the
y-coordinate, or "y", is our dependent variable.

[[../graphs/Wine_QQ2.png]]

Please answer the following questions using this figure.

#+BEGIN_SRC R :session :results output :exports all
  x <- c(1, 0, 1); y <- c(8, 2, 2); beta0 <- rep(mean(y), 3)
  yHat <- (3 * x) + 2

  eSqModel <- (y - yHat)^2
  eSqBL <- (y - beta0)^2
  data <- data.frame(x, y, yHat, eSqModel, eSqBL)

  writeLines("\n    The baseline prediction")
  beta0[1]

  writeLines("\n    The SSE for the model yHat")
  sum(data$eSqModel)

  writeLines("\n    The SSE for the Baseline")
  sum(data$eSqBL)

  writeLines("\n    The R^2 of the model")
  1 - (sum(data$eSqModel) / sum(data$eSqBL))
#+END_SRC

#+RESULTS:
#+begin_example

    The baseline prediction
[1] 4

    The SSE for the model yHat
[1] 18

    The SSE for the Baseline
[1] 24

    The R^2 of the model
[1] 0.25
#+end_example

*** Question a

What is the baseline prediction?

**** Answer

The baseline prediction is the average value of the dependent
variable. Since our dependent variable takes values 2, 2, and 8 in our
data set, the average is (2+2+8)/3 = 4.

*** Question b

What is the Sum of Squared Errors (SSE)?

**** Answer

The SSE is computed by summing the squared errors between the actual
values and our predictions. For each value of the independent variable
(x), our best fit line makes the following predictions:

If x = 0, y = 3(0) + 2 = 2,

If x = 1, y = 3(1) + 2 = 5.

Thus we make an error of 0 for the data point (0,2), an error of 3 for
the data point (1,2), and an error of 3 for the data point (1,8). So
we have

SSE = 0² + 3² + 3² = 18.

*** Question c

What is the Total Sum of Squares (SST)?

**** Answer

The SST is computed by summing the squared errors between the actual
values and the baseline prediction. From the first question, we
computed the baseline prediction to be 4. Thus the SST is:

SST = (2 - 4)² + (2 - 4)² + (8 - 4)² = 24.

*** Question d

What is the R² of the model?

**** Answer

The R² formula is:

R² = 1 - SSE/SST

Thus using our answers to the previous questions, we have that

R² = 1 - 18/24 = 0.25.

** Quick Question (1/1 point)

Suppose we add another variable, Average Winter Temperature, to our
model to predict wine price. Is it possible for the model's R² value
to go down from 0.83 to 0.80?

*** Answer

The model's R² value can never decrease from adding new variables to
the model. This is due to the fact that it is always possible to set
the coefficient for the new variable to zero in the new
model. However, this would be the same as the old model. So the only
reason to make the coefficient non-zero is if it improves the R² value
of the model, since linear regression picks the coefficients to
minimize the error terms, which is the same as maximizing the R².

** Video 4: Linear Regression in R

Before starting this video, please download the datasets wine.csv and
wine_test.csv. Save them to a folder on your computer that you will
remember, and in R, navigate to this folder (File->Change dir... on a
PC, and Misc->Change Working Directory on a Mac). This data comes from
Liquid Assets.

A script file containing all of the R commands used in this lecture
can be downloaded here.

*** Download the data sets

In this part we can download the data

#+BEGIN_SRC R :session :results output :exports all
  library(parallel)

  if(!file.exists("../data")) {
          dir.create("../data")
  }

  fileUrl <-
          c("https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/wine.csv", "https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/wine_test.csv")

  fileName <- c("wine.csv", "wine_test.csv")

  dataPath <- "../data"

  for(i in 1:2) {
          filePath <- paste(dataPath, fileName[i], sep = "/")

          if(!file.exists(filePath)) {
                  download.file(fileUrl[i], destfile = filePath, method = "curl")
          }
  }
  list.files("../data")
#+END_SRC

#+RESULTS:
:  [1] "AnonymityPoll.csv"      "BoeingStock.csv"        "CPSData.csv"
:  [4] "CocaColaStock.csv"      "CountryCodes.csv"       "GEStock.csv"
:  [7] "IBMStock.csv"           "MetroAreaCodes.csv"     "NBA_test.csv"
: [10] "NBA_train.csv"          "ProcterGambleStock.csv" "README.md"
: [13] "USDA.csv"               "WHO.csv"                "WHO_Europe.csv"
: [16] "baseball.csv"           "mvtWeek1.csv"           "wine.csv"
: [19] "wine_test.csv"

*** Load the wine data set

#+BEGIN_SRC R :session :results output :exports all
  writeLines("    Loading data into their data frames.")
  wine <- read.table("../data/wine.csv", sep = ",", header = TRUE)

  str(wine)
  summary(wine)
#+END_SRC

#+RESULTS:
#+begin_example
    Loading data into their data frames.
'data.frame':	25 obs. of  7 variables:
 $ Year       : int  1952 1953 1955 1957 1958 1959 1960 1961 1962 1963 ...
 $ Price      : num  7.5 8.04 7.69 6.98 6.78 ...
 $ WinterRain : int  600 690 502 420 582 485 763 830 697 608 ...
 $ AGST       : num  17.1 16.7 17.1 16.1 16.4 ...
 $ HarvestRain: int  160 80 130 110 187 187 290 38 52 155 ...
 $ Age        : int  31 30 28 26 25 24 23 22 21 20 ...
 $ FrancePop  : num  43184 43495 44218 45152 45654 ...
      Year          Price         WinterRain         AGST        HarvestRain
 Min.   :1952   Min.   :6.205   Min.   :376.0   Min.   :14.98   Min.   : 38.0
 1st Qu.:1960   1st Qu.:6.519   1st Qu.:536.0   1st Qu.:16.20   1st Qu.: 89.0
 Median :1966   Median :7.121   Median :600.0   Median :16.53   Median :130.0
 Mean   :1966   Mean   :7.067   Mean   :605.3   Mean   :16.51   Mean   :148.6
 3rd Qu.:1972   3rd Qu.:7.495   3rd Qu.:697.0   3rd Qu.:17.07   3rd Qu.:187.0
 Max.   :1978   Max.   :8.494   Max.   :830.0   Max.   :17.65   Max.   :292.0
      Age         FrancePop
 Min.   : 5.0   Min.   :43184
 1st Qu.:11.0   1st Qu.:46584
 Median :17.0   Median :50255
 Mean   :17.2   Mean   :49694
 3rd Qu.:23.0   3rd Qu.:52894
 Max.   :31.0   Max.   :54602
#+end_example

*** Building the models

Lets begin with a model with only one variable:

**** One variable model

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Linear Regression (one variable)")
  model1 <- lm(Price ~ AGST, data = wine)
  summary(model1)

  writeLines("\n :: Sum of Squared Errors:")
  model1$residuals

  writeLines("\n :: Calculating SSE:")
  SSE = sum(model1$residuals^2)
  SSE
#+END_SRC

#+RESULTS:
#+begin_example

 :: Linear Regression (one variable)

Call:
lm(formula = Price ~ AGST, data = wine)

Residuals:
     Min       1Q   Median       3Q      Max
-0.78450 -0.23882 -0.03727  0.38992  0.90318

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  -3.4178     2.4935  -1.371 0.183710
AGST          0.6351     0.1509   4.208 0.000335 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4993 on 23 degrees of freedom
Multiple R-squared:  0.435,	Adjusted R-squared:  0.4105
F-statistic: 17.71 on 1 and 23 DF,  p-value: 0.000335

 :: Sum of Squared Errors:
          1           2           3           4           5           6
 0.04204258  0.82983774  0.21169394  0.15609432 -0.23119140  0.38991701
          7           8           9          10          11          12
-0.48959140  0.90318115  0.45372410  0.14887461 -0.23882157 -0.08974238
         13          14          15          16          17          18
 0.66185660 -0.05211511 -0.62726647 -0.74714947  0.42113502 -0.03727441
         19          20          21          22          23          24
 0.10685278 -0.78450270 -0.64017590 -0.05508720 -0.67055321 -0.22040381
         25
 0.55866518

 :: Calculating SSE:
[1] 5.734875
#+end_example

Beside it is a number labeled Adjusted R-squared. In this case, it's
0.41. This number adjusts the R-squared value to account for the
number of independent variables used relative to the number of data
points. Multiple R-squared will always increase if you add more
independent variables.

But Adjusted R-squared will decrease if you add an independent
variable that doesn't help the model. This is a good way to determine
if an additional variable should even be included in the model.

We can compute the Sum of *Squared Errors*, or *SSE*, by taking the
*sum(model1$residuals^2)*. If we type SSE and hit Enter, we can see
that our sum of squared errors is 5.73.

**** Model with two variables

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Linear Regression (two variables)")
  model2 = lm(Price ~ AGST + HarvestRain, data=wine)
  summary(model2)

  writeLines("\n :: Sum of Squared Errors")
  SSE = sum(model2$residuals^2)

  writeLines("\n :: Calculating the SSE")
  SSE
#+END_SRC

#+RESULTS:
#+begin_example

 :: Linear Regression (two variables)

Call:
lm(formula = Price ~ AGST + HarvestRain, data = wine)

Residuals:
     Min       1Q   Median       3Q      Max
-0.88321 -0.19600  0.06178  0.15379  0.59722

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept) -2.20265    1.85443  -1.188 0.247585
AGST         0.60262    0.11128   5.415 1.94e-05 ***
HarvestRain -0.00457    0.00101  -4.525 0.000167 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.3674 on 22 degrees of freedom
Multiple R-squared:  0.7074,	Adjusted R-squared:  0.6808
F-statistic: 26.59 on 2 and 22 DF,  p-value: 1.347e-06

 :: Sum of Squared Errors

 :: Calculating the SSE
[1] 2.970373
#+end_example

And if you look at the R-squared near the bottom of the output, you
can see that this variable really helped our model.

Our Multiple R-squared and Adjusted R-squared both increased
significantly compared to the previous model.

If we type SSE, we can see that the sum of squared errors for model2
is 2.97, which is much better (less) than the sum of squared errors
for model1.

**** Model with all variables

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Linear Regression (all variables)")
  model3 <- lm(Price ~ AGST + HarvestRain + WinterRain + Age + FrancePop, data=wine)
  summary(model3)

  writeLines("\n :: Sum of Squared Errors")
  SSE <- sum(model3$residuals^2)

  writeLines("\n :: Calculating the SSE for all variable model")
  SSE
#+END_SRC

#+RESULTS:
#+begin_example

 :: Linear Regression (all variables)

Call:
lm(formula = Price ~ AGST + HarvestRain + WinterRain + Age +
    FrancePop, data = wine)

Residuals:
     Min       1Q   Median       3Q      Max
-0.48179 -0.24662 -0.00726  0.22012  0.51987

Coefficients:
              Estimate Std. Error t value Pr(>|t|)
(Intercept) -4.504e-01  1.019e+01  -0.044 0.965202
AGST         6.012e-01  1.030e-01   5.836 1.27e-05 ***
HarvestRain -3.958e-03  8.751e-04  -4.523 0.000233 ***
WinterRain   1.043e-03  5.310e-04   1.963 0.064416 .
Age          5.847e-04  7.900e-02   0.007 0.994172
FrancePop   -4.953e-05  1.667e-04  -0.297 0.769578
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.3019 on 19 degrees of freedom
Multiple R-squared:  0.8294,	Adjusted R-squared:  0.7845
F-statistic: 18.47 on 5 and 19 DF,  p-value: 1.044e-06

 :: Sum of Squared Errors

 :: Calculating the SSE for all variable model
[1] 1.732113
#+end_example

we can again see that the Multiple R-squared and Adjusted R-squared
have both increased.

Let's now compute the sum of squared errors for this new model. SSE
equals the sum(model3$residuals^2).

And if we type SSE, we can see that the sum of squared errors for
model3 is 1.7, even better than before.

Another way to build the model using all variables can be written as:

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Linear Regression (all variables)")
  model3 <- lm(Price ~ ., data=wine)
  summary(model3)

  writeLines("\n :: Sum of Squared Errors")
  SSE <- sum(model3$residuals^2)

  writeLines("\n :: Calculating the SSE for all variable model")
  SSE
#+END_SRC

#+RESULTS:
#+begin_example

 :: Linear Regression (all variables)

Call:
lm(formula = Price ~ ., data = wine)

Residuals:
     Min       1Q   Median       3Q      Max
-0.48179 -0.24662 -0.00726  0.22012  0.51987

Coefficients: (1 not defined because of singularities)
              Estimate Std. Error t value Pr(>|t|)
(Intercept)  7.092e-01  1.467e+02   0.005 0.996194
Year        -5.847e-04  7.900e-02  -0.007 0.994172
WinterRain   1.043e-03  5.310e-04   1.963 0.064416 .
AGST         6.012e-01  1.030e-01   5.836 1.27e-05 ***
HarvestRain -3.958e-03  8.751e-04  -4.523 0.000233 ***
Age                 NA         NA      NA       NA
FrancePop   -4.953e-05  1.667e-04  -0.297 0.769578
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.3019 on 19 degrees of freedom
Multiple R-squared:  0.8294,	Adjusted R-squared:  0.7845
F-statistic: 18.47 on 5 and 19 DF,  p-value: 1.044e-06

 :: Sum of Squared Errors

 :: Calculating the SSE for all variable model
[1] 1.732113
#+end_example

Using *model3 <- lm(Price ~ ., data=wine)* with a dot (.) indicate to
use all variables.

** Quick Question (3 points possible)

In R, use the dataset wine.csv to create a linear regression model to
predict Price using HarvestRain and WinterRain as independent
variables.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Linear Regression (HarvestRain and WinterRain variables)")
  modelQQ3 <- lm(Price ~ HarvestRain + WinterRain, data=wine)
  summary(modelQQ3)

  writeLines("\n :: Sum of Squared Errors")
  SSE <- sum(modelQQ3$residuals^2)

  writeLines("\n :: Calculating the SSE for all variable model")
  SSE
#+END_SRC

#+RESULTS:
#+begin_example

 :: Linear Regression (HarvestRain and WinterRain variables)

Call:
lm(formula = Price ~ HarvestRain + WinterRain, data = wine)

Residuals:
    Min      1Q  Median      3Q     Max
-1.0933 -0.3222 -0.1012  0.3871  1.1877

Coefficients:
              Estimate Std. Error t value Pr(>|t|)
(Intercept)  7.865e+00  6.616e-01  11.888 4.76e-11 ***
HarvestRain -4.971e-03  1.601e-03  -3.105  0.00516 **
WinterRain  -9.848e-05  9.007e-04  -0.109  0.91392
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5611 on 22 degrees of freedom
Multiple R-squared:  0.3177,	Adjusted R-squared:  0.2557
F-statistic: 5.122 on 2 and 22 DF,  p-value: 0.01492

 :: Sum of Squared Errors

 :: Calculating the SSE for all variable model
[1] 6.925756
#+end_example

Using the summary output of this model, answer the following
questions:

What is the "Multiple R-squared" value of your model?

*** Answer

In R, create the model by typing the following line into your R
console:

modelQQ4 = lm(Price ~ HarvestRain + WinterRain, data=wine)

Then, look at the output of summary(modelQQ4). The Multiple R-squared
is listed at the bottom of the output, and the coefficients can be
found in the coefficients table.

** Understanding the Model

In the summary of the models we can see some other columns, the model
coefficients and other columns.

The remaining columns help us to determine if a variable should be
included in the model, or if its coefficient is significantly
different from 0.

A coefficient of 0 means that the value of the independent variable
does not change our prediction for the dependent variable.

If a coefficient is not significantly different from 0, then we should
probably remove the variable from our model since it's not helping to
predict the dependent variable.

The standard error column gives a measure of how much the coefficient
is likely to vary from the estimate value.

The t value is the estimate divided by the standard error. It will be
negative if the estimate is negative and positive if the estimate is
positive. The larger the absolute value of the t value, the more
likely the coefficient is to be significant.

*So we want independent variables with a large absolute value in this
 column*.

The *last column of numbers* gives a measure of how plausible it is
that the coefficient is actually 0, given the data we used to build
the model.

The less plausible it is, or the smaller the probability number in
this column, the less likely it is that our coefficient estimate is
actually 0.

This number will be large if the absolute value of the t value is
small, and it will be small if the absolute value of the t value is
large. *We want independent variables with small values in this
column*.

This is a lot of information, but the easiest way in R to determine if
a variable is significant is to look at the stars at the end of each
row.

The *star coding scheme* is explained at the bottom of the
Coefficients table.

*Three stars is the highest level of significance* and corresponds to a
probability value less than 0.001, or the smallest possible
probabilities.

*Two stars is also very significant* and corresponds to a probability
between 0.001 and 0.01.

*One star is still significant* and corresponds to a probability between
0.01 and 0.05.

*A period*, or *dot*, means that the coefficient is almost significant
 and corresponds to a probability between 0.05 and 0.10.

When we ask you to list the significant variables in a problem, we
will usually not include these. Nothing at the end of a row means that
the variable is not significant in the model.

*** Removing non-significant variables from our model


#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Remove FrancePop")
  model4 <- lm(Price ~ AGST + HarvestRain + WinterRain + Age, data = wine)
  summary(model4)
#+END_SRC

#+RESULTS:
#+begin_example

 :: Remove FrancePop

Call:
lm(formula = Price ~ AGST + HarvestRain + WinterRain + Age, data = wine)

Residuals:
     Min       1Q   Median       3Q      Max
-0.45470 -0.24273  0.00752  0.19773  0.53637

Coefficients:
              Estimate Std. Error t value Pr(>|t|)
(Intercept) -3.4299802  1.7658975  -1.942 0.066311 .
AGST         0.6072093  0.0987022   6.152  5.2e-06 ***
HarvestRain -0.0039715  0.0008538  -4.652 0.000154 ***
WinterRain   0.0010755  0.0005073   2.120 0.046694 *
Age          0.0239308  0.0080969   2.956 0.007819 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.295 on 20 degrees of freedom
Multiple R-squared:  0.8286,	Adjusted R-squared:  0.7943
F-statistic: 24.17 on 4 and 20 DF,  p-value: 2.036e-07
#+end_example

We can see that the *R-squared*, for this model, is *0.8286* and our
Adjusted R-squared is *0.79*.

If we scroll back up in our R Console, we can see that for model3, the
R-squared was *0.8294*, and the Adjusted R-squared was *0.7845*.

So this model is just as strong, if not stronger, than the previous
model because our Adjusted R-squared actually increased by removing
*FrancePopulation*.

** Quick Question (2 points possible)

Use the dataset wine.csv to create a linear regression model to
predict Price using HarvestRain and WinterRain as independent
variables, like you did in the previous quick question.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Linear Regression (HarvestRain and WinterRain variables)")
  modelQQ3 <- lm(Price ~ HarvestRain + WinterRain, data=wine)
  summary(modelQQ3)

  writeLines("\n :: Sum of Squared Errors")
  SSE <- sum(modelQQ3$residuals^2)

  writeLines("\n :: Calculating the SSE for all variable model")
  SSE
#+END_SRC

#+RESULTS:
#+begin_example

 :: Linear Regression (HarvestRain and WinterRain variables)

Call:
lm(formula = Price ~ HarvestRain + WinterRain, data = wine)

Residuals:
    Min      1Q  Median      3Q     Max
-1.0933 -0.3222 -0.1012  0.3871  1.1877

Coefficients:
              Estimate Std. Error t value Pr(>|t|)
(Intercept)  7.865e+00  6.616e-01  11.888 4.76e-11 ***
HarvestRain -4.971e-03  1.601e-03  -3.105  0.00516 **
WinterRain  -9.848e-05  9.007e-04  -0.109  0.91392
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5611 on 22 degrees of freedom
Multiple R-squared:  0.3177,	Adjusted R-squared:  0.2557
F-statistic: 5.122 on 2 and 22 DF,  p-value: 0.01492

 :: Sum of Squared Errors

 :: Calculating the SSE for all variable model
[1] 6.925756
#+end_example

Using the summary output of this model, answer the following
questions:

*** Question a

Is the coefficient for HarvestRain significant?

**** Answer

Yes.

*** Question b

Is the coefficient for WinterRain significant?

**** Answer

No.

*From the summary output, you can see that HarvestRain is significant
 (two stars), but WinterRain is not (no stars)*.

** Video 6: Correlation and Multicollinearity

We observed that *Age* and *FrancePopulation* are highly
correlated. But what is correlation?

*Correlation* measures the linear relationship between two variables
and is a number between -1 and +1. A correlation of +1 means a perfect
positive linear relationship. A correlation of -1 means a perfect
negative linear relationship.

In the middle of these two extremes is a *correlation of 0*, which means
that there is no linear relationship between the two variables.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Correlations")
  cor(wine$WinterRain, wine$Price)
  cor(wine$Age, wine$FrancePop)
  cor(wine)
#+END_SRC

#+RESULTS:
#+begin_example

 :: Correlations
[1] 0.1366505
[1] -0.9944851
                   Year      Price   WinterRain        AGST HarvestRain
Year         1.00000000 -0.4477679  0.016970024 -0.24691585  0.02800907
Price       -0.44776786  1.0000000  0.136650547  0.65956286 -0.56332190
WinterRain   0.01697002  0.1366505  1.000000000 -0.32109061 -0.27544085
AGST        -0.24691585  0.6595629 -0.321090611  1.00000000 -0.06449593
HarvestRain  0.02800907 -0.5633219 -0.275440854 -0.06449593  1.00000000
Age         -1.00000000  0.4477679 -0.016970024  0.24691585 -0.02800907
FrancePop    0.99448510 -0.4668616 -0.001621627 -0.25916227  0.04126439
                    Age    FrancePop
Year        -1.00000000  0.994485097
Price        0.44776786 -0.466861641
WinterRain  -0.01697002 -0.001621627
AGST         0.24691585 -0.259162274
HarvestRain -0.02800907  0.041264394
Age          1.00000000 -0.994485097
FrancePop   -0.99448510  1.000000000
#+end_example

From these results we can see a high correlation between *Age* and
*FrancePop* independent variables.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Remove Age and FrancePop")
  model5 <- lm(Price ~ AGST + HarvestRain + WinterRain, data=wine)
  summary(model5)
#+END_SRC

#+RESULTS:
#+begin_example

 :: Remove Age and FrancePop

Call:
lm(formula = Price ~ AGST + HarvestRain + WinterRain, data = wine)

Residuals:
     Min       1Q   Median       3Q      Max
-0.67472 -0.12958  0.01973  0.20751  0.63846

Coefficients:
              Estimate Std. Error t value Pr(>|t|)
(Intercept) -4.3016263  2.0366743  -2.112 0.046831 *
AGST         0.6810242  0.1117011   6.097 4.75e-06 ***
HarvestRain -0.0039481  0.0009987  -3.953 0.000726 ***
WinterRain   0.0011765  0.0005920   1.987 0.060097 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.345 on 21 degrees of freedom
Multiple R-squared:  0.7537,	Adjusted R-squared:  0.7185
F-statistic: 21.42 on 3 and 21 DF,  p-value: 1.359e-06
#+end_example

There is no definitive cut-off value for what makes a correlation too
high. But typically, a correlation greater than 0.7 or less than -0.7
is cause for concern.

If you look back at all of the correlations we computed for our data
set, you can see that it doesn't look like we have any other
highly-correlated independent variables.

** Quick Question (1 point possible)

Using the data set wine.csv, what is the correlation between
HarvestRain and WinterRain?

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Correlations")
  cor(wine$HarvestRain ,wine$WinterRain)
#+END_SRC

#+RESULTS:
:
:  :: Correlations
: [1] -0.2754409

** Video 7: Making Predictions

Our wine model had an R-squared value of 0.83, which tells us how
accurate our model is on the data we used to construct the model.

So we know our model does a good job predicting the data it's seen.
For this particular application, Bordeaux wine buyers profit from
being able to predict the quality of a wine years before it matures.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("    Loading the wine test set into their data frame.")
  wineTest <- read.table("../data/wine_test.csv", sep = ",", header =
          TRUE)
  str(wineTest)
#+END_SRC

#+RESULTS:
:     Loading the wine test set into their data frame.
: 'data.frame':	2 obs. of  7 variables:
:  $ Year       : int  1979 1980
:  $ Price      : num  6.95 6.5
:  $ WinterRain : int  717 578
:  $ AGST       : num  16.2 16
:  $ HarvestRain: int  122 74
:  $ Age        : int  4 3
:  $ FrancePop  : num  54836 55110

Now we can use this data set to make some predictions

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Make test set predictions")
  predictTest <- predict(model4, newdata=wineTest)
  predictTest
#+END_SRC

#+RESULTS:
:
:  :: Make test set predictions
:        1        2
: 6.768925 6.684910

Actually the predictions are very close to the real data in the test
wine data set. Now we can calculate the $R^2$ to know how good is this
prediction.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Compute R-squared")
  SSE = sum((wineTest$Price - predictTest)^2)
  SST = sum((wineTest$Price - mean(wine$Price))^2)
  1 - SSE/SST
#+END_SRC

#+RESULTS:
:
:  :: Compute R-squared
: [1] 0.7944278

This is a pretty good out-of-sample R-squared. But while we do well on
these two test points, keep in mind that our test set is really small.
We should increase the size of our test set to be more confident about
the out-of-sample accuracy of our model.

[[../graphs/outOfSample-Rsq.png]]

The model R-squared will always increase or stay the same as we add
more independent variables.

However, this is not true for the test set. When selecting a model, we
want one with a good model R-squared but also with a good test set
R-squared.

It looks like *our model that uses* *AGST*, *HarvestRain*, *Age*, and
*WinterRain* does very well on the training data and on the test
data.

** Quick Question (1 point possible)

Which of the following are NOT valid values for an out-of-sample (test
set) $R^2$ ? Select all that apply.

*** Answer

*Explanation*

The formula for $R^2$ is

$R^2 = 1 - \frac{SSE}{SST}$,

where $SST$ is calculated using the average value of the dependent
variable on the training set.

Since $SSE$ and $SST$ are the sums of squared terms, we know that both
will be positive. Thus SSE/SST must be greater than or equal to
zero. This means it is not possible to have an out-of-sample $R^2$
value of 2.4.

However, all other values are valid (even the negative ones!), since
SSE can be more or less than SST, due to the fact that this is an
out-of-sample $R^2$, not a model $R^2$.

* Moneyball: The Power of the Sports Analytics

If you are unfamiliar with the game of baseball, please watch this
short video clip for a quick introduction to the game. You don't need
to be a baseball expert to understand this lecture, but basic
knowledge of the game will be helpful to you.

This video is from [[https://www.youtube.com/watch?v%3D0bKkGeROiPA][Baseball Rules Whiteboard Video Rules of Baseball]].

** Download the data

In this lecture, we will be using the dataset [[https://courses.edx.org/asset-v1:MITx%2B15.071x_2a%2B2T2015%2Btype@asset%2Bblock/baseball.csv][baseball.csv]]. Download
this dataset to follow along in R as we build regression models. This
data comes from [[http://www.baseball-reference.com][Baseball-Reference.com]].

A script file containing all of the R commands used in this lecture
can be downloaded [[https://courses.edx.org/asset-v1:MITx%2B15.071x_2a%2B2T2015%2Btype@asset%2Bblock/Unit2_Moneyball.R][here]].

*** Download the data set

In this part we can download the data

#+BEGIN_SRC R :session :results output :exports all
  library(parallel)

  if(!file.exists("../data")) {
          dir.create("../data")
  }

  fileUrl <-
          c("https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/baseball.csv")

  fileName <- c("baseball.csv")

  dataPath <- "../data"

  filePath <- paste(dataPath, fileName, sep = "/")

  if(!file.exists(filePath)) {
          download.file(fileUrl, destfile = filePath, method = "curl")
  }

  list.files("../data")
#+END_SRC

#+RESULTS:
:  [1] "AnonymityPoll.csv"      "BoeingStock.csv"        "CPSData.csv"
:  [4] "CocaColaStock.csv"      "CountryCodes.csv"       "GEStock.csv"
:  [7] "IBMStock.csv"           "MetroAreaCodes.csv"     "NBA_test.csv"
: [10] "NBA_train.csv"          "ProcterGambleStock.csv" "README.md"
: [13] "USDA.csv"               "WHO.csv"                "WHO_Europe.csv"
: [16] "baseball.csv"           "mvtWeek1.csv"           "wine.csv"
: [19] "wine_test.csv"

** The Problem

In 2002, the A's lost three key players. The key question: could they
continue winning without them?

So what is the key problem? Let us discuss the graph on the left of
the screen.

The horizontal axis shows the average payroll during the years 1998
to 2001. The vertical axis shows the average yearly wins over the same
years. So let's look at some of the teams in this graph. So which one
is this team?

This is a team (blue dot)  that won about 100 games and spent roughly
$90 million during this period. So this is the New York Yankees.

[[../graphs/MoneyBallProblem01.png]]

Let's look at this team (Red dot). This team spent about $80 million
and won about 90 games. This is the Red Sox. Where are the Oakland
A's? The A's are here (Green dot). They won about 90 games, and they
spent under $30 million.

If you compare it with the Red Sox, they won about the same number of
games during this period but the Red Sox spent about $50 million more
per year than the A's.

Clearly, rich teams like the Yankees and the Red Sox can afford the
all-star players.

*** The Approach

So the A's started using a different method to select players. The
traditional way of selecting players was through scouting.

Scouts would watch high school and college players, and they would
report back about their skills, especially discussing their speed and
their athletic build. The A's, however, selected players based on
their statistics, not on their looks.

The following are quotes from the book Moneyball. "The statistics
enable you to find your way past all sorts of sight-based scouting
prejudices." And a direct quote from Billy Beane, the manager of the
Oakland A's and the architect of this approach: "We are not selling
jeans here."

[[../graphs/TakingAQuantitativeView.png]]

** Video 2: Making it to the Playoffs

The A's approach was to get to the playoffs by using analytics.

We'll first show how we can predict whether or not a team will make
the playoffs by knowing how many games they won in the regular
season. We'll then use linear regression to predict how many games a
team will win using the difference between runs scored and runs allowed,
or opponent runs.

We'll then use linear regression again to predict the number of runs a
team will score using batting statistics, and the number of runs a
team will allow using fielding and pitching statistics.

We'll start by figuring out how many games a team needs to win to make
the playoffs, and then how many more runs a team needs to score than
their opponent to win that many games.

[[../graphs/TheGoal.png]]

So our first question is:

- How many games does a team need to win in the regular season to make
  it to the playoffs?

In Moneyball, Paul DePodesta reduced the regular season to a math
problem.

He judged that it would take *95* wins for the A's to make it to the
playoffs. Let's see if we can verify this using data. This graph uses
data from all teams and seasons from 1996 to 2001.

[[../graphs/MakingThePlayOffs.png]]

So we know that a team wants to win 95 or more games, but how does a
team win games?

Well, they score more runs than their opponent does. We just need to
figure out how many more. The A's calculated that they needed to score
135 more runs than they allowed during the regular season to expect to
win 95 games.

** Loading the data

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Read in data")
  baseball <- read.table("../data/baseball.csv", sep = ",", header = TRUE)
  str(baseball)
  summary(baseball)
#+END_SRC

#+RESULTS:
#+begin_example

 :: Read in data
'data.frame':	1232 obs. of  15 variables:
 $ Team        : Factor w/ 39 levels "ANA","ARI","ATL",..: 2 3 4 5 7 8 9 10 11 12 ...
 $ League      : Factor w/ 2 levels "AL","NL": 2 2 1 1 2 1 2 1 2 1 ...
 $ Year        : int  2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ...
 $ RS          : int  734 700 712 734 613 748 669 667 758 726 ...
 $ RA          : int  688 600 705 806 759 676 588 845 890 670 ...
 $ W           : int  81 94 93 69 61 85 97 68 64 88 ...
 $ OBP         : num  0.328 0.32 0.311 0.315 0.302 0.318 0.315 0.324 0.33 0.335 ...
 $ SLG         : num  0.418 0.389 0.417 0.415 0.378 0.422 0.411 0.381 0.436 0.422 ...
 $ BA          : num  0.259 0.247 0.247 0.26 0.24 0.255 0.251 0.251 0.274 0.268 ...
 $ Playoffs    : int  0 1 1 0 0 0 1 0 0 1 ...
 $ RankSeason  : int  NA 4 5 NA NA NA 2 NA NA 6 ...
 $ RankPlayoffs: int  NA 5 4 NA NA NA 4 NA NA 2 ...
 $ G           : int  162 162 162 162 162 162 162 162 162 162 ...
 $ OOBP        : num  0.317 0.306 0.315 0.331 0.335 0.319 0.305 0.336 0.357 0.314 ...
 $ OSLG        : num  0.415 0.378 0.403 0.428 0.424 0.405 0.39 0.43 0.47 0.402 ...
      Team     League        Year            RS               RA
 BAL    : 47   AL:616   Min.   :1962   Min.   : 463.0   Min.   : 472.0
 BOS    : 47   NL:616   1st Qu.:1977   1st Qu.: 652.0   1st Qu.: 649.8
 CHC    : 47            Median :1989   Median : 711.0   Median : 709.0
 CHW    : 47            Mean   :1989   Mean   : 715.1   Mean   : 715.1
 CIN    : 47            3rd Qu.:2002   3rd Qu.: 775.0   3rd Qu.: 774.2
 CLE    : 47            Max.   :2012   Max.   :1009.0   Max.   :1103.0
 (Other):950
       W              OBP              SLG               BA
 Min.   : 40.0   Min.   :0.2770   Min.   :0.3010   Min.   :0.2140
 1st Qu.: 73.0   1st Qu.:0.3170   1st Qu.:0.3750   1st Qu.:0.2510
 Median : 81.0   Median :0.3260   Median :0.3960   Median :0.2600
 Mean   : 80.9   Mean   :0.3263   Mean   :0.3973   Mean   :0.2593
 3rd Qu.: 89.0   3rd Qu.:0.3370   3rd Qu.:0.4210   3rd Qu.:0.2680
 Max.   :116.0   Max.   :0.3730   Max.   :0.4910   Max.   :0.2940

    Playoffs        RankSeason     RankPlayoffs         G
 Min.   :0.0000   Min.   :1.000   Min.   :1.000   Min.   :158.0
 1st Qu.:0.0000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:162.0
 Median :0.0000   Median :3.000   Median :3.000   Median :162.0
 Mean   :0.1981   Mean   :3.123   Mean   :2.717   Mean   :161.9
 3rd Qu.:0.0000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:162.0
 Max.   :1.0000   Max.   :8.000   Max.   :5.000   Max.   :165.0
                  NA's   :988     NA's   :988
      OOBP             OSLG
 Min.   :0.2940   Min.   :0.3460
 1st Qu.:0.3210   1st Qu.:0.4010
 Median :0.3310   Median :0.4190
 Mean   :0.3323   Mean   :0.4197
 3rd Qu.:0.3430   3rd Qu.:0.4380
 Max.   :0.3840   Max.   :0.4990
 NA's   :812      NA's   :812
#+end_example

This data set includes an observation for every team and year pair
from 1962 to 2012 for all seasons with 162 games. We have 15 variables
in our data set, including Runs Scored, RS, Runs Allowed, RA, and
Wins, W. We also have several other variables that we'll use when
building models later on in the lecture.

Since we're confirming the claims made in Moneyball, we want to build
models using data Paul DePodesta had in 2002, so let's start by
subsetting our data to only include the years before 2002.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Subset to only include moneyball years")
  moneyball <- subset(baseball, Year < 2002)
  str(moneyball)
  summary(moneyball)
#+END_SRC

#+RESULTS:
#+begin_example

 :: Subset to only include moneyball years
'data.frame':	902 obs. of  15 variables:
 $ Team        : Factor w/ 39 levels "ANA","ARI","ATL",..: 1 2 3 4 5 7 8 9 10 11 ...
 $ League      : Factor w/ 2 levels "AL","NL": 1 2 2 1 1 2 1 2 1 2 ...
 $ Year        : int  2001 2001 2001 2001 2001 2001 2001 2001 2001 2001 ...
 $ RS          : int  691 818 729 687 772 777 798 735 897 923 ...
 $ RA          : int  730 677 643 829 745 701 795 850 821 906 ...
 $ W           : int  75 92 88 63 82 88 83 66 91 73 ...
 $ OBP         : num  0.327 0.341 0.324 0.319 0.334 0.336 0.334 0.324 0.35 0.354 ...
 $ SLG         : num  0.405 0.442 0.412 0.38 0.439 0.43 0.451 0.419 0.458 0.483 ...
 $ BA          : num  0.261 0.267 0.26 0.248 0.266 0.261 0.268 0.262 0.278 0.292 ...
 $ Playoffs    : int  0 1 1 0 0 0 0 0 1 0 ...
 $ RankSeason  : int  NA 5 7 NA NA NA NA NA 6 NA ...
 $ RankPlayoffs: int  NA 1 3 NA NA NA NA NA 4 NA ...
 $ G           : int  162 162 162 162 161 162 162 162 162 162 ...
 $ OOBP        : num  0.331 0.311 0.314 0.337 0.329 0.321 0.334 0.341 0.341 0.35 ...
 $ OSLG        : num  0.412 0.404 0.384 0.439 0.393 0.398 0.427 0.455 0.417 0.48 ...
      Team     League        Year            RS               RA
 BAL    : 36   AL:462   Min.   :1962   Min.   : 463.0   Min.   : 472.0
 BOS    : 36   NL:440   1st Qu.:1973   1st Qu.: 641.2   1st Qu.: 640.0
 CHC    : 36            Median :1983   Median : 695.0   Median : 697.0
 CHW    : 36            Mean   :1982   Mean   : 703.8   Mean   : 703.8
 CIN    : 36            3rd Qu.:1992   3rd Qu.: 761.8   3rd Qu.: 763.0
 CLE    : 36            Max.   :2001   Max.   :1009.0   Max.   :1103.0
 (Other):686
       W               OBP             SLG               BA
 Min.   : 40.00   Min.   :0.277   Min.   :0.3010   Min.   :0.2140
 1st Qu.: 73.00   1st Qu.:0.314   1st Qu.:0.3680   1st Qu.:0.2500
 Median : 81.00   Median :0.324   Median :0.3880   Median :0.2580
 Mean   : 80.88   Mean   :0.325   Mean   :0.3904   Mean   :0.2582
 3rd Qu.: 89.00   3rd Qu.:0.335   3rd Qu.:0.4118   3rd Qu.:0.2670
 Max.   :116.00   Max.   :0.373   Max.   :0.4850   Max.   :0.2940

    Playoffs        RankSeason     RankPlayoffs         G
 Min.   :0.0000   Min.   :1.000   Min.   :1.000   Min.   :158.0
 1st Qu.:0.0000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:162.0
 Median :0.0000   Median :2.500   Median :3.000   Median :162.0
 Mean   :0.1707   Mean   :2.792   Mean   :2.454   Mean   :161.9
 3rd Qu.:0.0000   3rd Qu.:4.000   3rd Qu.:3.000   3rd Qu.:162.0
 Max.   :1.0000   Max.   :8.000   Max.   :4.000   Max.   :165.0
                  NA's   :748     NA's   :748
      OOBP             OSLG
 Min.   :0.3010   Min.   :0.3770
 1st Qu.:0.3290   1st Qu.:0.4160
 Median :0.3420   Median :0.4325
 Mean   :0.3405   Mean   :0.4325
 3rd Qu.:0.3500   3rd Qu.:0.4508
 Max.   :0.3840   Max.   :0.4990
 NA's   :812      NA's   :812
#+end_example

So we want to build a linear regression equation to predict wins using
the difference between runs scored and runs allowed.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Compute Run Difference")
  moneyball$RD <- moneyball$RS - moneyball$RA
  str(moneyball)
#+END_SRC

#+RESULTS:
#+begin_example

 :: Compute Run Difference
'data.frame':	902 obs. of  16 variables:
 $ Team        : Factor w/ 39 levels "ANA","ARI","ATL",..: 1 2 3 4 5 7 8 9 10 11 ...
 $ League      : Factor w/ 2 levels "AL","NL": 1 2 2 1 1 2 1 2 1 2 ...
 $ Year        : int  2001 2001 2001 2001 2001 2001 2001 2001 2001 2001 ...
 $ RS          : int  691 818 729 687 772 777 798 735 897 923 ...
 $ RA          : int  730 677 643 829 745 701 795 850 821 906 ...
 $ W           : int  75 92 88 63 82 88 83 66 91 73 ...
 $ OBP         : num  0.327 0.341 0.324 0.319 0.334 0.336 0.334 0.324 0.35 0.354 ...
 $ SLG         : num  0.405 0.442 0.412 0.38 0.439 0.43 0.451 0.419 0.458 0.483 ...
 $ BA          : num  0.261 0.267 0.26 0.248 0.266 0.261 0.268 0.262 0.278 0.292 ...
 $ Playoffs    : int  0 1 1 0 0 0 0 0 1 0 ...
 $ RankSeason  : int  NA 5 7 NA NA NA NA NA 6 NA ...
 $ RankPlayoffs: int  NA 1 3 NA NA NA NA NA 4 NA ...
 $ G           : int  162 162 162 162 161 162 162 162 162 162 ...
 $ OOBP        : num  0.331 0.311 0.314 0.337 0.329 0.321 0.334 0.341 0.341 0.35 ...
 $ OSLG        : num  0.412 0.404 0.384 0.439 0.393 0.398 0.427 0.455 0.417 0.48 ...
 $ RD          : int  -39 141 86 -142 27 76 3 -115 76 17 ...
#+end_example

Now, before we build the linear regression equation, let's visually
check to see if there's a linear relationship between Run Difference
and Wins.

#+BEGIN_SRC R :var basename="exploratoryMoneyBall" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  plot(moneyball$RD, moneyball$W)
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION: Scatterplot to check for linear relationship
#+NAME:   fig:exploratoryMoneyBall
#+ATTR_LaTeX: placement: [H]
[[../graphs/exploratoryMoneyBall.png]]

** Linear Regression Model

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Regression model to predict wins")
  WinsReg <- lm(W ~ RD, data = moneyball)
  summary(WinsReg)
#+END_SRC

#+RESULTS:
#+begin_example
null device
          1

 :: Regression model to predict wins

Call:
lm(formula = W ~ RD, data = moneyball)

Residuals:
     Min       1Q   Median       3Q      Max
-14.2662  -2.6509   0.1234   2.9364  11.6570

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept) 80.881375   0.131157  616.67   <2e-16 ***
RD           0.105766   0.001297   81.55   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.939 on 900 degrees of freedom
Multiple R-squared:  0.8808,	Adjusted R-squared:  0.8807
F-statistic:  6651 on 1 and 900 DF,  p-value: < 2.2e-16
#+end_example

We can take a look at the summary of our regression equation using the
summary function, which shows us that RD is very significant with
three stars, and the R-squared of our model is 0.88.

So we have a strong model to predict wins using the difference between
runs scored and runs allowed. Now, let's see if we can use this model
to confirm the claim made in Moneyball that a team needs to score at
least 135 more runs than they allow to win at least *95 games*.

[[../graphs/AppModel.png]]

So this tells us that if the run difference of a team is greater than
or equal to $133.4$, then we predict that the team will win at least
$95$ games. This is very close to the claim made in Moneyball that a
team needs to score at least $135$ more runs than they allow to win at
least $95$ games.

** Quick Question (1 point possible)

If a baseball team scores 713 runs and allows 614 runs, how many games
do we expect the team to win?

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Using the predict function")
  RD <- (713 - 614)
  TestData01 <- data.frame(RD)
  predict(WinsReg, TestData01)

  writeLines("\n :: Using the model directly")
  W <- 80.881375 + (0.105766 * TestData01$RD)
  W
#+END_SRC

#+RESULTS:
:
:  :: Using the predict function
:        1
: 91.35217
:
:  :: Using the model directly
: [1] 91.35221

Using the linear regression model constructed during the lecture,
enter the number of games we expect the team to win:

*** Answer

Our linear regression model was

$Wins = 80.88 + 0.1058 * (Run~Difference)$

Here, the run difference is $99$, so our prediction is

$Wins = 80.88 + 0.1058*99 = 91~games$.

** Video 3: Predicting Runs

The Oakland A's were interested in answering the question: how does a
team score more runs?

They discovered that two particular baseball statistics were very
predictive of runs scored:

- on-base percentage, or OBP, and
- slugging percentage, or SLG.

*On-base* percentage is the percentage of time a player gets on base,
including walks. *Slugging* percentage measures how far a player gets
around the bases on his turn, and measures the power of a hitter.

[[../graphs/ScoringRuns.png]]

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Structure of the DF")
  str(moneyball)

  writeLines("\n :: Regression model to predict runs scored")
  RunsReg <- lm(RS ~ OBP + SLG + BA, data=moneyball)
  summary(RunsReg)
#+END_SRC

#+RESULTS:
#+begin_example

 :: Structure of the DF
'data.frame':	902 obs. of  16 variables:
 $ Team        : Factor w/ 39 levels "ANA","ARI","ATL",..: 1 2 3 4 5 7 8 9 10 11 ...
 $ League      : Factor w/ 2 levels "AL","NL": 1 2 2 1 1 2 1 2 1 2 ...
 $ Year        : int  2001 2001 2001 2001 2001 2001 2001 2001 2001 2001 ...
 $ RS          : int  691 818 729 687 772 777 798 735 897 923 ...
 $ RA          : int  730 677 643 829 745 701 795 850 821 906 ...
 $ W           : int  75 92 88 63 82 88 83 66 91 73 ...
 $ OBP         : num  0.327 0.341 0.324 0.319 0.334 0.336 0.334 0.324 0.35 0.354 ...
 $ SLG         : num  0.405 0.442 0.412 0.38 0.439 0.43 0.451 0.419 0.458 0.483 ...
 $ BA          : num  0.261 0.267 0.26 0.248 0.266 0.261 0.268 0.262 0.278 0.292 ...
 $ Playoffs    : int  0 1 1 0 0 0 0 0 1 0 ...
 $ RankSeason  : int  NA 5 7 NA NA NA NA NA 6 NA ...
 $ RankPlayoffs: int  NA 1 3 NA NA NA NA NA 4 NA ...
 $ G           : int  162 162 162 162 161 162 162 162 162 162 ...
 $ OOBP        : num  0.331 0.311 0.314 0.337 0.329 0.321 0.334 0.341 0.341 0.35 ...
 $ OSLG        : num  0.412 0.404 0.384 0.439 0.393 0.398 0.427 0.455 0.417 0.48 ...
 $ RD          : int  -39 141 86 -142 27 76 3 -115 76 17 ...

 :: Regression model to predict runs scored

Call:
lm(formula = RS ~ OBP + SLG + BA, data = moneyball)

Residuals:
    Min      1Q  Median      3Q     Max
-70.941 -17.247  -0.621  16.754  90.998

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  -788.46      19.70 -40.029  < 2e-16 ***
OBP          2917.42     110.47  26.410  < 2e-16 ***
SLG          1637.93      45.99  35.612  < 2e-16 ***
BA           -368.97     130.58  -2.826  0.00482 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 24.69 on 898 degrees of freedom
Multiple R-squared:  0.9302,	Adjusted R-squared:   0.93
F-statistic:  3989 on 3 and 898 DF,  p-value: < 2.2e-16
#+end_example

All the variables (OBP + SLG + BA) are significant, and the $R^2 = 0.9302$.

But if we look at our coefficients, we can see that the coefficient
for batting average is *negative*. This implies that, all else being
equal, a team with a lower batting average will score more runs, which
is a little counterintuitive.

What's going on here is a case of multicollinearity. These three
hitting statistics are highly correlated.

** Refining the model

Let's try removing batting average, the variable with the least
significance, to see what happens to our model.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Regression model to predict runs scored")
  RunsReg <- lm(RS ~ OBP + SLG, data = moneyball)
  summary(RunsReg)
#+END_SRC

#+RESULTS:
#+begin_example

 :: Regression model to predict runs scored

Call:
lm(formula = RS ~ OBP + SLG, data = moneyball)

Residuals:
    Min      1Q  Median      3Q     Max
-70.838 -17.174  -1.108  16.770  90.036

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  -804.63      18.92  -42.53   <2e-16 ***
OBP          2737.77      90.68   30.19   <2e-16 ***
SLG          1584.91      42.16   37.60   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 24.79 on 899 degrees of freedom
Multiple R-squared:  0.9296,	Adjusted R-squared:  0.9294
F-statistic:  5934 on 2 and 899 DF,  p-value: < 2.2e-16
#+end_example

So this model is simpler, with only two independent variables, and has
about the same R-squared.

Overall a better model. You could experiment and see that if we'd
removed on-base percentage or slugging percentage instead of batting
average, our R-squared would have decreased.

If we look at the coefficients of our model, we can see that on-base
percentage has a larger coefficient than slugging percentage. Since
these variables are on about the same scale, this tells us that
on-base percentage is probably worth more than slugging percentage.

So by using linear regression, we're able to verify the claims made in
Moneyball: that batting average is overvalued, on-base percentage is
the most important, and slugging percentage is important for
predicting runs scored.

** Allowing Runs Model

We can create a very similar model to predict runs allowed, or
opponent runs. This model uses pitching statistics: opponents on-base
percentage, or OOBP, and opponents slugging percentage, or OSLG.

These statistics are computed in the same way as on-base percentage
and slugging percentage, but they use the actions of the opposing
batters against our team's pitcher and fielders.

[[../graphs/AllowingRuns.png]]

The key message here is that simple models, using only a couple
independent variables, can be constructed to answer some of the most
important questions in baseball.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Regression model to predict runs allowed")
  RunsAllowed <- lm(RA ~ OOBP + OSLG, data = moneyball)
  summary(RunsAllowed)
#+END_SRC

#+RESULTS:
#+begin_example

 :: Regression model to predict runs allowed

Call:
lm(formula = RA ~ OOBP + OSLG, data = moneyball)

Residuals:
    Min      1Q  Median      3Q     Max
-82.397 -15.178  -0.129  17.679  60.955

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  -837.38      60.26 -13.897  < 2e-16 ***
OOBP         2913.60     291.97   9.979 4.46e-16 ***
OSLG         1514.29     175.43   8.632 2.55e-13 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 25.67 on 87 degrees of freedom
  (812 observations deleted due to missingness)
Multiple R-squared:  0.9073,	Adjusted R-squared:  0.9052
F-statistic: 425.8 on 2 and 87 DF,  p-value: < 2.2e-16
#+end_example

** Quick Question (2 points possible)

If a baseball team's OBP is 0.311 and SLG is 0.405, how many runs do
we expect the team to score?

#+BEGIN_SRC R :session :results output :exports all
  OBP <- 0.311; SLG <- 0.405
  RSTest <- data.frame(OBP, SLG)
  predict(RunsReg, newdata = RSTest)
#+END_SRC

#+RESULTS:
:        1
: 688.7068

*** Question a

Using the linear regression model constructed during the lecture (the
one that uses OBP and SLG as independent variables), enter the number
of runs we expect the team to score:

**** Answer

688.7068

*Explanation*

Our linear regression model was:

Runs Scored = -804.63 + 2737.77*(OBP) + 1584.91*(SLG)

Here, OBP is 0.311 and SLG is 0.405, so our prediction is:

Runs Scored = -804.63 + 2737.77*0.311 + 1584.91*0.405 = 689 runs

*** Question b

If a baseball team's opponents OBP (OOBP) is 0.297 and oppenents SLG
(OSLG) is 0.370, how many runs do we expect the team to allow?

#+BEGIN_SRC R :session :results output :exports all
  OOBP <- 0.297; OSLG <- 0.370
  RATest <- data.frame(OOBP, OSLG)
  predict(RunsAllowed, newdata = RATest)
#+END_SRC

#+RESULTS:
:       1
: 588.247

Using the linear regression model discussed during the lecture (the
one on the last slide of the previous video), enter the number of runs
we expect the team to allow:

**** Answer

588.247

*Explanation*

Our linear regression model was:

Runs Allowed = -837.38 + 2913.60*(OOBP) + 1514.29*(OSLG)

Here, OOBP is 0.297 and OSLG is 0.370, so our prediction is:

Runs Scored = -837.38 + 2913.60*(.297) + 1514.29*(.370) = 588 runs

** Video 4: Using the Models to Make Predictions

Using our regression models, we would like to predict before the
season starts how many games the 2002 Oakland A's will win. To do
this, we first have to predict how many runs the team will score and
how many runs they will allow.

These models use team statistics. However, when we are predicting for
the 2002 Oakland A's before the season has occurred, the team is
probably different than it was the year before. So we don't know the
team statistics.

But we can estimate these statistics using past player
performance. This approach assumes that past performance correlates
with future performance and that there will be few injuries during the
season.

Using this approach, we can estimate the team statistics for 2002 by
using the 2001 player statistics. Let's start by making a prediction
for runs scored.

[[../graphs/PredictingRunsScored.png]]


#+BEGIN_SRC R :session :results output :exports all
  OBP <- 0.339; SLG <- 0.430
  RS2002 <- data.frame(OBP, SLG)
  predict(RunsReg, newdata = RS2002)
#+END_SRC

#+RESULTS:
:       1
: 804.987

** Predicting Runs Allowed

[[../graphs/PredictingRunsAllowed.png]]

#+BEGIN_SRC R :session :results output :exports all
  OOBP <- 0.307; OSLG <- 0.373
  RA2002 <- data.frame(OOBP, OSLG)
  predict(RunsAllowed, newdata = RA2002)
#+END_SRC

#+RESULTS:
:        1
: 621.9258

** Quick Question (1 point possible)

Suppose you are the General Manager of a baseball team, and you are
selecting TWO players for your team. You have a budget of $1,500,000,
and you have the choice between the following players:

| Player Name     |   OBP |   SLG |  Salary |
|-----------------+-------+-------+---------|
| Eric Chavez     | 0.338 | 0.540 | 1400000 |
| Jeremy Giambi   | 0.391 | 0.450 | 1065000 |
| Frank Menechino | 0.369 | 0.374 |  295000 |
| Greg Myers      | 0.313 | 0.447 |  800000 |
| Carlos Pena     | 0.361 | 0.500 |  300000 |

Given your budget and the player statistics, which TWO players would
you select?

#+BEGIN_SRC R :session :results output :exports all
  OBP <- c(0.338, 0.391, 0.369, 0.313, 0.361)
  SLG <- c(0.540, 0.450, 0.374, 0.447, 0.500)
  PlayerName <- c("Eric Chavez", "Jeremy Giambi", "Frank Menechino",
                  "Greg Myers", "Carlos Pena")
  Salary <- c(1400000, 1065000, 295000, 800000, 300000)

  RSnewPlayers <- data.frame(PlayerName, OBP, SLG, Salary)
  RSnewPlayers$RSPred <- predict(RunsReg, newdata = RSnewPlayers)
  RSnewPlayers

  writeLines("\n :: For the best players with the budget restriction:")
  RSnewPlayers$Salary[2] + RSnewPlayers$Salary[5]
#+END_SRC

#+RESULTS:
:        PlayerName   OBP   SLG  Salary   RSPred
: 1     Eric Chavez 0.338 0.540 1400000 976.5892
: 2   Jeremy Giambi 0.391 0.450 1065000 979.0491
: 3 Frank Menechino 0.369 0.374  295000 798.3652
: 4      Greg Myers 0.313 0.447  800000 760.7485
: 5     Carlos Pena 0.361 0.500  300000 976.1615
:
:  :: For the best players with the budget restriction:
: [1] 1365000

*** Answer

*Explanation*

We would select Jeremy Giambi and Carlos Pena, since they give the
highest contribution to Runs Scored.

We would not select Eric Chavez, since his salary consumes our entire
budget, and although he has the highest SLG, there are players with
better OBP.

We would not select Frank Menechino since even though he has a high
OBP, his SLG is low.

We would not select Greg Myers since he is dominated by Carlos Pena in
OBP and SLG, but has a much higher salary.

** Video 5: Winning the World Series

We stated that the goal of a baseball team is to make the playoffs and
we built predictive models to achieve this goal. But why isn't the
goal of a baseball team to win the playoffs or win the World Series?

Billy Beane and Paul Depodesta see their job as making sure the team
makes it to the playoffs, and after that, all bets are off. The A's
made it to the playoffs four years in a row-- 2000, 2001, 2002, and
2003-- but they didn't win the World Series.

*Why not?*

In Moneyball, they say that "over a long season luck evens out, and
skill shines through.

In Moneyball, they say that "over a long season luck evens out, and
skill shines through. But in a series of three out of five, or even
four out of seven, anything can happen."

In other words, the playoffs suffer from the sample size problem.

There are not enough games to make any statistical claims. Let's see
if we can verify this using our data set.

The number of teams in the playoffs has changed over the years. So
let's only use the years with eight teams in the playoffs, which was
the number of teams in the playoffs in 2002, the year Moneyball
discusses. We can compute the correlation between whether or not the
team wins the World Series-- a binary variable-- and the number of
regular season wins, since we would expect teams with more wins to be
more likely to win the World Series.

This *correlation* is $0.03$, which is *very low*. So it turns out that
winning regular season games gets you to the playoffs, but in the
playoffs, there too few games for luck to even out.

[[../graphs/IsPlayoffPerformancePredictable.png]]

** Quick Question (2 points possible)

In 2012 and 2013, there were 10 teams in the MLB playoffs: the six
teams that had the most wins in each baseball division, and four "wild
card" teams. The playoffs start between the four wild card teams - the
two teams that win proceed in the playoffs (8 teams remaining). Then,
these teams are paired off and play a series of games. The four teams
that win are then paired and play to determine who will play in the
World Series.

We can assign rankings to the teams as follows:

- Rank 1: the team that won the World Series
- Rank 2: the team that lost the World Series
- Rank 3: the two teams that lost to the teams in the World Series
- Rank 4: the four teams that made it past the wild card round, but
  lost to the above four teams
- Rank 5: the two teams that lost the wild card round

In your R console, create a corresponding rank vector by typing

#+BEGIN_SRC R :session :results output :exports all
  teamRank = c(1, 2, 3, 3, 4, 4, 4, 4, 5, 5)
#+END_SRC

#+RESULTS:

In this quick question, we'll see how well these rankings correlate
with the regular season wins of the teams. In 2012, the ranking of the
teams and their regular season wins were as follows:

- Rank 1: San Francisco Giants (Wins = 94)
- Rank 2: Detroit Tigers (Wins = 88)
- Rank 3: New York Yankees (Wins = 95), and St. Louis Cardinals (Wins = 88)
- Rank 4: Baltimore Orioles (Wins = 93), Oakland A's (Wins = 94),
  Washington Nationals (Wins = 98), Cincinnati Reds (Wins = 97)
- Rank 5: Texas Rangers (Wins = 93), and Atlanta Braves (Wins = 94)

Create a vector in R called wins2012, that has the wins of each team
in 2012, in order of rank (the vector should have 10 numbers).

#+BEGIN_SRC R :session :results output :exports all
  wins2012 <- c(94, 88, 95, 88, 93, 94, 98, 97, 93, 94)
#+END_SRC

#+RESULTS:

In 2013, the ranking of the teams and their regular season wins were as follows:

- Rank 1: Boston Red Sox (Wins = 97)
- Rank 2: St. Louis Cardinals (Wins = 97)
- Rank 3: Los Angeles Dodgers (Wins = 92), and Detroit Tigers (Wins = 93)
- Rank 4: Tampa Bay Rays (Wins = 92), Oakland A's (Wins = 96),
  Pittsburgh Pirates (Wins = 94), and Atlanta Braves (Wins = 96)
- Rank 5: Cleveland Indians (Wins = 92), and Cincinnati Reds (Wins = 90)

Create another vector in R called wins2013, that has the wins of each
team in 2013, in order of rank (the vector should have 10 numbers).

#+BEGIN_SRC R :session :results output :exports all
  wins2013 <- c(97, 97, 92, 93, 92, 96, 94, 96, 92, 90)
#+END_SRC

#+RESULTS:

*** Question a

What is the correlation between teamRank and wins2012?

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Correlation between teamRank and wins2012")
  cor(teamRank, wins2012)
#+END_SRC

#+RESULTS:
:
:  :: Correlation between teamRank and wins2012
: [1] 0.3477129

*** Question b

What is the correlation between teamRank and wins2013?

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Correlation between teamRank and wins2013")
  cor(teamRank, wins2013)
#+END_SRC

#+RESULTS:
:
:  :: Correlation between teamRank and wins2013
: [1] -0.6556945

*** Answer

The output of the last line is 0.3477129, which is the correlation
between teamRank and wins2012.

The output of the last line is -0.6556945, which is the correlation
between teamRank and wins2013.

Since one of the correlations is positive and the other is negative,
this means that there does not seem to be a pattern between regular
season wins and winning the playoffs. We wouldn't feel comfortable
making a bet for this year given this data!

** Quick Question (1 point possible)

Which of the following is MOST LIKELY to be a topic of Sabermetric
research?

- Evaluating how the attitude of managers influences player
  performance
- Determining the correlation between scouting predictions and player
  performance
- Predicting how many home runs the Oakland A's will hit next year

While Moneyball made the use of analytics in sports very popular,
baseball is not the only sport for which analytics is used. Analytics
is currently used in almost every single sport, including basketball,
soccer, cricket, and hockey.

Basketball: The study of analytics in basketball, called APBRmetrics,
is very popular. There have been many books written in this area,
including "Pro Basketball Forecast" by John Hollinger and "Basketball
on Paper" by Dean Oliver. There are also several websites dedicated to
the study of basketball analytics, including [[http://www.82games.com][82games.com]]. We'll talk
more about basketball during recitation.

Soccer: The soccer analytics community is currently growing, and new
data is constantly being collected. Many argue that it is much harder
to apply analytics to soccer, but there are several books and websites
on the topic. Check out "The Numbers Game: Why Everything You Know
about Football is Wrong" by Chris Anderson and David Sally, as well as
the websites [[http://www.socceranalysts.com][socceranalysts.com]] and [[http://www.soccermetrics.net/][soccermetrics.net]].

Cricket: There are several websites dedicated to building models for
evaluating player performance in cricket. Check out [[http://www.cricmetric.com][cricmetric.com]] and
[[http://www.impactindexcricket.com][impactindexcricket.com]].

Hockey: Analytics are used in hockey to track player performance and
to better shape the composition of teams. Check out the websites
[[http://www.hockeyanalytics.com][hockeyanalytics.com]] and [[http://www.lighthousehockey.com][lighthousehockey.com]].

*** Answer

*Explanation*

*Sabermetric* research tries to take a quantitative approach to
baseball. Predicting how many home runs the Oakland A's will hit next
year is a very quantitative problem. While the other two topics could
be an area of Sabermetric research, they are more qualitative.

* Recitation: Playing Moneyball in the NBA

Now, we'll apply the same method, linear regression, to data from the
National Basketball Association, the NBA.

** Video 1: The Data

In this recitation, we'll apply some of the ideas from Moneyball to
data from the National Basketball Association (NBA). Please download
the datasets [[https://courses.edx.org/asset-v1:MITx%2B15.071x_2a%2B2T2015%2Btype@asset%2Bblock/NBA_train.csv][NBA_train.csv]] and [[https://courses.edx.org/asset-v1:MITx%2B15.071x_2a%2B2T2015%2Btype@asset%2Bblock/NBA_test.csv][NBA_test.csv]], and save them to a
location on your computer that you will remember. This data comes from
[[http://www.basketball-reference.com][Basketball-Reference.com]].

A script file containing all of the R commands used in this recitation
can be downloaded [[https://courses.edx.org/asset-v1:MITx%2B15.071x_2a%2B2T2015%2Btype@asset%2Bblock/Unit2_Recitation.R][here]].

*** Download the data set

In this part we can download the data

#+BEGIN_SRC R :session :results output :exports all
  library(parallel)

  if(!file.exists("../data")) {
          dir.create("../data")
  }

  fileUrl <-
          c("https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/NBA_train.csv", "https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/NBA_test.csv")

  fileName <- c("NBA_train.csv", "NBA_test.csv")

  dataPath <- "../data"

  for(i in 1:2) {
          filePath <- paste(dataPath, fileName[i], sep = "/")

          if(!file.exists(filePath)) {
                  download.file(fileUrl[i], destfile = filePath, method = "curl")
          }
  }
  list.files("../data")
#+END_SRC

#+RESULTS:
:  [1] "AnonymityPoll.csv"      "BoeingStock.csv"        "CPSData.csv"
:  [4] "CocaColaStock.csv"      "CountryCodes.csv"       "GEStock.csv"
:  [7] "IBMStock.csv"           "MetroAreaCodes.csv"     "NBA_test.csv"
: [10] "NBA_train.csv"          "ProcterGambleStock.csv" "README.md"
: [13] "USDA.csv"               "WHO.csv"                "WHO_Europe.csv"
: [16] "baseball.csv"           "mvtWeek1.csv"           "wine.csv"
: [19] "wine_test.csv"

** Loading the data

In this recitation we will apply some of the ideas from Moneyball to
data from the National Basketball Association-- that is, the *NBA*.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Read the data")
  NBA <- read.table("../data/NBA_train.csv", sep = ",", header = TRUE)
  str(NBA)
  summary(NBA)
#+END_SRC

#+RESULTS:
#+begin_example

 :: Read the data
'data.frame':	835 obs. of  20 variables:
 $ SeasonEnd: int  1980 1980 1980 1980 1980 1980 1980 1980 1980 1980 ...
 $ Team     : Factor w/ 37 levels "Atlanta Hawks",..: 1 2 5 6 8 9 10 11 12 13 ...
 $ Playoffs : int  1 1 0 0 0 0 0 1 0 1 ...
 $ W        : int  50 61 30 37 30 16 24 41 37 47 ...
 $ PTS      : int  8573 9303 8813 9360 8878 8933 8493 9084 9119 8860 ...
 $ oppPTS   : int  8334 8664 9035 9332 9240 9609 8853 9070 9176 8603 ...
 $ FG       : int  3261 3617 3362 3811 3462 3643 3527 3599 3639 3582 ...
 $ FGA      : int  7027 7387 6943 8041 7470 7596 7318 7496 7689 7489 ...
 $ X2P      : int  3248 3455 3292 3775 3379 3586 3500 3495 3551 3557 ...
 $ X2PA     : int  6952 6965 6668 7854 7215 7377 7197 7117 7375 7375 ...
 $ X3P      : int  13 162 70 36 83 57 27 104 88 25 ...
 $ X3PA     : int  75 422 275 187 255 219 121 379 314 114 ...
 $ FT       : int  2038 1907 2019 1702 1871 1590 1412 1782 1753 1671 ...
 $ FTA      : int  2645 2449 2592 2205 2539 2149 1914 2326 2333 2250 ...
 $ ORB      : int  1369 1227 1115 1307 1311 1226 1155 1394 1398 1187 ...
 $ DRB      : int  2406 2457 2465 2381 2524 2415 2437 2217 2326 2429 ...
 $ AST      : int  1913 2198 2152 2108 2079 1950 2028 2149 2148 2123 ...
 $ STL      : int  782 809 704 764 746 783 779 782 900 863 ...
 $ BLK      : int  539 308 392 342 404 562 339 373 530 356 ...
 $ TOV      : int  1495 1539 1684 1370 1533 1742 1492 1565 1517 1439 ...
   SeasonEnd                     Team        Playoffs            W
 Min.   :1980   Atlanta Hawks      : 31   Min.   :0.0000   Min.   :11.0
 1st Qu.:1989   Boston Celtics     : 31   1st Qu.:0.0000   1st Qu.:31.0
 Median :1996   Chicago Bulls      : 31   Median :1.0000   Median :42.0
 Mean   :1996   Cleveland Cavaliers: 31   Mean   :0.5749   Mean   :41.0
 3rd Qu.:2005   Denver Nuggets     : 31   3rd Qu.:1.0000   3rd Qu.:50.5
 Max.   :2011   Detroit Pistons    : 31   Max.   :1.0000   Max.   :72.0
                (Other)            :649
      PTS            oppPTS            FG            FGA            X2P
 Min.   : 6901   Min.   : 6909   Min.   :2565   Min.   :5972   Min.   :1981
 1st Qu.: 7934   1st Qu.: 7934   1st Qu.:2974   1st Qu.:6564   1st Qu.:2510
 Median : 8312   Median : 8365   Median :3150   Median :6831   Median :2718
 Mean   : 8370   Mean   : 8370   Mean   :3200   Mean   :6873   Mean   :2881
 3rd Qu.: 8784   3rd Qu.: 8768   3rd Qu.:3434   3rd Qu.:7157   3rd Qu.:3296
 Max.   :10371   Max.   :10723   Max.   :3980   Max.   :8868   Max.   :3954

      X2PA           X3P             X3PA              FT            FTA
 Min.   :4153   Min.   : 10.0   Min.   :  75.0   Min.   :1189   Min.   :1475
 1st Qu.:5269   1st Qu.:131.5   1st Qu.: 413.0   1st Qu.:1502   1st Qu.:2008
 Median :5706   Median :329.0   Median : 942.0   Median :1628   Median :2176
 Mean   :5956   Mean   :319.0   Mean   : 916.9   Mean   :1650   Mean   :2190
 3rd Qu.:6754   3rd Qu.:481.5   3rd Qu.:1347.5   3rd Qu.:1781   3rd Qu.:2352
 Max.   :7873   Max.   :841.0   Max.   :2284.0   Max.   :2388   Max.   :3051

      ORB              DRB            AST            STL
 Min.   : 639.0   Min.   :2044   Min.   :1423   Min.   : 455.0
 1st Qu.: 953.5   1st Qu.:2346   1st Qu.:1735   1st Qu.: 599.0
 Median :1055.0   Median :2433   Median :1899   Median : 658.0
 Mean   :1061.6   Mean   :2427   Mean   :1912   Mean   : 668.4
 3rd Qu.:1167.0   3rd Qu.:2516   3rd Qu.:2078   3rd Qu.: 729.0
 Max.   :1520.0   Max.   :2753   Max.   :2575   Max.   :1053.0

      BLK             TOV
 Min.   :204.0   Min.   : 931
 1st Qu.:359.0   1st Qu.:1192
 Median :410.0   Median :1289
 Mean   :419.8   Mean   :1303
 3rd Qu.:469.5   3rd Qu.:1396
 Max.   :716.0   Max.   :1873
#+end_example

The data we have is located in the file NBA_train.csv and contains
data from all teams in season since 1980, except for ones with less
than 82 games. So I'll read this in to the variable NBA.

*** Exploratory Analysis

- We have 835 observations of 20 variables.
- SeasonEnd is the year the season ended.
- Team is the name of the team.
- And playoffs is a binary variable for whether or not a team made it
  to the playoffs that year. If they made it to the playoffs it's a 1,
  if not it's a 0.
- W stands for the number of regular season wins.
- PTS stands for points scored during the regular season.
- oppPTS stands for opponent points scored during the regular season.

And then we've got quite a few variables that have the variable name
and then the same variable with an 'A' afterwards. So we've got FG and
FGA, X2P, X2PA, X3P, X3PA, FT, and FTA. So what this notation is, is
it means if there is an 'A' it means the number that were attempted.

And if not it means the number that were successful. So for example FG
is the number of successful field goals, including two and three
pointers.

Whereas FGA is the number of field goal attempts. So this also
contains the number of unsuccessful field goals. So FGA will always be
a bigger number than FG.

- The next pair is for two pointers. The number of successful two
  pointers and the number attempted.
- The pair after that, right down here, is for three pointers, the
  number successful and the number attempted.
- And the next pair is for free throws, the number successful and the
  number attempted.

Now you'll notice, actually, that the two pointer and three pointer
variables have an 'X' in front of them. Well, this isn't because we
had an 'X' in the original data.

In fact, if you were to open up the csv file of the original data, it
would just say, 2P and 2PA, and, 3P and 3PA, without the 'X' in
front. The reason there's an 'X' in front of it is because when we
load it into R, R doesn't like it when a variable begins with a
number.

So if a variable begins with a number it will put an 'X' in front of
it. This is fine. It's just something we need to be mindful of when
we're dealing with variables in R.

- We've got ORB and DRB. These are offensive and defensive rebounds.
- AST stands for assists.
- STL stands for steals.
- BLK stands for blocks.
- And TOV stands for turnovers.

Ee just wanted to familiarize you with some common basketball
statistics that are recorded, and explain the labeling notation that
we use in our data.

** Video 2: Playoffs and Wins

The goal of a basketball team is similar to that of a baseball team,
making the playoffs. So how many games does a team need to win in
order to make the playoffs?

Recall that in the lecture we found this number by looking at a
graph.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: How many wins to make the playoffs?")
  head(table(NBA$W, NBA$Playoffs), 15)
#+END_SRC

#+RESULTS:
#+begin_example

 :: How many wins to make the playoffs?

      0 1
  11  2 0
  12  2 0
  13  2 0
  14  2 0
  15 10 0
  16  2 0
  17 11 0
  18  5 0
  19 10 0
  20 10 0
  21 12 0
  22 11 0
  23 11 0
  24 18 0
  25 11 0
#+end_example

So for all of our data, for example, consider all the times that a
team won 17 games. So this happened 11 times in total. And all 11
times the teams didn't make it to the playoffs when they won 17
games.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: How many wins to make the playoffs?")
  tail(table(NBA$W, NBA$Playoffs), 15)
#+END_SRC

#+RESULTS:
#+begin_example

 :: How many wins to make the playoffs?

     0  1
  55 0 24
  56 0 16
  57 0 23
  58 0 13
  59 0 14
  60 0  8
  61 0 10
  62 0 13
  63 0  7
  64 0  3
  65 0  3
  66 0  2
  67 0  4
  69 0  1
  72 0  1
#+end_example

At the end of the table, for example, 61 wins. If a team won 61 games
then 10 of those times they made it to the playoffs, and 0 times they
didn't.

So it seems like if you win 61 games you are definitely going to make
it to the playoffs. But I'm sure we can find a much better threshold.

Let's take a look at the table, say around the middle section. OK, so
here we can see that a team who wins say about 35 games or fewer
almost never makes it to the playoffs.

We see a lot of 0s and 1s in this column up until 35. After 35 we
start seeing some numbers over here. So teams are starting to make it
to the playoffs. And if we scroll down, we see that after about 45
wins, teams almost always make it to the playoffs.

We see very few 1s and 0s in the category of not making it. So it
seems like a good goal would be to try to win about 42 games.

*If a team can win about 42 games then they have a very good chance of
making it to the playoffs*. So in basketball, games are won by scoring
more points than the other team.

Can we use the difference between points scored and points allowed
throughout the regular season in order to predict the number of games
that a team will win?

First we add a variable that is the difference between points scored
and points allowed.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n ::Compute Points Difference")
  NBA$PTSdiff <- NBA$PTS - NBA$oppPTS
#+END_SRC

#+RESULTS:
:
:  ::Compute Points Difference

#+BEGIN_SRC R :var basename="exploratoryNBA" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  plot(NBA$PTSdiff, NBA$W)
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION: Scatterplot to check for linear relationship
#+NAME:   fig:exploratoryNBA
#+ATTR_LaTeX: placement: [H]
[[../graphs/exploratoryNBA.png]]

So our graph pops up and it looks like there's an incredibly strong
linear relationship between these two variables. So it seems like
linear regression is going to be a good way to predict how many wins a
team will have given the point difference.

** Building a linear regression model

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Linear regression model for wins")
  WinsReg <- lm(W ~ PTSdiff, data = NBA)
  summary(WinsReg)
#+END_SRC

#+RESULTS:
#+begin_example
 null device
          1

 :: Linear regression model for wins

Call:
lm(formula = W ~ PTSdiff, data = NBA)

Residuals:
    Min      1Q  Median      3Q     Max
-9.7393 -2.1018 -0.0672  2.0265 10.6026

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept) 4.100e+01  1.059e-01   387.0   <2e-16 ***
PTSdiff     3.259e-02  2.793e-04   116.7   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.061 on 833 degrees of freedom
Multiple R-squared:  0.9423,	Adjusted R-squared:  0.9423
F-statistic: 1.361e+04 on 1 and 833 DF,  p-value: < 2.2e-16
#+end_example

OK, so the first thing that we notice is that we've got very
significant variables over here. And an R squared of *0.9423*, which is
very high. And this is verifying the scatter plot we saw before that
there's a very strong linear relationship between the wins and the
points difference.

$$W = 41 + 0.03259 \times PTSdiff$$

If we know that a good chance to go to the playoffs is about 42
wins. Then

$$PTSdiff \geq \frac{42 - 41}{0.03259} $$

making the calculations we obtain

$$ PTSdiff = 30.6$$

So we need to score at least $31$ more points than we allow in order to
win at least $42$ games.

** Video 3: Points Scored

So now let's build an equation to predict points scored using some
common basketball statistics. So our dependent variable would now be
PTS, and our independent variables would be some of the common
basketball statistics that we have in our data set.

So for example, the number of two-point field goal attempts, the
number of three-point field goal attempts, offensive rebounds,
defensive rebounds, assists, steals, blocks, turnovers, free throw
attempts-- we can use all of these.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Linear regression model for points scored")
  PointsReg <- lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + DRB + TOV + STL + BLK, data = NBA)
  summary(PointsReg)
#+END_SRC

#+RESULTS:
#+begin_example

 :: Linear regression model for points scored

Call:
lm(formula = PTS ~ X2PA + X3PA + FTA + AST + ORB + DRB + TOV +
    STL + BLK, data = NBA)

Residuals:
    Min      1Q  Median      3Q     Max
-527.40 -119.83    7.83  120.67  564.71

Coefficients:
              Estimate Std. Error t value Pr(>|t|)
(Intercept) -2.051e+03  2.035e+02 -10.078   <2e-16 ***
X2PA         1.043e+00  2.957e-02  35.274   <2e-16 ***
X3PA         1.259e+00  3.843e-02  32.747   <2e-16 ***
FTA          1.128e+00  3.373e-02  33.440   <2e-16 ***
AST          8.858e-01  4.396e-02  20.150   <2e-16 ***
ORB         -9.554e-01  7.792e-02 -12.261   <2e-16 ***
DRB          3.883e-02  6.157e-02   0.631   0.5285
TOV         -2.475e-02  6.118e-02  -0.405   0.6859
STL         -1.992e-01  9.181e-02  -2.169   0.0303 *
BLK         -5.576e-02  8.782e-02  -0.635   0.5256
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 185.5 on 825 degrees of freedom
Multiple R-squared:  0.8992,	Adjusted R-squared:  0.8981
F-statistic: 817.3 on 9 and 825 DF,  p-value: < 2.2e-16
#+end_example

Okay, so taking a look at this, we can see that some of our variables
are indeed very, very, significant. Others are less significant. For
example, steals only has one significance star. And some don't seem to
be significant at all.

For example, defensive rebounds, turnovers, and blocks. We do have a
pretty good R-squared value, $0.8992$, so it shows that there really is
a linear relationship between points and all of these basketball
statistics.

We'll use this to compute the sum of squared errors. SSE, standing for
sum of squared errors:

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Residuals:")
  head(PointsReg$residuals, 20)

  writeLines("\n :: Sum of Squared Errors:")
  SSE <- sum(PointsReg$residuals^2)
  SSE
#+END_SRC

#+RESULTS:
#+begin_example

 :: Residuals:
          1           2           3           4           5           6
  38.572271  142.872004  -92.895718   -8.391347 -258.470561  171.460833
          7           8           9          10          11          12
 150.408162  169.381143   40.775620  -75.325661  444.908874   94.386470
         13          14          15          16          17          18
-205.680905  113.596904   64.199400  -76.571200  249.488801   28.036324
         19          20
 329.448799   96.324834

 :: Sum of Squared Errors:
[1] 28394314
#+end_example

But remember, we can also calculate the root mean squared error, which
is much more interpretable. It's more like the average error we make
in our predictions. So the root mean squared error, RMSE-- let's
calculate it here. So RMSE is just equal to the square root of the sum
of squared errors divided by n, where n here is the number of rows in
our data set.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Root mean squared error")
  RMSE <- sqrt(SSE / nrow(NBA))
  RMSE
#+END_SRC

#+RESULTS:
:
:  :: Root mean squared error
: [1] 184.4049

That seems like quite a lot, until you remember that the average
number of points in a season is:

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Average number of points in a season")
  mean(NBA$PTS)
#+END_SRC

#+RESULTS:
:
:  :: Average number of points in a season
: [1] 8370.24

So, okay, if we have an average number of points of $8,370$, being off
by about 184.4 points is really not so bad.

#+BEGIN_SRC R :session :results output :exports all
  summary(PointsReg)
#+END_SRC

#+RESULTS:
#+begin_example

Call:
lm(formula = PTS ~ X2PA + X3PA + FTA + AST + ORB + DRB + TOV +
    STL + BLK, data = NBA)

Residuals:
    Min      1Q  Median      3Q     Max
-527.40 -119.83    7.83  120.67  564.71

Coefficients:
              Estimate Std. Error t value Pr(>|t|)
(Intercept) -2.051e+03  2.035e+02 -10.078   <2e-16 ***
X2PA         1.043e+00  2.957e-02  35.274   <2e-16 ***
X3PA         1.259e+00  3.843e-02  32.747   <2e-16 ***
FTA          1.128e+00  3.373e-02  33.440   <2e-16 ***
AST          8.858e-01  4.396e-02  20.150   <2e-16 ***
ORB         -9.554e-01  7.792e-02 -12.261   <2e-16 ***
DRB          3.883e-02  6.157e-02   0.631   0.5285
TOV         -2.475e-02  6.118e-02  -0.405   0.6859
STL         -1.992e-01  9.181e-02  -2.169   0.0303 *
BLK         -5.576e-02  8.782e-02  -0.635   0.5256
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 185.5 on 825 degrees of freedom
Multiple R-squared:  0.8992,	Adjusted R-squared:  0.8981
F-statistic: 817.3 on 9 and 825 DF,  p-value: < 2.2e-16
#+end_example

We'll take a look again at our model, summary(PointsReg), in order to
figure out which variable we should remove first. The first variable
we would want to remove is probably turnovers.

And why do I say turnovers?

It's because the p value for turnovers, which you see here in this
column, 0.6859, is the highest of all of the p values. So that means
that turnovers is the least statistically significant variable in our
model.

So let's create a new regression model without turnovers.

#+BEGIN_SRC R :session :results output :exports all
  PointsReg2 <- lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + DRB + STL +
                           BLK, data = NBA)
  summary(PointsReg2)
#+END_SRC

#+RESULTS:
#+begin_example

Call:
lm(formula = PTS ~ X2PA + X3PA + FTA + AST + ORB + DRB + STL +
    BLK, data = NBA)

Residuals:
    Min      1Q  Median      3Q     Max
-526.79 -121.09    6.37  120.74  565.94

Coefficients:
              Estimate Std. Error t value Pr(>|t|)
(Intercept) -2.077e+03  1.931e+02 -10.755   <2e-16 ***
X2PA         1.044e+00  2.951e-02  35.366   <2e-16 ***
X3PA         1.263e+00  3.703e-02  34.099   <2e-16 ***
FTA          1.125e+00  3.308e-02  34.023   <2e-16 ***
AST          8.861e-01  4.393e-02  20.173   <2e-16 ***
ORB         -9.581e-01  7.758e-02 -12.350   <2e-16 ***
DRB          3.892e-02  6.154e-02   0.632   0.5273
STL         -2.068e-01  8.984e-02  -2.301   0.0216 *
BLK         -5.863e-02  8.749e-02  -0.670   0.5029
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 185.4 on 826 degrees of freedom
Multiple R-squared:  0.8991,	Adjusted R-squared:  0.8982
F-statistic: 920.4 on 8 and 826 DF,  p-value: < 2.2e-16
#+end_example

So in our first regression model, *PointsReg*, we had an *R-squared* of
$0.8992$. Let's take a look at the *R-squared* of *PointsReg2*. And we see
that it's $0.8991$. It does go down, as we would expect, but very,
very slightly.

*So it seems that we're justified in removing turnovers.*

The next one, based on p-value, that we would want to remove is
*defensive rebounds*.

#+BEGIN_SRC R :session :results output :exports all
  PointsReg3 <- lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + STL + BLK, data
                   = NBA)
  summary(PointsReg3)
#+END_SRC

#+RESULTS:
#+begin_example

Call:
lm(formula = PTS ~ X2PA + X3PA + FTA + AST + ORB + STL + BLK,
    data = NBA)

Residuals:
    Min      1Q  Median      3Q     Max
-523.79 -121.64    6.07  120.81  573.64

Coefficients:
              Estimate Std. Error t value Pr(>|t|)
(Intercept) -2.015e+03  1.670e+02 -12.068  < 2e-16 ***
X2PA         1.048e+00  2.852e-02  36.753  < 2e-16 ***
X3PA         1.271e+00  3.475e-02  36.568  < 2e-16 ***
FTA          1.128e+00  3.270e-02  34.506  < 2e-16 ***
AST          8.909e-01  4.326e-02  20.597  < 2e-16 ***
ORB         -9.702e-01  7.519e-02 -12.903  < 2e-16 ***
STL         -2.276e-01  8.356e-02  -2.724  0.00659 **
BLK         -3.882e-02  8.165e-02  -0.475  0.63462
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 185.4 on 827 degrees of freedom
Multiple R-squared:  0.8991,	Adjusted R-squared:  0.8982
F-statistic:  1053 on 7 and 827 DF,  p-value: < 2.2e-16
#+end_example

Let's look at the summary again to see if the *R-squared* has
changed. And it's the same, it's $0.8991$. So I think we're justified
again in removing *defensive rebounds*.

Let's try this one more time and see if we can remove *blocks*.

#+BEGIN_SRC R :session :results output :exports all
  PointsReg4 <- lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + STL, data =
                                                                      NBA)
  summary(PointsReg4)
#+END_SRC

#+RESULTS:
#+begin_example

Call:
lm(formula = PTS ~ X2PA + X3PA + FTA + AST + ORB + STL, data = NBA)

Residuals:
    Min      1Q  Median      3Q     Max
-523.33 -122.02    6.93  120.68  568.26

Coefficients:
              Estimate Std. Error t value Pr(>|t|)
(Intercept) -2.033e+03  1.629e+02 -12.475  < 2e-16 ***
X2PA         1.050e+00  2.829e-02  37.117  < 2e-16 ***
X3PA         1.273e+00  3.441e-02  37.001  < 2e-16 ***
FTA          1.127e+00  3.260e-02  34.581  < 2e-16 ***
AST          8.884e-01  4.292e-02  20.701  < 2e-16 ***
ORB         -9.743e-01  7.465e-02 -13.051  < 2e-16 ***
STL         -2.268e-01  8.350e-02  -2.717  0.00673 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 185.3 on 828 degrees of freedom
Multiple R-squared:  0.8991,	Adjusted R-squared:  0.8983
F-statistic:  1229 on 6 and 828 DF,  p-value: < 2.2e-16
#+end_example

And again, the *R-squared* value stayed the same. So now we've gotten
down to a model which is a bit simpler. All the variables are
significant. We've still got an *R-squared* $0.899$.

And let's take a look now at the sum of squared errors and the root
mean square error, just to make sure we didn't inflate those too much
by removing a few variables.

Remember that the sum of squared errors that we had in the original
model was this giant number, 28,394,314. And the root mean squared
error, the much more interpretable number, was 184.4.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Compute SSE and RMSE for new model")
  SSE_4 <- sum(PointsReg4$residuals^2)
  RMSE_4 <- sqrt(SSE_4/nrow(NBA))
  SSE_4
  RMSE_4
#+END_SRC

#+RESULTS:
:
:  :: Compute SSE and RMSE for new model
: [1] 28421465
: [1] 184.493

So although we've increased the root mean squared error a little bit
by removing those variables, it's really a very, very, small
amount. Essentially, we've kept the root mean squared error the
same. So it seems like we've narrowed down on a much better model
because it's simpler, it's more interpretable, and it's got just about
the same amount of error.

** Video 4: Making Predictions

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Read the data of the test data set")
  NBA_test <- read.table("../data/NBA_test.csv", sep = ",", header = TRUE)
  str(NBA_test)
  summary(NBA_test)
#+END_SRC

#+RESULTS:
#+begin_example

 :: Read the data of the test data set
'data.frame':	28 obs. of  20 variables:
 $ SeasonEnd: int  2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...
 $ Team     : Factor w/ 28 levels "Atlanta Hawks",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ Playoffs : int  1 1 0 1 0 0 1 0 1 1 ...
 $ W        : int  44 49 21 45 24 41 57 29 47 45 ...
 $ PTS      : int  8032 7944 7661 7641 7913 8293 8704 7778 8296 8688 ...
 $ oppPTS   : int  7999 7798 8418 7615 8297 8342 8287 8105 8223 8403 ...
 $ FG       : int  3084 2942 2823 2926 2993 3182 3339 2979 3130 3124 ...
 $ FGA      : int  6644 6544 6649 6698 6901 6892 6983 6638 6840 6782 ...
 $ X2P      : int  2378 2314 2354 2480 2446 2576 2818 2466 2472 2257 ...
 $ X2PA     : int  4743 4784 5250 5433 5320 5264 5465 5198 5208 4413 ...
 $ X3P      : int  706 628 469 446 547 606 521 513 658 867 ...
 $ X3PA     : int  1901 1760 1399 1265 1581 1628 1518 1440 1632 2369 ...
 $ FT       : int  1158 1432 1546 1343 1380 1323 1505 1307 1378 1573 ...
 $ FTA      : int  1619 1958 2060 1738 1826 1669 2148 1870 1744 2087 ...
 $ ORB      : int  758 1047 917 1026 1004 767 1092 991 885 909 ...
 $ DRB      : int  2593 2460 2389 2514 2359 2670 2601 2463 2801 2652 ...
 $ AST      : int  2007 1668 1587 1886 1694 1906 2002 1742 1845 1902 ...
 $ STL      : int  664 599 591 588 647 648 762 574 567 679 ...
 $ BLK      : int  369 391 479 417 334 454 533 400 346 359 ...
 $ TOV      : int  1219 1206 1153 1171 1149 1144 1253 1241 1236 1348 ...
   SeasonEnd                     Team       Playoffs         W
 Min.   :2013   Atlanta Hawks      : 1   Min.   :0.0   Min.   :20.00
 1st Qu.:2013   Brooklyn Nets      : 1   1st Qu.:0.0   1st Qu.:29.00
 Median :2013   Charlotte Bobcats  : 1   Median :0.5   Median :42.00
 Mean   :2013   Chicago Bulls      : 1   Mean   :0.5   Mean   :40.68
 3rd Qu.:2013   Cleveland Cavaliers: 1   3rd Qu.:1.0   3rd Qu.:50.25
 Max.   :2013   Dallas Mavericks   : 1   Max.   :1.0   Max.   :66.00
                (Other)            :22
      PTS           oppPTS           FG            FGA            X2P
 Min.   :7640   Min.   :7319   Min.   :2823   Min.   :6348   Min.   :2105
 1st Qu.:7763   1st Qu.:7898   1st Qu.:2975   1st Qu.:6643   1st Qu.:2375
 Median :8014   Median :8068   Median :3052   Median :6696   Median :2474
 Mean   :8062   Mean   :8073   Mean   :3051   Mean   :6737   Mean   :2460
 3rd Qu.:8294   3rd Qu.:8288   3rd Qu.:3126   3rd Qu.:6893   3rd Qu.:2540
 Max.   :8704   Max.   :8619   Max.   :3339   Max.   :7197   Max.   :2818

      X2PA           X3P             X3PA            FT            FTA
 Min.   :4318   Min.   :382.0   Min.   :1107   Min.   :1004   Min.   :1359
 1st Qu.:4845   1st Qu.:511.5   1st Qu.:1469   1st Qu.:1298   1st Qu.:1695
 Median :5203   Median :584.5   Median :1608   Median :1357   Median :1786
 Mean   :5091   Mean   :591.1   Mean   :1646   Mean   :1368   Mean   :1819
 3rd Qu.:5336   3rd Qu.:659.2   3rd Qu.:1761   3rd Qu.:1440   3rd Qu.:1906
 Max.   :5572   Max.   :891.0   Max.   :2371   Max.   :1819   Max.   :2289

      ORB              DRB            AST            STL             BLK
 Min.   : 666.0   Min.   :2359   Min.   :1579   Min.   :520.0   Min.   :294.0
 1st Qu.: 882.2   1st Qu.:2452   1st Qu.:1737   1st Qu.:590.2   1st Qu.:366.5
 Median : 927.5   Median :2482   Median :1840   Median :653.5   Median :408.5
 Mean   : 920.0   Mean   :2533   Mean   :1819   Mean   :640.4   Mean   :419.4
 3rd Qu.: 989.5   3rd Qu.:2622   3rd Qu.:1887   3rd Qu.:686.2   3rd Qu.:448.0
 Max.   :1092.0   Max.   :2801   Max.   :2058   Max.   :784.0   Max.   :624.0

      TOV
 Min.   : 988
 1st Qu.:1152
 Median :1201
 Mean   :1191
 3rd Qu.:1233
 Max.   :1348
#+end_example

All right, so now let's try to predict using our model that we made in
the previous video, how many points we'll see in the 2012-2013
season.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Make predictions on test set")
  PointsPredictions = predict(PointsReg4, newdata=NBA_test)
#+END_SRC

#+RESULTS:
:
:  :: Make predictions on test set

OK, so now that we have our prediction, how good is it? We can compute
the out of sample R-squared. This is a measurement of how well the
model predicts on test data.

The *R-squared* value we had before from our model, the $0.8991$, you
might remember, is the measure of an in-sample R-squared, which is how
well the model fits the training data.

But to get a measure of the predictions goodness of fit, we need to
calculate the *out of sample R-squared*.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Compute out-of-sample R^2")
  SSE <- sum((PointsPredictions - NBA_test$PTS)^2)
  SST <- sum((mean(NBA$PTS) - NBA_test$PTS)^2)
  R2 <- 1 - (SSE/SST)
  R2
#+END_SRC

#+RESULTS:
:
:  :: Compute out-of-sample R^2
: [1] 0.8127142

We can also calculate the root mean squared error the same way as
before, root mean squared error is going to be the square root of the
sum of squared errors divided by n, which is the number of rows in our
test data set.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("\n :: Compute the RMSE")
  RMSE <- sqrt(SSE/nrow(NBA_test))
  RMSE
#+END_SRC

#+RESULTS:
:
:  :: Compute the RMSE
: [1] 196.3723

OK and the *root mean squared error* here is $196.37$.

So it's a little bit higher than before. But it's not too bad. We're
making an *average error* of about $196$ points.
