#+TITLE:         Assignment 7. Visualization
#+AUTHOR:        Sergio-Feliciano Mendoza-Barrera
#+DRAWERS:       sfmb
#+EMAIL:         smendoza.barrera@gmail.com
#+DATE:          24/07/2015
#+DESCRIPTION:   Unit 7 Visualization Assignment of the Analytical Edge course
#+KEYWORDS:      R, data science, emacs, ESS, org-mode, the analytical edge, visualization
#+LANGUAGE:      en
#+OPTIONS:       H:10 num:t toc:nil \n:nil @:t ::t |:t ^:{} -:t f:t *:t <:t d:HIDDEN
#+OPTIONS:       TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+OPTIONS:       LaTeX:dvipng
#+INFOJS_OPT:    view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:
#+LINK_HOME:
#+XSLT:
#+STYLE: <link rel="stylesheet" type="text/css" href="dft.css"/>

#+LaTeX_CLASS: IEEEtran
#+LATEX_CLASS_OPTIONS: [letterpaper, 9pt, onecolumn, twoside, technote, final]
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage{makeidx}

#+LATEX_HEADER: \usepackage[lining,tabular]{fbb} % so math uses tabular lining figures
#+LATEX_HEADER: \usepackage[scaled=.95,type1]{cabin} % sans serif in style of Gill Sans
#+LATEX_HEADER: \usepackage[varqu,varl]{zi4}% inconsolata typewriter
#+LATEX_HEADER: \usepackage[T1]{fontenc} % LY1 also works
#+LATEX_HEADER: \usepackage[libertine,bigdelims]{newtxmath}
#+LATEX_HEADER: \usepackage[cal=boondoxo,bb=boondox,frak=boondox]{mathalfa}
#+LATEX_HEADER: \useosf % change normal text to use proportional oldstyle figures

#+LATEX_HEADER: \markboth{Unit 7 Visualization. July 2015.}%
#+LATEX_HEADER: {Sergio-Feliciano Mendoza-Barrera}

#+LATEX_HEADER: \newcommand{\degC}{$^\circ$C{}}

#+STYLE: <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script>

#+ATTR_HTML: width="500px"

# -*- mode: org; -*-
#+OPTIONS:   toc:2

#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>

#+HTML_HEAD: <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
#+HTML_HEAD: <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>

#+BEGIN_ABSTRACT
Unit 7 Visualization assignment of The Analytical Edge course from MIT.
#+END_ABSTRACT

* Election Forecasting Revisited [12/12]

In the recitation from Unit 3, we used logistic regression on polling
data in order to construct US presidential election predictions. We
separated our data into a training set, containing data from 2004 and
2008 polls, and a test set, containing the data from 2012 polls. We
then proceeded to develop a logistic regression model to forecast the
2012 US presidential election.

In this homework problem, we'll revisit our logistic regression model
from Unit 3, and learn how to plot the output on a map of the United
States. Unlike what we did in the Crime lecture, this time we'll be
plotting predictions rather than data!

First, load the ~ggplot2~, ~maps~, and ~ggmap~ packages using the
library function. All three packages should be installed on your
computer from lecture, but if not, you may need to install them too
using the ~install.packages~ function.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Loading the necessary libraries...")
  library(ggplot2)
  library(maps)
  library(ggmap)
#+end_src

#+RESULTS:
:
:  :: Loading the necessary libraries...

Then, load the US map and save it to the variable statesMap, like we
did during the Crime lecture:

~statesMap = map_data("state")~

#+begin_src R :session :results output :exports all
  writeLines("\n :: Load the US map in R...")
  statesMap <- map_data("state")
#+end_src

#+RESULTS:
:
:  :: Load the US map in R...

The maps package contains other built-in maps, including a US county
map, a world map, and maps for France and Italy.

** DONE Problem 1.1 - Drawing a Map of the US (1 point possible)
CLOSED: [2015-08-31 Mon 09:35]

If you look at the structure of the statesMap data frame using the ~str~
function, you should see that there are 6 variables. One of the
variables, group, defines the different shapes or polygons on the
map. Sometimes a state may have multiple groups, for example, if it
includes islands.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Showing the structure of stateMap:")
  str(statesMap)
#+end_src

#+RESULTS:
:
:  :: Showing the structure of stateMap:
: 'data.frame':	15537 obs. of  6 variables:
:  $ long     : num  -87.5 -87.5 -87.5 -87.5 -87.6 ...
:  $ lat      : num  30.4 30.4 30.4 30.3 30.3 ...
:  $ group    : num  1 1 1 1 1 1 1 1 1 1 ...
:  $ order    : int  1 2 3 4 5 6 7 8 9 10 ...
:  $ region   : chr  "alabama" "alabama" "alabama" "alabama" ...
:  $ subregion: chr  NA NA NA NA ...

*** Question

How many different groups are there?

#+begin_src R :session :results output :exports all
  writeLines("\n :: The number of groups:")
  str(factor(statesMap$group))

  writeLines("\n :: Another useful way:")
  table(statesMap$group)

  writeLines("\n :: Yet another way to calculate:")
  length(table(statesMap$group))
#+end_src

#+RESULTS:
#+begin_example

 :: The number of groups:
 Factor w/ 63 levels "1","2","3","4",..: 1 1 1 1 1 1 1 1 1 1 ...

 :: Another useful way:

   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16
 202  149  312  516   79   91   94   10  872  381  233  329  257  256  113  397
  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32
 650  399  566   36  220   30  460  370  373  382  315  238  208   70  125  205
  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48
  78   16  290   21  168   37  733   12  105  238  284  236  172   66  304  166
  49   50   51   52   53   54   55   56   57   58   59   60   61   62   63
 289 1088   59  129   96   15  623   17   17   19   44  448  373  388   68

 :: Yet another way to calculate:
[1] 63
#+end_example

**** Answer

63

*Explanation*

You can count the number of different values of the group variable by
using the command ~table(statesMap$group)~. There are $63$ different
values.

Alternatively, you could use the command

~length(table(statesMap$group))~

as a shortcut to counting the number of groups in the table output.

The variable ~order~ defines the order to connect the points within
each group, and the variable ~region~ gives the name of the state.

** DONE Problem 1.2 - Drawing a Map of the US (1 point possible)
CLOSED: [2015-08-31 Mon 09:36]

You can draw a map of the United States by typing the following in
your R console:

#+BEGIN_SRC R :var basename="USmapHW01" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  ggplot(statesMap, aes(x = long, y = lat, group = group)) +
          geom_polygon(fill = "white", color = "black")
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  United States map in R
#+NAME:     fig:USmapHW01
#+ATTR_LaTeX: placement: [H]
[[../graphs/USmapHW01.png]]

We specified two colors in geom_polygon -- ~fill~ and ~color~. Which
one defined the color of the outline of the states?

*** Answer

- [ ] fill
- [X] color color - correct
- [ ] Neither

*Explanation*

In our plot, the states are outlined in black, which is the color we
specified for the option ~color~. To confirm that this is changing the
outline color of the states, you can try re-running the command with a
different color:

#+BEGIN_SRC R :var basename="USMapHW02" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  ggplot(statesMap, aes(x = long, y = lat, group = group)) +
          geom_polygon(fill = "white", color = "dark red")
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  United States map in other outline color
#+NAME:     fig:USMapHW02
#+ATTR_LaTeX: placement: [H]
[[../graphs/USMapHW02.png]]

** DONE Problem 2.1 - Coloring the States by Predictions (2 points possible)
CLOSED: [2015-08-31 Mon 09:36]

Now, let's color the map of the US according to our 2012 US
presidential election predictions from the Unit 3 Recitation. We'll
rebuild the model here, using the dataset ~PollingImputed.csv~. Be sure
to use this file so that you don't have to redo the imputation to fill
in the missing values, like we did in the Unit 3 Recitation.

#+BEGIN_SRC R :session :results output :exports all
  library(parallel)

  if(!file.exists("../data")) {
          dir.create("../data")
  }

  fileUrl <- "https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/PollingImputed.csv"
  fileName <- "PollingImputed.csv"
  dataPath <- "../data"

  filePath <- paste(dataPath, fileName, sep = "/")

  if(!file.exists(filePath)) {
          download.file(fileUrl, destfile = filePath, method = "curl")
  }

  writeLines("\n :: File downloaded...")
#+END_SRC

#+RESULTS:
: quartz_off_screen
:                 2
:  quartz_off_screen
:                 2
:
:  :: File downloaded...

Load the data using the ~read.csv~ function, and call it ~polling~. Then
split the data using the subset function into a training set called
~Train~ that has observations from 2004 and 2008, and a testing set
called ~Test~ that has observations from 2012.

#+BEGIN_SRC R :session :results output :exports all
  writeLines("    Loading the imputed data into their data frame.")
  polling <- read.table("../data/PollingImputed.csv", sep = ",", header = TRUE)
  str(polling)
  table(polling$Year)
  summary(polling)
#+END_SRC

#+RESULTS:
#+begin_example
    Loading the imputed data into their data frame.
'data.frame':	145 obs. of  7 variables:
 $ State     : Factor w/ 50 levels "Alabama","Alaska",..: 1 1 2 2 3 3 3 4 4 4 ...
 $ Year      : int  2004 2008 2004 2008 2004 2008 2012 2004 2008 2012 ...
 $ Rasmussen : int  11 21 19 16 5 5 8 7 10 13 ...
 $ SurveyUSA : int  18 25 21 18 15 3 5 5 7 21 ...
 $ DiffCount : int  5 5 1 6 8 9 4 8 5 2 ...
 $ PropR     : num  1 1 1 1 1 ...
 $ Republican: int  1 1 1 1 1 1 1 1 1 1 ...

2004 2008 2012
  50   50   45
         State          Year        Rasmussen         SurveyUSA
 Arizona    :  3   Min.   :2004   Min.   :-41.000   Min.   :-33.000
 Arkansas   :  3   1st Qu.:2004   1st Qu.:-10.000   1st Qu.:-11.000
 California :  3   Median :2008   Median :  3.000   Median :  1.000
 Colorado   :  3   Mean   :2008   Mean   :  2.048   Mean   :  1.359
 Connecticut:  3   3rd Qu.:2012   3rd Qu.: 12.000   3rd Qu.: 16.000
 Florida    :  3   Max.   :2012   Max.   : 39.000   Max.   : 30.000
 (Other)    :127
   DiffCount           PropR          Republican
 Min.   :-19.000   Min.   :0.0000   Min.   :0.0000
 1st Qu.: -6.000   1st Qu.:0.0000   1st Qu.:0.0000
 Median :  1.000   Median :0.6250   Median :1.0000
 Mean   : -1.269   Mean   :0.5259   Mean   :0.5103
 3rd Qu.:  4.000   3rd Qu.:1.0000   3rd Qu.:1.0000
 Max.   : 11.000   Max.   :1.0000   Max.   :1.0000
#+end_example

Splitting the dataset in two data frames ~Train~ and ~Test~ data frames.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Subset data into training set and test set")
  Train <- subset(polling, Year == 2004 | Year == 2008)
  Test <- subset(polling, Year == 2012)
#+end_src

#+RESULTS:
:
:  :: Subset data into training set and test set

Note that we only have 45 states in our testing set, since we are
missing observations for Alaska, Delaware, Alabama, Wyoming, and
Vermont, so these states will not appear colored in our map.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Revising the Testing data frame structure:")
  str(Test)
#+end_src

#+RESULTS:
#+begin_example

 :: Revising the Testing data frame structure:
'data.frame':	45 obs. of  7 variables:
 $ State     : Factor w/ 50 levels "Alabama","Alaska",..: 3 4 5 6 7 9 10 11 12 13 ...
 $ Year      : int  2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ...
 $ Rasmussen : int  8 13 -12 3 -7 2 5 -22 31 -22 ...
 $ SurveyUSA : int  5 21 -14 -2 -13 0 8 -24 24 -16 ...
 $ DiffCount : int  4 2 -6 -5 -8 6 4 -2 1 -5 ...
 $ PropR     : num  0.833 1 0 0.308 0 ...
 $ Republican: int  1 1 0 0 0 0 1 0 1 0 ...
#+end_example

Then, create a logistic regression model and make predictions on the
test set using the following commands:

#+begin_src R :session :results output :exports all
  writeLines("\n :: Creating the Logistic Regression model...")
  mod2 <- glm(Republican ~ SurveyUSA + DiffCount, data = Train,
              family = "binomial")

  writeLines("\n :: Making preditions from Testing data set...")
  TestPrediction <- predict(mod2, newdata = Test, type = "response")
#+end_src

#+RESULTS:
:
:  :: Creating the Logistic Regression model...
:
:  :: Making preditions from Testing data set...

~TestPrediction~ gives the predicted probabilities for each state, but
let's also create a vector of Republican/Democrat predictions by using
the following command:

#+begin_src R :session :results output :exports all
  writeLines("\n :: Creting binary preditions with a threshold of 50%...")
  TestPredictionBinary <- as.numeric(TestPrediction > 0.5)
#+end_src

#+RESULTS:
:
:  :: Creting binary preditions with a threshold of 50%...

Now, put the predictions and state labels in a data.frame so that we
can use ~ggplot~:

#+begin_src R :session :results output :exports all
  writeLines("\n :: New data frame for plotting purposes...")
  predictionDataFrame <- data.frame(TestPrediction,
                                    TestPredictionBinary, Test$State)
#+end_src

#+RESULTS:
:
:  :: New data frame for plotting purposes...

To make sure everything went smoothly, answer the following
questions.

*** Question a

For how many states is our binary prediction 1 (for 2012),
corresponding to Republican?

#+begin_src R :session :results output :exports all
  writeLines("\n :: Number of states with TestPredictionbinary == 1:")
  nrow(subset(predictionDataFrame, predictionDataFrame$TestPredictionBinary == 1))
#+end_src

#+RESULTS:
:
:  :: Number of states with TestPredictionbinary == 1:
: [1] 22

**** Answer

22

*** Question

What is the average predicted probability of our model (on the Test
set, for 2012)?

#+begin_src R :session :results output :exports all
  writeLines("\n :: The average predicted probability of our model:")
  mean(predictionDataFrame$TestPrediction)
#+end_src

#+RESULTS:
:
:  :: The average predicted probability of our model:
: [1] 0.4852626

**** Answer

0.4852626

*Explanation*

You can create the data frame predictionDataFrame by running the
following lines of R code:

~polling = read.csv("PollingImputed.csv")~

~Train = subset(polling, Year < 2012)~

~Test = subset(polling, Year == 2012)~

~mod2 = glm(Republican~SurveyUSA+DiffCount, data=Train, family="binomial")~

~TestPrediction = predict(mod2, newdata=Test, type="response")~

TestPredictionBinary = as.numeric(TestPrediction > 0.5)~

~predictionDataFrame = data.frame(TestPrediction, TestPredictionBinary, Test$State)~

You can answer the two questions with the functions
~table(TestPredictionBinary)~ and ~mean(TestPrediction)~.

** DONE Problem 2.2 - Coloring the States by Predictions (2 points possible)
CLOSED: [2015-08-31 Mon 09:36]

Now, we need to merge ~predictionDataFrame~ with the map data
~statesMap~, like we did in lecture. Before doing so, we need to
convert the ~Test.State~ variable to lowercase, so that it matches the
region variable in ~statesMap~. Do this by typing the following in your
R console:

#+begin_src R :session :results output :exports all
  writeLines("\n :: convert the Test.State variable to lowercase...")
  predictionDataFrame$region <- tolower(predictionDataFrame$Test.State)
#+end_src

#+RESULTS:
:
:  :: convert the Test.State variable to lowercase...

Now, merge the two data frames using the following command:

#+begin_src R :session :results output :exports all
  writeLines("\n :: Merging the two data frames...")
  predictionMap <- merge(statesMap, predictionDataFrame, by = "region")
#+end_src

#+RESULTS:
:
:  :: Merging the two data frames...

Lastly, we need to make sure the observations are in order so that the
map is drawn properly, by typing the following:

#+begin_src R :session :results output :exports all
  writeLines("\n :: Ordering the map data...")
  predictionMap <- predictionMap[order(predictionMap$order),]
#+end_src

#+RESULTS:
:
:  :: Ordering the map data...

*** Question

How many observations are there in ~predictionMap~?

**** Answer

#+begin_src R :session :results output :exports all
  writeLines("\n :: Number of observations in the predition map:")
  nrow(predictionMap)
#+end_src

#+RESULTS:
:
:  :: Number of observations in the predition map:
: [1] 15034

*** Question

How many observations are there in ~statesMap~?

**** Answer

#+begin_src R :session :results output :exports all
  writeLines("\n :: Number of observations in the states map:")
  nrow(statesMap)
#+end_src

#+RESULTS:
:
:  :: Number of observations in the states map:
: [1] 15537

*Explanation*

If you type ~str(predictionMap)~, you should see that there are $15034$
observations, and if you type ~str(statesMap)~ you should see that there
are $15537$ observations.

** DONE Problem 2.3 - Coloring the States by Predictions (1 point possible)
CLOSED: [2015-08-31 Mon 09:36]

When we merged the data in the previous problem, it caused the number
of observations to change. Why? Check out the help page for merge by
typing ?merge to help you answer this question.

*** Answer

- [ ] Merging the data just combines the two data frames like it would
  if we used rbind, so the number of observations increased.

- [ ] We have more observations for each state now, because some
  observations have the statesMap data, and some observations have the
  prediction data.

- [X] Because we only make predictions for 45 states, we no longer
  have observations for some of the states. These observations were
  removed in the merging process.

- [ ] We merged the observations for which our predictions are
  identical.

*Explanation*

When we merge data, it only merged the observations that exist in both
data sets. So since we are merging based on the region variable, we
will lose all observations that have a value of ~region~ that doesn't
exist in both data frames.

You can change this default behavior by using the ~all.x~ and ~all.y~
arguments of the merge function. For more information, look at the
help page for the merge function by typing ~?merge~ in your R console.

** DONE Problem 2.4 - Coloring the States by Predictions (1 point possible)
CLOSED: [2015-08-31 Mon 09:37]

Now we are ready to color the US map with our predictions! You can
color the states according to our binary predictions by typing the
following in your R console:

#+BEGIN_SRC R :var basename="USElectionsPredictions01" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  ggplot(predictionMap, aes(x = long, y = lat, group = group, fill =
          TestPredictionBinary)) +
          geom_polygon(color = "black")
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  US Presidential predictions map v1
#+NAME:     fig:USElectionsPredictions01
#+ATTR_LaTeX: placement: [H]
[[../graphs/USElectionsPredictions01.png]]

The states appear light blue and dark blue in this map. Which color
represents a Republican prediction?

*** Answer

- [X] Light blue
- [ ] Dark blue

*Explanation*

Our logistic regression model assigned $1$ to *Republican* and $0$ to
*Democrat*. As we can see from the legend, $1$ corresponds to a *light
blue* color on the map and $0$ corresponds to a *dark blue* color on
the map.

** DONE Problem 2.5 - Coloring the States by Predictions (1 point possible)
CLOSED: [2015-08-31 Mon 09:37]

We see that the legend displays a *blue* gradient for outcomes between
$0$ and $1$. However, when plotting the binary predictions there are
only two possible outcomes: $0$ or $1$. Let's replot the map with
discrete outcomes. We can also change the color scheme to *blue* and
*red*, to match the *blue* color associated with the *Democratic
Party* in the US and the *red* color associated with the *Republican
Party* in the US. This can be done with the following command:

#+BEGIN_SRC R :var basename="USElectionsPredictions02" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary)) +
          geom_polygon(color = "black") +
          scale_fill_gradient(low = "blue", high = "red", guide =
          "legend", breaks= c(0,1), labels = c("Democrat",
                                               "Republican"), name =
                                                           "Prediction 2012")
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  US presidential election map v2
#+NAME:     fig:USElectionsPredictions02
#+ATTR_LaTeX: placement: [H]
[[../graphs/USElectionsPredictions02.png]]


Alternatively, we could plot the probabilities instead of the binary
predictions. Change the ~plot~ command above to instead color the
states by the variable ~TestPrediction~. You should see a gradient of
colors ranging from *red* to *blue*. Do the colors of the states in
the map for ~TestPrediction~ look different from the colors of the
states in the map with ~TestPredictionBinary~? Why or why not?

NOTE: If you have a hard time seeing the red/blue gradient, feel free
to change the color scheme, by changing the arguments ~low = "blue"~ and
~high = "red"~ to colors of your choice (to see all of the color options
in R, type colors() in your R console). You can even change it to a
gray scale, by changing the low and high colors to ~gray~ and
~black~.

#+BEGIN_SRC R :var basename="USElectionsPredictions03" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  ggplot(predictionMap, aes(x = long, y = lat, group = group, fill =
                                                                      TestPrediction)) +
          geom_polygon(color = "black") +
          scale_fill_gradient(low = "blue", high = "red", name = "Prediction 2012")
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  US Presidential Election map v3
#+NAME:     fig:USElectionsPredictions03
#+ATTR_LaTeX: placement: [H]
[[../graphs/USElectionsPredictions03.png]]

*** Answer

- [X] The two maps look very similar. This is because most of our
  predicted probabilities are close to 0 or close to 1. The two maps
  look very similar. This is because most of our predicted
  probabilities are close to 0 or close to 1. - correct

- [ ] The two maps look very similar. This is because ~TestPrediction~
  and TestPredictionBinary have the exact same values.

- [ ] The two maps look very different. This is because we have
  switched from plotting discrete values to plotting continuous
  values.

- [ ] The two maps look very different. This is because our predicted
  probabilites have a wide range of values, and we were not sure about
  many states.

*Explanation*

This plot can be generated by using the command:

~ggplot(predictionMap, aes(x = long, y = lat, group = group, fill =~
~TestPrediction)) + geom_polygon(color = "black") +~
~scale_fill_gradient(low = "blue", high = "red", name = "Prediction~
~2012")~

The only state that appears *purple* (the color between *red* and
*blue*) is the state of Iowa, so the maps look very similar. If you
take a look at ~TestPrediction~, you can see that most of our
predicted probabilities are very close to $0$ or very close to $1$. In
fact, we don't have a single predicted probability between $0.065$ and
$0.93$.

** DONE Problem 3.1 - Understanding the Predictions (1 point possible)
CLOSED: [2015-08-31 Mon 09:38]

In the 2012 election, the state of *Florida* ended up being a very close
race. It was ultimately won by the *Democratic* party. Did we predict
this state correctly or incorrectly? To see the names and locations of
the different states, take a look at the World Atlas map here.

*** Answer

- [ ] We correctly predicted that this state would be won by the
  Democratic party.

- [X] We incorrectly predicted this state by predicting that it would
  be won by the Republican party.

*Explanation*

In our prediction map, the state of Florida is colored *red*, meaning
that we predicted Republican. So *we incorrectly predicted this
state*.

** DONE Problem 3.2 - Understanding the Predictions (2 points possible)
CLOSED: [2015-08-31 Mon 09:38]

*** Question a

What was our predicted probability for the state of Florida?

**** Answer

#+begin_src R :session :results output :exports all
  writeLines("\n :: The predicted probability for Florida:")
  head(predictionMap[predictionMap$region == "florida", ], 1)
#+end_src

#+RESULTS:
#+begin_example
 quartz_off_screen
                2
 quartz_off_screen
                2
 quartz_off_screen
                2

 :: The predicted probability for Florida:
      region      long      lat group order subregion TestPrediction
1150 florida -85.01548 30.99702     9  1462      <NA>      0.9640395
     TestPredictionBinary Test.State
1150                    1    Florida
#+end_example

Predicted probability: $0.9640395$

*** Question b

What does this imply?

**** Answer

- [ ] Our prediction model did a good job of correctly predicting the
  state of Florida, and we were very confident in our prediction.

- [ ] Our prediction model did a good job of correctly predicting the
  state of Florida, but we were not very confident in the prediction.

- [ ] Our prediction model did not do a very good job of correctly
  predicting the state of Florida, but we were not very confident in
  our prediction.

- [X] Our prediction model did not do a very good job of correctly
  predicting the state of Florida, and we were very confident in our
  incorrect prediction.

*Explanation*

We predicted Republican for the state of Florida with high
probability, meaning that we were very confident in our incorrect
prediction! Historically, Florida is usually a close race, but our
model doesn't know this. The model only uses polling results for the
particular year. For Florida in 2012, Survey USA predicted a tie, but
other polls predicted Republican, so our model predicted Republican.

** DONE Problem 4 - Parameter Settings
CLOSED: [2015-08-31 Mon 09:38]

In this part, we'll explore what the different parameter settings of
~geom_polygon~ do. Throughout the problem, use the help page for
~geom_polygon~, which can be accessed by ?geom_polygon. To see more
information about a certain parameter, just type a question mark and
then the parameter name to get the help page for that
parameter. Experiment with different parameter settings to try and
replicate the plots!

We'll be asking questions about the following three plots:

*Plot (1)*

[[../graphs/ElectionForecastingMap1.png]]

*Plot (2)*

[[../graphs/ElectionForecastingMap2.png]]

*Plot (3)*

[[../graphs/ElectionForecastingMap3.png]]

*** Answer

Recreating *plot (1)*

#+BEGIN_SRC R :var basename="RecreatingPlot01" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPrediction)) +
            geom_polygon(color = "black", linetype = 3) +
            scale_fill_gradient(low = "blue", high = "red", name = "Prediction 2012")
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Recreating plot 1
#+NAME:     fig:RecreatingPlot01
#+ATTR_LaTeX: placement: [H]
[[../graphs/RecreatingPlot01.png]]

Now we will recreate the *Plot (2)*

#+BEGIN_SRC R :var basename="RecreatingPlot02" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPrediction)) +
              geom_polygon(color = "black", size = 3) +
              scale_fill_gradient(low = "blue", high = "red", name = "Prediction 2012")
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Recreating the plot 2
#+NAME:     fig:RecreatingPlot02
#+ATTR_LaTeX: placement: [H]
[[../graphs/RecreatingPlot02.png]]

Now let recreate the *plot (3)*

#+BEGIN_SRC R :var basename="RecreatingPlot03" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
    ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPrediction)) +
                geom_polygon(color = "black", alpha = 0.3) +
                scale_fill_gradient(low = "blue", high = "red", name = "Prediction 2012")
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Recreating the plot 3
#+NAME:     fig:RecreatingPlot03
#+ATTR_LaTeX: placement: [H]
[[../graphs/RecreatingPlot03.png]]

** DONE Problem 4.1 - Parameter Settings (2/2 points)
CLOSED: [2015-08-31 Mon 09:38]

Plots (1) and (2) were created by setting different parameters of
geom_polygon to the value 3.

*** Question

What is the name of the parameter we set to have value 3 to create
plot (1)?

**** Answer

~linetype~

*** Question

What is the name of the parameter we set to have value 3 to create
plot (2)?

**** Answer

~size~

*Explanation*

The first plot can be generated by setting the parameter ~linetype=3~:

~ggplot(predictionMap, aes(x = long, y = lat, group = group, fill =~
~TestPrediction))+~
~geom_polygon(color = "black", linetype=3) +~
~scale_fill_gradient(low = "blue", high = "red", guide~
~= "legend", breaks = c(0,1), labels = c("Democrat", "Republican") , name~
~= "Prediction 2012")~

The second plot can be generated by setting the parameter ~size = 3~:

~ggplot(predictionMap, aes(x = long, y = lat, group = group, fill =~
~TestPrediction))+ geom_polygon(color = "black", size=3) +~
~scale_fill_gradient(low = "blue", high = "red", guide~
~= "legend", breaks= c(0,1), labels = c("Democrat", "Republican"),name~
~= "Prediction 2012")~

** DONE Problem 4.2 - Parameter Settings (1 point possible)
CLOSED: [2015-08-31 Mon 09:38]

*** Question

Plot (3) was created by changing the value of a different geom_polygon
parameter to have value 0.3. Which parameter did we use?

**** Answer

*alpha*

*Explanation*

Plot (3) can be created by changing the alpha parameter:

~ggplot(predictionMap, aes(x = long, y = lat, group = group, fill =~
~TestPrediction))+ geom_polygon(color = "black", alpha=0.3) +~
~scale_fill_gradient(low = "blue", high = "red", guide~
~= "legend", breaks= c(0,1), labels = c("Democrat", "Republican"),name~
~= "Prediction 2012")~

The ~alpha~ parameter controls the transparency or darkness of the
color. A smaller value of alpha will make the colors lighter.

* Visualizing Network Data [0/11]

The cliche goes that the world is an increasingly interconnected
place, and the connections between different entities are often best
represented with a graph. Graphs are comprised of vertices (also often
called "nodes") and edges connecting those nodes. In this assignment,
we will learn how to visualize networks using the ~igraph~ package in
R.

For this assignment, we will visualize social networking data using
anonymized data from Facebook; this data was originally curated in a
[[http://i.stanford.edu/~julian/pdfs/nips2012.pdf][recent paper]] about computing social circles in social networks. In our
visualizations, the vertices in our network will represent Facebook
users and the edges will represent these users being Facebook friends
with each other.

The first file we will use, [[https://courses.edx.org/asset-v1:MITx%2B15.071x_2a%2B2T2015%2Btype@asset%2Bblock/edges.csv][edges.csv]], contains variables ~V1~ and ~V2~,
which label the endpoints of edges in our network. Each row represents
a pair of users in our graph who are Facebook friends. For a pair of
friends A and B, ~edges.csv~ will only contain a single row -- the
smaller identifier will be listed first in this row. From this row, we
will know that A is friends with B and B is friends with A.

The second file, [[https://courses.edx.org/asset-v1:MITx%2B15.071x_2a%2B2T2015%2Btype@asset%2Bblock/users.csv][users.csv]], contains information about the Facebook
users, who are the vertexes in our network. This file contains the
following variables:

- *id*: A unique identifier for this user; this is the value that
  appears in the rows of ~edges.csv~

- *gender*: An identifier for the gender of a user taking the values A
  and B. Because the data is anonymized, we don't know which value
  refers to males and which value refers to females.

- *school*: An identifier for the school the user attended taking the
  values A and AB (users with AB attended school A as well as another
  school B). Because the data is anonymized, we don't know the schools
  represented by A and B.

- *locale*: An identifier for the locale of the user taking the values
  A and B. Because the data is anonymized, we don't know which value
  refers to what locale.

#+BEGIN_SRC R :session :results output :exports all
  library(parallel)

  if(!file.exists("../data")) {
          dir.create("../data")
  }

  fileUrl <-
          c("https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/edges.csv",
          "https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/users.csv")

  fileName <- c("edges.csv", "users.csv")
  dataPath <- "../data"

  for(i in 1:2) {
          filePath <- paste(dataPath, fileName[i], sep = "/")

          if(!file.exists(filePath)) {
                  download.file(fileUrl[i], destfile = filePath, method = "curl")
          }
  }
  writeLines("\n :: Files downloaded...")
#+END_SRC

#+RESULTS:
:  quartz_off_screen
:                 2
:  quartz_off_screen
:                 2
:  quartz_off_screen
:                 2
:
:  :: Files downloaded...

** TODO Problem 1.1 - Summarizing the Data (2 points possible)

Load the data from ~edges.csv~ into a data frame called ~edges~, and load
the data from ~users.csv~ into a data frame called ~users~.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Loading the data sets in dataframes...")
  edges <- read.csv("../data/edges.csv", na.strings = "")
  users <- read.csv("../data/users.csv", na.strings = "")

  writeLines("\n :: Revising the edges dataframe structure:")
  str(edges)

  writeLines("\n :: Revising the users dataframe structure:")
  str(users)
#+end_src

#+RESULTS:
#+begin_example

 :: Loading the data sets in dataframes...

 :: Revising the edges dataframe structure:
'data.frame':	146 obs. of  2 variables:
 $ V1: int  4019 4023 4023 4027 3988 3982 3994 3998 3993 3982 ...
 $ V2: int  4026 4031 4030 4032 4021 3986 3998 3999 3995 4021 ...

 :: Revising the users dataframe structure:
'data.frame':	59 obs. of  4 variables:
 $ id    : int  3981 3982 3983 3984 3985 3986 3987 3988 3989 3990 ...
 $ gender: Factor w/ 2 levels "A","B": 1 2 2 2 2 2 1 2 2 1 ...
 $ school: Factor w/ 2 levels "A","AB": 1 NA NA NA NA 1 NA NA 1 NA ...
 $ locale: Factor w/ 2 levels "A","B": 2 2 2 2 2 2 1 2 2 1 ...
#+end_example

*** Question a

How many Facebook users are there in our dataset?

**** Answer

*59*

*Explanation*

From ~str(users)~ or ~nrow(users)~, we see that there are 59 Facebook
users in this dataset.

*** Question b

In our dataset, what is the average number of friends per user? Hint:
this question is tricky, and it might help to start by thinking about
a small example with two users who are friends.

**** Answer

#+begin_src R :session :results output :exports all
  writeLines("\n :: Number of frienship relationships:")
  nrow(edges)

  writeLines("\n :: Number of users:")
  nrow(users)

  writeLines("\n :: Average number of friends per user:")
  (2 * nrow(edges)) / nrow(users)
#+end_src

#+RESULTS:
:
:  :: Number of frienship relationships:
: [1] 146
:
:  :: Number of users:
: [1] 59
:
:  :: Average number of friends per user:
: [1] 4.949153

*Explanation*

From ~str(edges)~ or ~nrow(edges)~, we see that there are $146 pairs
of users in our dataset who are Facebook friends. However, each pair
$(A, B)$ must be counted twice, because $B$ is a friend of $A$ and $A$
is a friend of $B$. To think of this in simpler terms, consider a
network with just new people, $A$ and $B$, and a single edge $(A,
B)$. Even though there are two vertexes and one edge, each user has on
average one friend.

For our network, the average number of friends per user is

$$
\frac{292}{59} = 4.95
$$

Finally, note that in all likelihood these users have a much higher
number of Facebook friends. We are computing here the average number
of people in this dataset who are their friends, instead of the
average total number of Facebook friends.

** TODO Problem 1.2 - Summarizing the Data (1 point possible)

*** Question

Out of all the students who listed a school, what was the most common
locale?

**** Answer

#+begin_src R :session :results output :exports all
  writeLines("\n :: the most common locale for students who listed a school:")
  subset(users, users$school == "A" | users$school == "AB")

  writeLines("\n :: Other way to answer:")
  table(users$locale, users$school)
#+end_src

#+RESULTS:
#+begin_example

 :: the most common locale for students who listed a school:
     id gender school locale
1  3981      A      A      B
6  3986      B      A      B
9  3989      B      A      B
11 3991      B      A      B
12 3992      A      A      B
16  594      B      A      B
20 3999      B     AB      B
23 4002      B      A      B
26 4005      B      A      B
31 4010      B      A      B
39 4018      A      A      B
40 4019      B      A      B
41 4020   <NA>      A      B
42 4021      B      A      B
44 4023      B      A      B
49 4028      B      A      B
52 4031      B      A      B
55 4034      B      A      B
57 4036      A     AB      B

 :: Other way to answer:

     A AB
  A  0  0
  B 17  2
#+end_example

- [ ] Locale A
- [X] Locale B

*Explanation*

From ~table(users$locale, users$school)~, we read that all students
listed at schools $A$ and $B$ listed their locale as $B$.

** TODO Problem 1.3 - Summarizing the Data (1 point possible)

Is it possible that either school A or B is an all-girls or all-boys
school?

*** Answer

#+begin_src R :session :results output :exports all
  writeLines("\n :: Exploratory analysis of the data frame:")
  head(users, 20)

  writeLines("\n :: Are all students of the same gender in some school?")
  table(users$school, users$gender)
#+end_src

#+RESULTS:
#+begin_example

 :: Exploratory analysis of the data frame:
     id gender school locale
1  3981      A      A      B
2  3982      B   <NA>      B
3  3983      B   <NA>      B
4  3984      B   <NA>      B
5  3985      B   <NA>      B
6  3986      B      A      B
7  3987      A   <NA>      A
8  3988      B   <NA>      B
9  3989      B      A      B
10 3990      A   <NA>      A
11 3991      B      A      B
12 3992      A      A      B
13 3993      B   <NA>   <NA>
14 3994      A   <NA>      B
15 3995      B   <NA>      B
16  594      B      A      B
17 3996      B   <NA>      B
18 3997      B   <NA>      B
19 3998      B   <NA>      B
20 3999      B     AB      B

 :: Are all students of the same gender in some school?

      A  B
  A   3 13
  AB  1  1
#+end_example

- [X] No No - correct
- [ ] Yes

*Explanation*

We see from ~table(users$gender, users$school)~ that both genders $A$
and $B$ have attended schools $A$ and $B$.

** TODO Problem 2.1 - Creating a Network (1 point possible)

We will be using the ~igraph~ package to visualize networks; install
and load this package using the ~install.packages~ and ~library~
commands.

We can create a new graph object using the ~graph.data.frame()~
function. Based on ~?graph.data.frame~, which of the following
commands will create a graph ~g~ describing our social network, with
the attributes of each user correctly loaded?

Note: A directed graph is one where the edges only go one way -- they
point from one vertex to another. The other option is an undirected
graph, which means that the relations between the vertexes are
symmetric.

*** Answer

#+begin_src R :session :results output :exports all
  writeLines("\n :: Install new package: igraph ...")
  ## install.packages('igraph', repos='http://cran.rstudio.com/')
  writeLines("\n :: NOTE: Please comment after install once...")

  library(igraph)
  writeLines("\n :: Library igraph loaded...")
#+end_src

#+RESULTS:
:
:  :: Install new package: igraph ...
:
:  :: NOTE: Please comment after install once...
:
:  :: Library igraph loaded...

- [ ] ~g = graph.data.frame(edges, FALSE, users)~
- [ ] ~g = graph.data.frame(users, FALSE, edges)~
- [ ] ~g = graph.data.frame(edges, TRUE, users)~
- [ ] ~g = graph.data.frame(users, TRUE, edges)~

#+begin_src R :session :results output :exports all
  g <- graph.data.frame(edges, FALSE, users)
  ## g2 <- graph.data.frame(users, FALSE, edges)
  g3 <- graph.data.frame(edges, TRUE, users)
  ## g4 <- graph.data.frame(users, TRUE, edges)

  writeLines("\n :: Our undirected graph g:")
  g
#+end_src

#+RESULTS:
#+begin_example

 :: Our undirected graph g:
 attr: name (v/c), gender (v/c), school (v/c), locale (v/c)
+ edges (vertex names):
 [1] 4019--4026 4023--4031 4023--4030 4027--4032 3988--4021 3982--3986
 [7] 3994--3998 3998--3999 3993--3995 3982--4021 3982--4037 3997--4019
[13] 3994--4019 3992--4017 3981--3998 3997--4018 4009--4030 3994--4018
[19] 3995--4000 4000--4026 4027--4038 4031--4038 4000--4021 3986--4030
[25] 3985--4014 3994--4030 3998--4021 3994--4009 3982--4023 3998--4019
[31] 4020--4031 4009--4023 3994--3997 3981--4023 3997--4030 3997--4021
[37] 4023--4034 3993--4004 3994--3996 4000--4030 3998--4014 4004--4013
[43] 4016--4025 3990--4016 3999--4005 4004--4023 4002--4020 3998--4018
+ ... omitted several edges
#+end_example

*Explanation*

From ~?graph.data.frame~, we can see that the function expects the
first two columns of parameter d to specify the edges in the graph --
our edges object fits this description.

Our edges are undirected -- if $A$ is a Facebook friend of $B$ then
$B$ is a Facebook friend of $A$. Therefore, we set the directed
parameter to ~FALSE~.

The vertices parameter expects a data frame where the first column is
a vertex id and the remaining columns are properties of vertices in
our graph. This is the case with our users data frame.

** TODO Problem 2.2 - Creating a Network (2 points possible)

Use the correct command from Problem 2.1 to load the graph ~g~.

Now, we want to plot our graph. By default, the vertices are large and
have text labels of a user's identifier. Because this would clutter
the output, we will plot with no text labels and smaller vertices:

#+BEGIN_SRC R :var basename="FBFriendshipGraph01" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  plot(g, vertex.size = 5, vertex.label = NA)
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Facebook friendship graph v1
#+NAME:     fig:FBFriendshipGraph01
#+ATTR_LaTeX: placement: [H]
[[../graphs/FBFriendshipGraph01.png]]

In this graph, there are a number of groups of nodes where all the
nodes in each group are connected but the groups are disjoint from one
another, forming *islands* in the graph. Such groups are called
~connected components~, or ~components~ for short.

*** Question a

How many connected components with at least 2 nodes are there in the
graph?

**** Answer

4

*Explanation*

In addition to the large connected component, there is a 4-node
component and two 2-node components.


*** Question b

How many users are there with no friends in the network?

**** Answer

7

*Explanation*

There are 7 nodes that are not connected to any other nodes. Each
forms a 1-node connected component.

** TODO Problem 2.3 - Creating a Network (1 point possible)

In our graph, the *degree* of a node is its number of friends. We have
already seen that some nodes in our graph have degree 0 (these are the
nodes with no friends), while others have much higher degree. We can
use ~degree(g)~ to compute the degree of all the nodes in our graph
~g~.

*** Question

How many users are friends with 10 or more other Facebook users in
this network?

**** Answer

#+begin_src R :session :results output :exports all
  writeLines("\n :: Calculating the degree of users in the graph:")
  sort(degree(g), decreasing = TRUE)

  writeLines("\n :: Other useful way to calculate the degree > 10:")
  table(degree(g) >= 10)
#+end_src

#+RESULTS:
#+begin_example
 quartz_off_screen
                2

 :: Calculating the degree of users in the graph:
4030 4023 3982 3998 4014 3994 3997 4021 4031 4004 4009 3986 3995 4000 4017 4026
  18   17   13   13   11   10   10   10   10    9    9    8    8    8    8    8
4038 3981 4019 4020 3988 4002 4018 4027 3985 3989 3993 4013 4003 3990  594 3996
   8    7    7    7    6    6    6    6    5    5    5    5    4    3    3    3
3999 4007 4011 4016 4025 4037 3991 3992 4005 4033 3983 3987 4001 4006 4012 4028
   3    3    3    3    3    3    2    2    2    2    1    1    1    1    1    1
4029 4032 4034 4036 3984 4008 4010 4015 4022 4024 4035
   1    1    1    1    0    0    0    0    0    0    0

 :: Other useful way to calculate the degree > 10:

FALSE  TRUE
   50     9
#+end_example

9 users have equal or more to 10 friends.

*Explanation*

From ~table(degree(g))~ or ~table(degree(g) >= 10)~, we can see that
there are 9 users with 10 or more friends in this network.

** TODO Problem 2.4 - Creating a Network (2 points possible)

In a network, it's often visually useful to draw attention to
*important* nodes in the network. While this might mean different
things in different contexts, in a social network we might consider a
user with a large number of friends to be an *important user*. From the
previous problem, we know this is the same as saying that nodes with a
high degree are important users.

To visually draw attention to these nodes, we will change the size of
the vertices so the vertices with high degrees are larger. To do this,
we will change the "size" attribute of the vertices of our graph to be
an increasing function of their degrees:

~V(g)$size = degree(g)/ 2 + 2~

Now that we have specified the vertex size of each vertex, we will no
longer use the ~vertex.size~ parameter when we plot our graph:

#+BEGIN_SRC R :var basename="FBFriendshipGraph02" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  V(g)$size = degree(g)/2+2
  plot(g, vertex.label = NA)
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Facebook friendship graph v2
#+NAME:     fig:FBFriendshipGraph02
#+ATTR_LaTeX: placement: [H]
[[../graphs/FBFriendshipGraph02.png]]

*** Question a

What is the largest size we assigned to any node in our graph?

#+begin_src R :session :results output :exports all
  writeLines("\n :: The largest size assigned to a node:")
  sort(V(g)$size, decreasing = TRUE)
#+end_src

#+RESULTS:
:  quartz_off_screen
:                 2
:
:  :: The largest size assigned to a node:
:  [1] 11.0 10.5  8.5  8.5  7.5  7.0  7.0  7.0  7.0  6.5  6.5  6.0  6.0  6.0  6.0
: [16]  6.0  6.0  5.5  5.5  5.5  5.0  5.0  5.0  5.0  4.5  4.5  4.5  4.5  4.0  3.5
: [31]  3.5  3.5  3.5  3.5  3.5  3.5  3.5  3.5  3.0  3.0  3.0  3.0  2.5  2.5  2.5
: [46]  2.5  2.5  2.5  2.5  2.5  2.5  2.5  2.0  2.0  2.0  2.0  2.0  2.0  2.0

*11 is the largest size assigned to a node.*

*** Question b

What is the smallest size we assigned to any node in our graph?

**** Answer

*2 is the smallest size assigned to a node.*

*Explanation*

From ~table(degree(g))~ or ~summary(degree(g))~, we see that the maximum
degree of any node in the graph is $18$ and the minimum degree of any
node is $0$. Therefore, the maximum size of any point is ~18/2+2=11~, and
the minimum size is ~0/2+2=2~.

** TODO Problem 3.1 - Coloring Vertices (1 point possible)

Thus far, we have changed the *size* attributes of our
vertices. However, we can also change the colors of vertices to
capture additional information about the Facebook users we are
depicting.

When changing the size of nodes, we first obtained the vertices of our
graph with ~V(g)~ and then accessed the the size attribute with
~V(g)$size~. To change the color, we will update the attribute
~V(g)$color~.

To color the vertices based on the gender of the user, we will need
access to that variable. When we created our graph ~g~, we provided it
with the data frame users, which had variables ~gender~, ~school~, and
~locale~. These are now stored as attributes ~V(g)$gender~, ~V(g)$school~,
and ~V(g)$locale~.

We can update the colors by setting the color to black for all
vertices, than setting it to red for the vertices with gender A and
setting it to gray for the vertices with gender B:

#+begin_src R :session :results output :exports all
  writeLines("\n :: Assigning black color to the nodes...")
  V(g)$color <- "black"

  writeLines("\n :: Assigning red color to gender A...")
  V(g)$color[V(g)$gender == "A"] <- "red"

  writeLines("\n :: Assigning gray color to gender B...")
  V(g)$color[V(g)$gender == "B"] <- "gray"
#+end_src

#+RESULTS:
:
:  :: Assigning black color to the nodes...
:
:  :: Assigning red color to gender A...
:
:  :: Assigning gray color to gender B...

*** Question

Plot the resulting graph. What is the gender of the users with the
highest degree in the graph?

#+BEGIN_SRC R :var basename="FBFriendshipGraph03" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  plot(g, vertex.label = NA)
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Facebook friendship graph v3
#+NAME:     fig:FBFriendshipGraph03
#+ATTR_LaTeX: placement: [H]
[[../graphs/FBFriendshipGraph03.png]]

**** Answer

- [ ] Missing gender value
- [ ] Gender A
- [X] Gender B

*Explanation*

After updating ~V(g)$color~, run ~plot(g, vertex.label=NA)~ to plot
the graph. All the largest nodes (the ones with the highest degree)
are colored *gray*, which corresponds to Gender *B*.

** TODO Problem 3.2 - Coloring Vertices (2 points possible)

Now, color the vertices based on the school that each user in our
network attended.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Assigning black color to the nodes...")
  V(g)$color <- "black"

  writeLines("\n :: Assigning blue color to school A...")
  V(g)$color[V(g)$school == "A"] <- "blue"

  writeLines("\n :: Assigning red color to school AB...")
  V(g)$color[V(g)$school == "AB"] <- "red"
#+end_src

#+RESULTS:
:  quartz_off_screen
:                 2
:
:  :: Assigning black color to the nodes...
:
:  :: Assigning blue color to school A...
:
:  :: Assigning red color to school AB...

*** Question

#+BEGIN_SRC R :var basename="FBFriendshipGraph04" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  plot(g, vertex.label = NA)
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Facebook friendship graph v4
#+NAME:     fig:FBFriendshipGraph04
#+ATTR_LaTeX: placement: [H]
[[../graphs/FBFriendshipGraph04.png]]

Are the two users who attended both schools A and B Facebook friends
with each other?

**** Answer

- [X] Yes Yes - correct
- [ ] No

*** Question

What best describes the users with highest degree?

- [ ] None of the high-degree users attended school A

- [X] Some, but not all, of the high-degree users attended school A
  Some, but not all, of the high-degree users attended school A -
  correct

- [ ] All of the high-degree users attended school A

*Explanation*

As with coloring by gender, we will set the color for all vertices to
black, and then we will set the color for students from school A to
red and the color for students from schools A and B to gray. Finally
we will plot the updated graph:

~(V(g)$color = "black")~

~(V(g)$color[V(g)$school == "A"] = "red")~

~(V(g)$color[V(g)$school == "AB"] = "gray")~

~plot(g, vertex.label=NA)~

The two students who attended schools A and B are colored gray; we can
see from the graph that they are Facebook friends (aka they are
connected by an edge). The high-degree users (depicted by the large
nodes) are a mixture of red and black color, meaning some of these
users attended school A and other did not.

** TODO Problem 3.3 - Coloring Vertices (2 points possible)

Now, color the vertices based on the locale of the user.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Assigning black color to the nodes...")
  V(g)$color <- "black"

  writeLines("\n :: Assigning green color to locale A...")
  V(g)$color[V(g)$locale == "A"] <- "green"

  writeLines("\n :: Assigning red color to locale B...")
  V(g)$color[V(g)$locale == "B"] <- "red"
#+end_src

#+RESULTS:
:  quartz_off_screen
:                 2
:
:  :: Assigning black color to the nodes...
:
:  :: Assigning green color to locale A...
:
:  :: Assigning red color to locale B...

#+BEGIN_SRC R :var basename="FBFriendshipGraph05" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  plot(g, vertex.label = NA)
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Facebook friendship graph v5
#+NAME:     fig:FBFriendshipGraph05
#+ATTR_LaTeX: placement: [H]
[[../graphs/FBFriendshipGraph05.png]]

*** Question

The large connected component is most associated with which locale?

**** Answer

- [ ] Locale A
- [X] Locale B

*** Question

The 4-user connected component is most associated with which locale?

**** Answer

- [X] Locale A
- [ ] Locale B

*Explanation*

As with the other coloring tasks, we will set the color for all
vertices to black, and then we will set the color for users from
locale A to red and the color for users from locale B to gray. Finally
we will plot the updated graph:

~(V(g)$color = "black")~

~(V(g)$color[V(g)$locale == "A"] = "red")~

~(V(g)$color[V(g)$locale == "B"] = "gray")~

~plot(g, vertex.label=NA)~

Nearly all of the vertices from the large connected component are
colored gray, indicating users from Locale B. Meanwhile, all the
vertices in the 4-user connected component are colored red, indicating
users from Locale A.

** TODO Problem 4 - Other Plotting Options (2 points possible)

The help page is a helpful tool when making visualizations. Answer the
following questions with the help of ~?igraph.plotting~ and
experimentation in your R console.

*** Question

Which ~igraph~ plotting function would enable us to plot our graph in
3-D?

**** Answer

We need to install the ~rgl~ package.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Install new package: rgl ...")
  ## install.packages('rgl', repos='http://cran.rstudio.com/')
  writeLines("\n :: NOTE: Please comment after install once...")

  library(rgl)
  writeLines("\n :: Library rgl loaded...")
#+end_src

#+RESULTS:
:  quartz_off_screen
:                 2
:
:  :: Install new package: rgl ...
:
:  :: NOTE: Please comment after install once...
:
:  :: Library rgl loaded...

Now we can use the ~rglplot~ function:

#+CAPTION:  Facebook friendship graph v6
#+NAME:     fig:FBFriendshipGraph06
#+ATTR_LaTeX: placement: [H]
[[../graphs/FBFriendSGraphRGLPlot.png]]

*** Question

What parameter to the ~plot()~ function would we use to change the edge
width when plotting ~g~?

**** Answer

~edge.width~ is the parameter to control the width of the edges.

#+BEGIN_SRC R :var basename="FBFriendshipGraph07" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  plot(g, vertex.label = NA, edge.width = 3)
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Facebook friendship graph v7
#+NAME:     fig:FBFriendshipGraph07
#+ATTR_LaTeX: placement: [H]
[[../graphs/FBFriendshipGraph07.png]]

*Explanation*

The three functions to plot the ~igraph~ are ~plot.igraph~ (the function
we used through the command ~plot~), ~tkplot~, and ~rglplot~. ~rglplot~
makes 3D plots -- you can try one with

~rglplot(g, vertex.label=NA)~

Once you've made the plot, you can click and drag to rotate the
graph. To use this function, you will need to install and load the
~rgl~ package.

To change the edge width, you need to change the edge parameter called
~width~. From ~?igraph.plotting~, we read that we need to append the
prefix ~edge~. to the beginning for our call to plot, so the full
parameter is called ~edge.width~. For instance, we could plot with
edge width 2 with the command

~plot(g, edge.width=2, vertex.label=NA)~.

* Visualizing Text Data Using Word CLouds [0/15]

Earlier in the course, we used text analytics as a predictive tool,
using word frequencies as independent variables in our
models. However, sometimes our goal is to understand commonly
occurring topics in text data instead of to predict the value of some
dependent variable. In such cases, word clouds can be a visually
appealing way to display the most frequent words in a body of text.

A word cloud arranges the most common words in some text, using size
to indicate the frequency of a word. For instance, this is a word
cloud for the complete works of Shakespeare, removing English
~stopwords~:

[[../graphs/shakespeare.png]]

While we could generate word clouds using free generators available on
the Internet, we will have more flexibility and control over the
process if we do so in R. We will visualize the text of tweets about
Apple, a dataset we used earlier in the course. As a reminder, this
dataset (which can be downloaded from [[https://courses.edx.org/asset-v1:MITx%2B15.071x_2a%2B2T2015%2Btype@asset%2Bblock/tweets.csv][tweets.csv]]) has the following
variables:

- *Tweet* -- the text of the tweet

- *Avg* -- the sentiment of the tweet, as assigned by users of Amazon
  Mechanical Turk. The score ranges on a scale from -2 to 2, where 2
  means highly positive sentiment, -2 means highly negative sentiment,
  and 0 means neutral sentiment.

** TODO Problem 1.1 - Preparing the Data (1 point possible)

Download the dataset ~tweets.csv~, and load it into a data frame
called ~tweets~ using the ~read.csv()~ function, remembering to use
~stringsAsFactors = FALSE~ when loading the data.

#+BEGIN_SRC R :session :results output :exports all
  library(parallel)

  if(!file.exists("../data")) {
          dir.create("../data")
  }

  fileUrl <- "https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/tweets.csv"
  fileName <- "tweetsU7.csv"
  dataPath <- "../data"

  filePath <- paste(dataPath, fileName, sep = "/")

  if(!file.exists(filePath)) {
          download.file(fileUrl, destfile = filePath, method = "curl")
  }

  writeLines("\n :: File downloaded...")
#+END_SRC

#+RESULTS:
:  quartz_off_screen
:                 2
:
:  :: File downloaded...

Load in a data frame

#+begin_src R :session :results output :exports all
  writeLines("\n :: Loading the dataset in a dataframe...")
  tweets <- read.csv("../data/tweetsU7.csv", stringsAsFactors = FALSE)
  str(tweets)
#+end_src

#+RESULTS:
:
:  :: Loading the dataset in a dataframe...
: 'data.frame':	1181 obs. of  2 variables:
:  $ Tweet: chr  "I have to say, Apple has by far the best customer care service I have ever received! @Apple @AppStore" "iOS 7 is so fricking smooth & beautiful!! #ThanxApple @Apple" "LOVE U @APPLE" "Thank you @apple, loving my new iPhone 5S!!!!!  #apple #iphone5S pic.twitter.com/XmHJCU4pcb" ...
:  $ Avg  : num  2 2 1.8 1.8 1.8 1.8 1.8 1.6 1.6 1.6 ...

Loading some useful libraries

#+begin_src R :session :results output :exports all
  library(tm)
  ## library(SnowballC)
  ## library(caTools)
  ## library(rpart)
  ## library(rpart.plot)
  ## library(randomForest)
  ## library(ROCR)
#+end_src

#+RESULTS:

Next, perform the following pre-processing tasks (like we did in Unit
5), noting that we don't stem the words in the document or remove
sparse terms:

1) Create a corpus using the Tweet variable

#+begin_src R :session :results output :exports all
  corpus <- Corpus(VectorSource(tweets$Tweet))
  corpus[[1]]
#+end_src

#+RESULTS:
: <<PlainTextDocument>>
: Metadata:  7
: Content:  chars: 101

2) Convert the corpus to lowercase (don't forget to type "corpus =
   tm_map(corpus, PlainTextDocument)" in your R console right after
   this step)

#+begin_src R :session :results output :exports all
  corpus <- tm_map(corpus, tolower)
  corpus <- tm_map(corpus, PlainTextDocument)
#+end_src

#+RESULTS:

3) Remove punctuation from the corpus

#+begin_src R :session :results output :exports all
  corpus <- tm_map(corpus, removePunctuation)
  corpus[[1]]
#+end_src

#+RESULTS:
: <<PlainTextDocument>>
: Metadata:  7
: Content:  chars: 97

4) Remove all English-language stopwords

#+begin_src R :session :results output :exports all
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
#+end_src

#+RESULTS:

5) Build a document-term matrix out of the corpus

#+begin_src R :session :results output :exports all
  DocTermMatrix <- DocumentTermMatrix(corpus)
#+end_src

#+RESULTS:

6) Convert the document-term matrix to a data frame called allTweets

#+begin_src R :session :results output :exports all
  allTweets <- as.data.frame(as.matrix(DocTermMatrix))
#+end_src

#+RESULTS:

*** Question

How many unique words are there across all the documents?

#+begin_src R :session :results output :exports all
  writeLines("\n :: allTweets dataframe matrix portion:")
  allTweets[1:13, 1:13]

  writeLines("\n :: Unique words across all documents:")
  ncol(allTweets)
#+end_src

#+RESULTS:
#+begin_example

 :: allTweets dataframe matrix portion:
                000 075 0909 0910 099 100 100m 1085 10min 110 13apple 13th 1415
character(0)      0   0    0    0   0   0    0    0     0   0       0    0    0
character(0).1    0   0    0    0   0   0    0    0     0   0       0    0    0
character(0).2    0   0    0    0   0   0    0    0     0   0       0    0    0
character(0).3    0   0    0    0   0   0    0    0     0   0       0    0    0
character(0).4    0   0    0    0   0   0    0    0     1   0       0    0    0
character(0).5    0   0    0    0   0   0    0    0     0   0       0    0    0
character(0).6    0   0    0    0   0   0    0    0     0   0       0    0    0
character(0).7    0   0    0    0   0   0    0    0     0   0       0    0    0
character(0).8    0   0    0    0   0   0    0    0     0   0       0    0    0
character(0).9    0   0    0    0   0   0    0    0     0   0       0    0    0
character(0).10   0   0    0    0   0   0    0    0     0   0       0    0    0
character(0).11   0   0    0    0   0   0    0    0     0   0       0    0    0
character(0).12   0   0    0    0   0   0    0    0     0   0       0    0    0

 :: Unique words across all documents:
[1] 3780
#+end_example

**** Answer

3780

*Explanation*

We can complete the pre-processing steps with the following commands:

~library(tm)~

~tweets = read.csv("tweets.csv", stringsAsFactors=FALSE)~

~corpus = Corpus(VectorSource(tweets$Tweet))~

~corpus = tm_map(corpus, tolower)~

~corpus = tm_map(corpus, PlainTextDocument)~

~corpus = tm_map(corpus, removePunctuation)~

~corpus = tm_map(corpus, removeWords, stopwords("english"))~

~frequencies = DocumentTermMatrix(corpus)~

~allTweets = as.data.frame(as.matrix(frequencies))~

From the commands ~frequencies~, ~str(allTweets)~ or
~ncol(allTweets)~, we can read that there are $3780$ unique words across
all the tweets.

** TODO Problem 1.2 - Preparing the Data (1 point possible)

Although we typically stem words during the text preprocessing step,
we did not do so here.

*** Question

What is the most compelling rationale for skipping this step when
visualizing text data?

**** Answer

- [ ] It avoids the computational burden of stemming

- [X] It will be easier to read and understand the word cloud if it
  includes full words instead of just the word stems

- [ ] We would not be able to create a word cloud if we stemmed the
  document

*Explanation*

We want to create an interpretable display of a document's contents,
and our results will be easier to read if they include full words
instead of just the stems.

Stemming has relatively minor computational burden, and we certainly
could create a word cloud with a stemmed document.

** TODO Problem 2.1 - Building a Word Cloud (1 point possible)

Install and load the ~wordcloud~ package, which is needed to build
word clouds.

#+begin_src R :session :results output :exports all
  writeLines("\n :: Install new package: wordcloud ...")
  ## install.packages('wordcloud', repos='http://cran.rstudio.com/')
  writeLines("\n :: NOTE: Please comment after install once...")

  library(wordcloud)
  writeLines("\n :: Library wordcloud loaded...")
#+end_src

#+RESULTS:
:
:  :: Install new package: wordcloud ...
:
:  :: NOTE: Please comment after install once...
:
:  :: Library wordcloud loaded...

As we can read from ~?wordcloud~, we will need to provide the function
with a vector of words and a vector of word frequencies. Which
function can we apply to ~allTweets~ to get a vector of the words in our
dataset, which we'll pass as the first argument to ~wordcloud()~?

#+begin_src R :session :results output :exports all
  writeLines("\n :: We can pass the column names:")
  head(colnames(allTweets), 50)

  writeLines("\n :: Is the same as passing the names of the features:")
  head(names(allTweets), 50)
#+end_src

#+RESULTS:
#+begin_example

 :: We can pass the column names:
 [1] "000"           "075"           "0909"          "0910"
 [5] "099"           "100"           "100m"          "1085"
 [9] "10min"         "110"           "13apple"       "13th"
[13] "1415"          "16gb"          "16gbs"         "180"
[17] "18092013"      "18th"          "199"           "1am"
[21] "1jazzyjeff"    "1st"           "200"           "2000ad"
[25] "2001"          "2002"          "2004"          "2005"
[29] "2011with"      "2013"          "2014"          "20th"
[33] "211"           "21st"          "22000"         "22nd"
[37] "244tsuyoponzu" "2nd"           "2shaneez"      "2week"
[41] "300"           "30aud"         "30mins"        "320k"
[45] "350"           "3gs"           "3rd"           "3yr"
[49] "40000"         "40k"

 :: Is the same as passing the names of the features:
 [1] "000"           "075"           "0909"          "0910"
 [5] "099"           "100"           "100m"          "1085"
 [9] "10min"         "110"           "13apple"       "13th"
[13] "1415"          "16gb"          "16gbs"         "180"
[17] "18092013"      "18th"          "199"           "1am"
[21] "1jazzyjeff"    "1st"           "200"           "2000ad"
[25] "2001"          "2002"          "2004"          "2005"
[29] "2011with"      "2013"          "2014"          "20th"
[33] "211"           "21st"          "22000"         "22nd"
[37] "244tsuyoponzu" "2nd"           "2shaneez"      "2week"
[41] "300"           "30aud"         "30mins"        "320k"
[45] "350"           "3gs"           "3rd"           "3yr"
[49] "40000"         "40k"
#+end_example

*** Answer

- [ ] str
- [ ] rownames
- [X] colnames

*Explanation*

Each tweet represents a row of ~allTweets~, and each word represents a
column. We need the names of all the columns of ~allTweets~, which is
returned by ~colnames(allTweets)~. While ~str(allTweets)~ displays the
names of the variables along with other information, it doesn't return
a vector that we can use as the first argument to ~wordcloud()~.

** TODO Problem 2.2 - Building a Word Cloud (1 point possible)

Which function should we apply to ~allTweets~ to obtain the frequency of
each word across all tweets?

#+begin_src R :session :results output :exports all
  head(colSums(allTweets), 50)
#+end_src

#+RESULTS:
#+begin_example
          000           075          0909          0910           099
            1             3             1             1             1
          100          100m          1085         10min           110
            2             1             3             1             1
      13apple          13th          1415          16gb         16gbs
            1             1             1             1             1
          180      18092013          18th           199           1am
            1             1             2             3             1
   1jazzyjeff           1st           200        2000ad          2001
            1             1             2             1             1
         2002          2004          2005      2011with          2013
            1             1             2             1             4
         2014          20th           211          21st         22000
            3             4             1             1             1
         22nd 244tsuyoponzu           2nd      2shaneez         2week
            1             6             2             1             1
          300         30aud        30mins          320k           350
            2             1             1             1             1
          3gs           3rd           3yr         40000           40k
            1             1             1             1             1
#+end_example

*** Answer

- [X] colSums
- [ ] rowSums
- [ ] sum

*Explanation*

Each tweet represents a row in ~allTweets~, and each word represents a
column. Therefore, we need to access the sums of each column in
~allTweets~, which is returned by ~colSums(allTweets)~.

** TODO Problem 2.3 - Building a Word Cloud (1 point possible)

Use ~allTweets~ to build a word cloud. Make sure to check out the help
page for ~wordcloud~ if you are not sure how to do this.

Because we are plotting a large number of words, you might get
warnings that some of the words could not be fit on the page and were
therefore not plotted -- this is especially likely if you are using a
smaller screen.

You can address these warnings by plotting the words smaller. From
~?wordcloud~, we can see that the ~scale~ parameter controls the sizes
of the plotted words. By default, the sizes range from 4 for the most
frequent words to 0.5 for the least frequent, as denoted by the
parameter ~scale=c(4, 0.5)~. We could obtain a much smaller plot with,
for instance, parameter ~scale=c(2, 0.25)~.

#+BEGIN_SRC R :var basename="AppleTweetsWordCloud" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  wordcloud(names(allTweets), colSums(allTweets))
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Apple tweets wordcloud v1
#+NAME:     fig:AppleTweetsWordCloud
#+ATTR_LaTeX: placement: [H]
[[../graphs/AppleTweetsWordCloud.png]]

What is the most common word across all the tweets (it will be the
largest in the outputted word cloud)? Please type the word exactly how
you see it in the word cloud. The most frequent word might not be
printed if you got a warning about words being cut off -- if this
happened, be sure to follow the instructions in the paragraph above.

*Explanation*

We can output the word cloud with:

~wordcloud(colnames(allTweets), colSums(allTweets))~

For smaller words, we could have used:

~wordcloud(colnames(allTweets), colSums(allTweets), scale=c(2, .25))~

*apple* is by far the largest, and therefore most common, word.

** TODO Problem 2.4 - Building a Word Cloud (1 point possible)

In the previous subproblem, we noted that there is one word with a
much higher frequency than the other words. Repeat the steps to load
and pre-process the corpus, this time removing the most frequent word
in addition to all elements of stopwords("english") in the call to
tm_map with removeWords. For a refresher on how to remove this
additional word, see the Twitter text analytics lecture.

#+begin_src R :session :results output :exports all
  writeLines("\n :: 1) Create a corpus using the Tweet variable...")
  corpus <- Corpus(VectorSource(tweets$Tweet))

  writeLines("\n :: 2) Convert the corpus to lowercase...")
  corpus <- tm_map(corpus, tolower)
  corpus <- tm_map(corpus, PlainTextDocument)

  writeLines("\n :: 3) Remove punctuation from the corpus...")
  corpus <- tm_map(corpus, removePunctuation)

  writeLines("\n :: 4) Remove all English-language stopwords...")
  corpus <- tm_map(corpus, removeWords, c("apple", stopwords("english")))

  writeLines("\n :: 5) Build a document-term matrix out of the corpus...")
  DocTermMatrix <- DocumentTermMatrix(corpus)

  writeLines("\n :: 6) Convert the document-term matrix to a data frame called allTweets...")
  allTweets <- as.data.frame(as.matrix(DocTermMatrix))
#+end_src

#+RESULTS:
#+begin_example
 quartz_off_screen
                2

 :: 1) Create a corpus using the Tweet variable...

 :: 2) Convert the corpus to lowercase...

 :: 3) Remove punctuation from the corpus...

 :: 4) Remove all English-language stopwords...

 :: 5) Build a document-term matrix out of the corpus...

 :: 6) Convert the document-term matrix to a data frame called allTweets...
#+end_example

Replace allTweets with the document-term matrix of this new corpus --
we will use this updated corpus for the remainder of the assignment.

#+begin_src R :session :results output :exports all
  DocTermMatrix <- DocumentTermMatrix(corpus)
  allTweets <- as.data.frame(as.matrix(DocTermMatrix))
#+end_src

#+RESULTS:

Create a word cloud with the updated corpus. What is the most common
word in this new corpus (the largest word in the outputted word
cloud)? The most frequent word might not be printed if you got a
warning about words being cut off -- if this happened, be sure to
follow the instructions in the previous problem.

#+BEGIN_SRC R :var basename="AppleTweetsWordcloud02" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  wordcloud(names(allTweets), colSums(allTweets))
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Apple tweets wordcloud v2
#+NAME:     fig:AppleTweetsWordcloud02
#+ATTR_LaTeX: placement: [H]
[[../graphs/AppleTweetsWordcloud02.png]]

*** Answer

*iphone*

*Explanation*

We can do the specified update with the following commands:

~tweets = read.csv("tweets.csv", stringsAsFactors=FALSE)~

~corpus = Corpus(VectorSource(tweets$Tweet))~

~corpus = tm_map(corpus, tolower)~

~corpus = tm_map(corpus, removePunctuation)~

~corpus = tm_map(corpus, removeWords, c("apple", stopwords("english")))~

~frequencies = DocumentTermMatrix(corpus)~

~allTweets = as.data.frame(as.matrix(frequencies))~

~wordcloud(colnames(allTweets), colSums(allTweets))~

For a much smaller plot, we could have used:

~wordcloud(colnames(allTweets), colSums(allTweets), scale=c(2, 0.25))~

The most common (largest) word is now *iphone*.

** TODO Problem 3 - Size and Color

So far, the word clouds we've built have not been too visually
appealing -- they are crowded by having too many words displayed, and
they don't take advantage of color. One important step to building
visually appealing visualizations is to experiment with the parameters
available, which in this case can be viewed by typing ~?wordcloud~ in
your R console. In this problem, you should look through the help page
and experiment with different parameters to answer the questions.

Below are four word clouds, each of which uses different parameter
settings in the call to the ~wordcloud()~ function:

*Word Cloud A*:

[[../graphs/wordcloudA.png]]

*Word Cloud B*:

[[../graphs/wordcloudB.png]]

*Word Cloud C*:

[[../graphs/wordcloudC.png]]

*Word Cloud D*:

[[../graphs/wordcloudD.png]]

** TODO Problem 3.1 - Size and Color (1 point possible)

*** Question

Which word cloud is based only on the negative tweets (tweets with Avg
value -1 or less)?

#+BEGIN_SRC R :var basename="AppleTweetsWordcloud03" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  negativeTweets = subset(allTweets, tweets$Avg <= -1)
  wordcloud(colnames(negativeTweets), colSums(negativeTweets))
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Apple tweets wordcloud v3
#+NAME:     fig:AppleTweetsWordcloud03
#+ATTR_LaTeX: placement: [H]
[[../graphs/AppleTweetsWordcloud03.png]]

**** Answer

- [ ] Word Cloud A
- [ ] Word Cloud B
- [X] Word Cloud C
- [ ] Word Cloud D

*Explanation*

Word Cloud C is the only one with a different distribution of the most
frequent words -- negative words (or censored versions of negative
words) are much more common in this cloud.

It is quite simple to obtain a word cloud that is limited to a subset
of the tweets using the subset function:

~negativeTweets = subset(allTweets, tweets$Avg <= -1)~

~wordcloud(colnames(negativeTweets), colSums(negativeTweets))~

** TODO Problem 3.2 - Size and Color (1 point possible)

*** Question

#+BEGIN_SRC R :var basename="AppleTweetsWordcloud04" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  wordcloud(colnames(allTweets), colSums(allTweets))
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Apple tweets wordcloud v4
#+NAME:     fig:AppleTweetsWordcloud04
#+ATTR_LaTeX: placement: [H]
[[../graphs/AppleTweetsWordcloud04.png]]

Only one word cloud was created without modifying parameters min.freq
or max.words. Which word cloud is this?

**** Answer

- [X] Word Cloud A
- [ ] Word Cloud B
- [ ] Word Cloud C
- [ ] Word Cloud D

*Explanation*

~min.freq~ and ~max.words~ are parameters that can be used to remove
the least frequent words, resulting is a less cluttered word
cloud. Word Cloud A is much more cluttered than the others because it
did not use either of these parameters, and therefore is displaying
every word that appears more than 3 times.

** TODO Problem 3.3 - Size and Color (1 point possible)

*** Question

#+BEGIN_SRC R :var basename="AppleTweetsWordcloud05" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  wordcloud(colnames(allTweets), colSums(allTweets), random.order = FALSE)
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Apple tweets wordcloud v5
#+NAME:     fig:AppleTweetsWordcloud05
#+ATTR_LaTeX: placement: [H]
[[../graphs/AppleTweetsWordcloud05.png]]

Which word clouds were created with parameter ~random.order~ set to
~FALSE~?

**** Answer

- [ ] Word Cloud A
- [X] Word Cloud B
- [ ] Word Cloud C
- [X] Word Cloud D

*Explanation*

If ~random.order~ is set to ~FALSE~, then the most frequent (largest)
words will be plotted first, resulting in them being displayed
together in the center of the word cloud. This is the case in Word
Cloud B and Word Cloud D.

** TODO Problem 3.4 - Size and Color (1 point possible)

*** Question

#+BEGIN_SRC R :var basename="AppleTweetsWordcloud06" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  wordcloud(colnames(allTweets), colSums(allTweets), rot.per = 0.1)
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Apple tweets wordcloud v6
#+NAME:     fig:AppleTweetsWordcloud06
#+ATTR_LaTeX: placement: [H]
[[../graphs/AppleTweetsWordcloud06.png]]

Which word cloud was built with a non-default value for parameter
~rot.per~?

**** Answer

- [X] Word Cloud A
- [ ] Word Cloud B
- [ ] Word Cloud C
- [ ] Word Cloud D

*Explanation*

~rot.per~ controls the proportion of words that are rotated to be
vertical in the word cloud. By default $10\%$ of words are
rotated. However in Word Cloud A a much higher proportion ($50\%$) are
rotated, which was achieved by setting ~rot.per=0.5~.

** TODO Problem 3.5 - Size and Color (1 point possible)

*** Question

#+BEGIN_SRC R :var basename="AppleTweetsWordcloud07" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  wordcloud(names(allTweets), colSums(allTweets), colors=brewer.pal(9,
                                                   "Blues"), random.color = TRUE)
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Apple tweets wordcloud v7
#+NAME:     fig:AppleTweetsWordcloud07
#+ATTR_LaTeX: placement: [H]
[[../graphs/AppleTweetsWordcloud07.png]]

In Word Cloud C and Word Cloud D, we provided a color palette ranging
from light purple to dark purple as the parameter colors (you will
learn how to make such a color palette later in this assignment). For
which word cloud was the parameter ~random.color~ set to ~TRUE~?

**** Answer

- [ ] Word Cloud C
- [X] Word Cloud D

*Explanation*

When ~random.color~ is set to ~TRUE~, the words will be colored
randomly. This is the case in Word Cloud D. Meanwhile, colors were
assigned based on the number of appearances in Word Cloud C.

** TODO Problem 4.1 - Selecting a Color Palette (1 point possible)

The use of a palette of colors can often improve the overall effect of
a visualization. We can easily select our own colors when plotting;
for instance, we could pass ~c("red", "green", "blue")~ as the colors
parameter to ~wordcloud()~. The ~RColorBrewer~ package, which is based on
the ~ColorBrewer~ project (~colorbrewer.org~), provides pre-selected
palettes that can lead to more visually appealing images. Though these
palettes are designed specifically for coloring maps, we can also use
them in our word clouds and other visualizations.

Begin by installing and loading the ~RColorBrewer~ package. This
package may have already been installed and loaded when you installed
and loaded the ~wordcloud~ package, in which case you don't need to go
through this additional installation step. If you obtain errors (for
instance, ~Error: lazy-load database 'P' is corrupt~) after installing
and loading the ~RColorBrewer~ package and running some of the commands,
try closing and re-opening R.

The function ~brewer.pal()~ returns color palettes from the ~ColorBrewer~
project when provided with appropriate parameters, and the function
~display.brewer.all()~ displays the palettes we can choose from.

#+BEGIN_SRC R :var basename="ColorBrewerPalette" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  display.brewer.all()
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  ColorBrewer package palette
#+NAME:     fig:ColorBrewerPalette
#+ATTR_LaTeX: placement: [H]
[[../graphs/ColorBrewerPalette.png]]

Which color palette would be most appropriate for use in a word cloud
for which we want to use color to indicate word frequency?

*** Answer

- [ ] Accent
- [ ] Set2
- [X] YlOrRd

*Explanation*

From ~?brewer.pal~ we read that ~Accent~ and ~Set2~ are both
*qualitative palettes*, which means color changes don't imply a change
in magnitude (we can also see this in the output of
~display.brewer.all()~). As a result, the colors selected would not
visually identify the least and most frequent words.

On the other hand, ~YlOrRd~ is a *sequential palette*, with earlier
colors begin lighter and later colors being darker. Therefore, it is a
good palette choice for indicating low-frequency vs. high-frequency
words.

** TODO Problem 4.2 - Selecting a Color Palette (1 point possible)

*** Question

Which ~RColorBrewer~ palette name would be most appropriate to use when
preparing an image for a document that must be in grayscale?

**** Answer

*Greys*

*Explanation*

As we can see from display.brewer.all(), palette "Greys" is the only
one completely in grayscale.

** TODO Problem 4.3 - Selecting a Color Palette (1 point possible)

In sequential palettes, sometimes there is an undesirably large
contrast between the lightest and darkest colors. You can see this
effect when plotting a word cloud for ~allTweets~ with parameter
~colors=brewer.pal(9, "Blues")~, which returns a sequential blue palette
with 9 colors.

#+BEGIN_SRC R :var basename="AppleTweetsWordcloud08" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  wordcloud(names(allTweets), colSums(allTweets), colors = brewer.pal(9, "Blues"))
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Apple tweets wordcloud palette
#+NAME:     fig:AppleTweetsWordcloud08
#+ATTR_LaTeX: placement: [H]
[[../graphs/AppleTweetsWordcloud08.png]]

*** Question

#+BEGIN_SRC R :var basename="AppleTweetsWordcloud09" :session :results none silent :exports none
  filename <- paste("../graphs/", basename, ".png", sep = "")

  png(filename = filename, bg = "white", width = 640, height = 480, units = "px")

  ## ----- Plot code begin here
  wordcloud(names(allTweets), colSums(allTweets),
            colors = brewer.pal(9, "Blues")[c(5, 6, 7, 8, 9)])
  ## ----- Plot code ends here

  ## Close the PNG device and plots
  dev.off()
#+END_SRC

#+CAPTION:  Better palette for our wordcloud
#+NAME:     fig:AppleTweetsWordcloud09
#+ATTR_LaTeX: placement: [H]
[[../graphs/AppleTweetsWordcloud09.png]]

Which of the following commands addresses this issue by removing the
first 4 elements of the 9-color palette of blue colors? Select all
that apply.

**** Answer

- [ ] ~brewer.pal(9, "Blues")[c(-5, -6, -7, -8, -9)]~
- [X] ~brewer.pal(9, "Blues")[c(-1, -2, -3, -4)]~
- [ ] ~brewer.pal(9, "Blues")[c(1, 2, 3, 4)]~
- [X] ~brewer.pal(9, "Blues")[c(5, 6, 7, 8, 9)]~

*Explanation*

The fourth option limits to elements 5-9, which removes the first
four. The second option uses negative indexes, which means remove
elements 1-4. The first and third options actually keep colors 1-4,
discarding the rest.

A shorthand for this indexing is:

~brewer.pal(9, "Blues")[-1:-4]~

~brewer.pal(9, "Blues")[5:9]~

* Visualizing Attributes of Parole Violators (OPTIONAL) [0/12]

IMPORTANT NOTE: This problem is optional, and will not count towards
your grade. We have created this problem to give you extra practice
with the topics covered in this unit.

In the crime lecture, we saw how we can use heatmaps to give a 2-dimensional representation of 3-dimensional data: we made heatmaps of crime counts by time of the day and day of the week. In this problem, we'll learn how to use histograms to show counts by one variable, and then how to visualize 3 dimensions by creating multiple histograms.

We'll use the parole data parole.csv from Unit 3. Before, we used this data to predict parole violators. Now, let's try to get a little more insight into this dataset using histograms. As a reminder, the variables in this dataset are:

    male = 1 if the parolee is male, 0 if female

    race = 1 if the parolee is white, 2 otherwise

    age = the parolee's age in years at the time of release from prison

    state = a code for the parolee's state. 2 is Kentucky, 3 is Louisiana, 4 is Virginia, and 1 is any other state. These three states were selected due to having a high representation in the dataset.

    time.served = the number of months the parolee served in prison (limited by the inclusion criteria to not exceed 6 months).

    max.sentence = the maximum sentence length for all charges, in months (limited by the inclusion criteria to not exceed 18 months).

    multiple.offenses = 1 if the parolee was incarcerated for multiple offenses, 0 otherwise.

    crime = a code for the parolee's main crime leading to incarceration. 2 is larceny, 3 is drug-related crime, 4 is driving-related crime, and 1 is any other crime.

    violator = 1 if the parolee violated the parole, and 0 if the parolee completed the parole without violation.

** TODO Problem 1.1 - Loading the Data

Using the read.csv function, load the dataset parole.csv and call it parole. Since male, state, and crime are all unordered factors, convert them to factor variables using the following commands:

parole$male = as.factor(parole$male)

parole$state = as.factor(parole$state)

parole$crime = as.factor(parole$crime)

What fraction of parole violators are female?
- unanswered

0.1794872

Explanation

This can be found by using table:

table(parole$male, parole$violator)

The total number of violators is 78, and 14 of them are female.
You have used 0 of 3 submissions
** TODO Problem 1.2 - Loading the Data

In this dataset, which crime is the most common in Kentucky?
Larceny
Drug-related crime
Driving-related crime
Other
- unanswered

Explanation

This can be found by using table:

table(parole$state, parole$crime)

The code 2 corresponds to Kentucky, and the most common crime is 3, which corresponds to Drug-related crime.
You have used 0 of 1 submissions
** TODO Problem 2.1 - Creating a Basic Histogram

Recall from lecture that in ggplot, we need to specify the dataset, the aesthetic, and the geometry. To create a histogram, the geometry will be geom_histogram. The data we'll use is parole, and the aesthetic will be the map from a variable to the x-axis of the histogram.

Create a histogram to find out the distribution of the age of parolees, by typing the following command in your R console (you might need to load the ggplot2 package first by typing library(ggplot2) in your R console):

ggplot(data = parole, aes(x = age)) + geom_histogram()

By default, geom_histogram divides the data into 30 bins. Change the width of the bins to 5 years by adding the argument "binwidth = 5" to geom_histogram.

Note that by default, histograms create bins where the left endpoint is included in the bin, but the right endpoint isn't. So the first bin in this histogram represents parolees who are between 15 and 19 years old. The last bin in this histogram represents parolees who are between 65 and 69 years old.

What is the age bracket with the most parolees?
20-24
25-29
30-34
35-39
- unanswered

Explanation

You can generate the histogram with a bin width of 5 with the command:

ggplot(data = parole, aes(x = age)) + geom_histogram(binwidth=5)

The tallest bar corresponds to the age bracket with the most parolees, which is 20-24.
You have used 0 of 1 submissions
** TODO Problem 2.2 - Creating a Basic Histogram

Redo the histogram, adding the following argument to the geom_histogram function: color="blue". What does this do? Select all that apply.
Changes the fill color of the bars
Changes the background color of the plot
Changes the outline color of the bars
Changes the color of the axis labels
- unanswered

Explanation

You can generate the histogram by typing:

ggplot(data = parole, aes(x = age)) + geom_histogram(binwidth=5, color="blue")

Adding the color argument changes the outline color of the bars.
You have used 0 of 2 submissions
** TODO Problem 3.1 - Adding Another Dimension

Now suppose we are interested in seeing how the age distribution of male parolees compares to the age distribution of female parolees.

One option would be to create a heatmap with age on one axis and male (a binary variable in our data set) on the other axis. Another option would be to stick with histograms, but to create a separate histogram for each gender. ggplot has the ability to do this automatically using the facet_grid command.

To create separate histograms for male and female, type the following command into your R console:

ggplot(data = parole, aes(x = age)) + geom_histogram(binwidth = 5) + facet_grid(male ~ .)

The histogram for female parolees is shown at the top, and the histogram for male parolees is shown at the bottom.

What is the age bracket with the most female parolees?
20-24
25-29
30-34
35-39
- unanswered

Explanation

Looking at the histogram at the top, we can see that the tallest bar corresponds to the age bracket 35-39.
You have used 0 of 1 submissions
** TODO Problem 3.2 - Adding Another Dimension

Now change the facet_grid argument to be ".~male" instead of "male~.". What does this do?
Creates histograms of the male variable, sorted by the different values of age.
Puts the histograms side-by-side instead of on top of each other.
This doesn't change anything - the plot looks exactly the same as it did before.
- unanswered

Explanation

You can create the new plot with the command:

ggplot(data = parole, aes(x = age)) + geom_histogram(binwidth = 5) + facet_grid(.~male)

This puts the plots side-by-side instead of on top of each other.
You have used 0 of 1 submissions
** TODO Problem 3.3 - Adding Another DImension

An alternative to faceting is to simply color the different groups differently. To color the data points by group, we need to tell ggplot that a property of the data (male or not male) should be translated to an aesthetic property of the histogram. We can do this by setting the fill parameter within the aesthetic to male.

Run the following command in your R console to produce a histogram where data points are colored by group:

ggplot(data = parole, aes(x = age, fill = male)) + geom_histogram(binwidth = 5)

Since we didn't specify colors to use, ggplot will use its default color selection. Let's change this by defining our own color palette. First, type in your R console:

colorPalette = c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

This is actually a colorblind-friendly palette, desribed on this Cookbook for R page. Now, generate your histogram again, using colorPalette, with the following command:

ggplot(data = parole, aes(x = age, fill = male)) + geom_histogram(binwidth = 5) + scale_fill_manual(values=colorPalette)

What color is the histogram for the female parolees?
Orange
Black
- unanswered

Explanation

From the previous question, we saw that the female parolee histogram was much smaller than the male parolee histogram. So it looks like the female histogram is the black-colored one. We can also read this from the legend.
You have used 0 of 1 submissions
** TODO Problem 3.4 - Adding Another Dimension

Coloring the groups differently is a good way to see the breakdown of age by sex within the single, aggregated histogram. However, the bars here are stacked, meaning that the height of the orange bars in each age bin represents the total number of parolees in that age bin, not just the number of parolees in that group.

An alternative to a single, stacked histogram is to create two histograms and overlay them on top of each other. This is a simple adjustment to our previous command.

We just need to:

1) Tell ggplot not to stack the histograms by adding the argument position="identity" to the geom_histogram function.

2) Make the bars semi-transparent so we can see both colors by adding the argument alpha=0.5 to the geom_histogram function.

Redo the plot, making both of these changes.

Which of the following buckets contain no female paroles? Select all that apply.
15-19
20-24
25-29
30-34
35-39
40-44
45-49
50-54
55-59
60-64
65-69
- unanswered

Explanation

This plot can be generated with the following command:

ggplot(parole, aes(x = age, fill = male)) + geom_histogram(binwidth = 5, position = "identity", alpha = 0.5) + scale_fill_manual(values=colorPalette)

If you look at the plot, you can see that there are no female parolees in the age groups 15-19, 55-59, and 65-69 (the bars have height zero).
You have used 0 of 2 submissions
** TODO Problem 4.1 - Time Served

Now let's explore another aspect of the data: the amount of time served by parolees. Create a basic histogram like the one we created in Problem 2, but this time with time.served on the x-axis. Set the bin width to one month.

What is the most common length of time served, according to this histogram?
Between 2 and 3 months
Between 3 and 4 months
Between 4 and 5 months
Between 5 and 6 months
- unanswered

Explanation

You can create this histogram with the following command:

ggplot(data = parole, aes(x = time.served)) + geom_histogram(binwidth = 1)

The highest bar corresponds to between 4 and 5 months.
You have used 0 of 1 submissions
** TODO Problem 4.2 - Time Served

Change the binwidth to 0.1 months. Now what is the most common length of time served, according to the histogram?
Between 2.1 and 2.2 months
Between 3.0 and 3.1 months
Between 4.2 and 4.3 months
Between 4.8 and 4.9 months
- unanswered

Explanation

You can change the binwidth by using the following command:

ggplot(data = parole, aes(x = time.served)) + geom_histogram(binwidth = .1)

Now, the highest bar corresponds to between 3.0 and 3.1 months.

Be careful when choosing the binwidth - it can significantly affect the interpretation of a histogram! When visualizing histograms, it is always a good idea to vary the bin size in order to understand the data at various granularities.
You have used 0 of 1 submissions
** TODO Problem 4.3 - Time Served

Now, suppose we suspect that it is unlikely that each type of crime has the same distribution of time served. To visualize this, change the binwidth back to 1 month, and use facet_grid to create a separate histogram of time.served for each value of the variable crime.

Which crime type has no observations where time served is less than one month? Recall that crime type #2 is larceny, #3 is drug-related crime, #4 is driving-related crime, and #1 is any other crime.
Larceny
Drug-related
Driving-related
Other
- unanswered

For which crime does the frequency of 5-6 month prison terms exceed the frequencies of each other term length?
Larceny
Drug-related
Driving-related
Other
- unanswered

Explanation

This histogram can be generated using the command:

ggplot(data = parole, aes(x = time.served)) + geom_histogram(binwidth = 1) + facet_grid(crime ~ .)
You have used 0 of 1 submissions

** TODO Problem 4.4 - Time Served

Now, instead of faceting the histograms, overlay them. Remember to set the position and alpha parameters so that the histograms are not stacked. Also, make sure to indicate that the fill aesthetic should be "crime".

In this case, faceting seems like a better alternative. Why?
With four different groups, it can be hard to tell them apart when they are overlayed.
ggplot doesn't let us overlay plots with more than two groups.
Overlaying the plots doesn't allow us to observe which crime type is the most common.
- unanswered

Explanation

You can generate this plot with the following command:

ggplot(data=parole, aes(x=time.served, fill=crime)) + geom_histograph(binwidth=1, position="identity", alpha=0.5)

While overlaying the plots is allowed and lets us observe some attributes of the plots like the most common crime type, it can be hard to tell them apart and if they have similar values it can be hard to read.
